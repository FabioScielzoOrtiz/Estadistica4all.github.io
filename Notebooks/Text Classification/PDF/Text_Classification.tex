% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  11pt,
  a4paper,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[left=3cm,right=3cm,top=2cm,bottom=2cm]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{fvextra}
\usepackage{mathtools}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{lscape}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{lscape}
\usepackage{longtable}
\renewcommand{\contentsname}{Índice}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{\vspace{-2.5em}}

\begin{document}

%\begin{titlepage}

\graphicspath{ {images/} }

\begin{center}
\centering
	\includegraphics[width=1.5\textwidth]{uc3m}\par\vspace{1cm}
{\scshape\LARGE Universidad Carlos III de Madrid \par}
	\vspace{1cm}
	{\scshape\Large Métodos Bayesianos, Grado en Estadística y Empresa \par}
	\vspace{1.5cm}
	{\huge\bfseries Análisis  y clasificación de textos \par}
	\vspace{2cm}
	{\Large\itshape Fabio Scielzo Ortiz \par}
	\date{28 de octubre de 2022}
	\vfill
	\vfill
\end{center}
% \end{titlepage}

\newpage
\tableofcontents
\newpage

\hypertarget{introducciuxf3n}{%
\section{Introducción}\label{introducciuxf3n}}

En este trabajo se va a realizar un análisis y clasificación de textos.
Para ellos se utilizaran dos lenguajes de programación, \texttt{Python}
y \texttt{R}. El trabajo puede dividirse en dos partes bien
diferenciadas, una primera parte en la que se trabaja con
\texttt{Python} y una segunda en la que se usa \texttt{R}.

En la primera parte, en la que trabajamos con \texttt{Python}, se
llevará acabo una descripción y preprocesado del data-set con el que
trabajaremos, posteriormente se llevara acabo un análisis de texto, y
para finalizar se realizaran tareas de clasificación aplicando
algoritmos de clasificación supervisada, especialmente el algoritmo de
clasificación ingenua bayesiana.

En la parte en la que trabajamos con \texttt{R} se seguirán los pasos
del ejemplo ilustrado en clase.

\hypertarget{carga-de-los-datos-python}{%
\section{\texorpdfstring{Carga de los datos
(\texttt{Python})}{Carga de los datos (Python)}}\label{carga-de-los-datos-python}}

El data-set con el que vamos a trabajar contiene como observaciones
noticias fechadas entre el 31 de marzo de 2015 y el 18 de febrero de
2018, y como variables la fecha, el título y el texto de la noticia, y
si es una noticia falsa (fake new) o es verdadera (no fake new). La
variable respuesta será \texttt{Fake} . Las variables predictoras que se
usaran en el apartado de aplicación de algoritmos de clasificación no
aparecen en el data-set original, pero serán creadas usando la
información de la variable \texttt{texto}.

El data set ha sido obtenido de la pagina web
\href{https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset}{Kaggle}

Importamos la libreria \texttt{pandas}, que es la liberia de
\texttt{Python} mas usada para la manipulación y manejo de datos en
formato de tabla, es decir, data-frames.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

Ahora importamos los datos, que originalmente estan distribuidos en dos
data-sets, uno que contiene las fake news (\texttt{df\_Fake}) y otro que
contiene las no fake news (\texttt{df\_True}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_Fake }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}Fake.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{df\_True }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}True.csv\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Creamos una variable que indicará en nuestro data-set final si la
noticia es fake o no fake:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_Fake[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \DecValTok{1}
\NormalTok{df\_True[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

Si para una noticia la nueva variable creada \texttt{Fake} toma el valor
1 , indica que es fake new, y si toma el 0 indica que no es fake new.

Ahora concatenamos (por filas) los dos data-sets anteriores, para
generar el data-set con el que trabajaremos:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data }\OperatorTok{=}\NormalTok{ pd.concat([df\_Fake, df\_True])}
\end{Highlighting}
\end{Shaded}

Seleccionamos las columnas (variables) de nuestro interés:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data }\OperatorTok{=}\NormalTok{ Fake\_News\_Data.loc[: , [}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}title\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}text\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}date\textquotesingle{}}\NormalTok{] ]}
\end{Highlighting}
\end{Shaded}

Añadimos un índice al data-set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data.index }\OperatorTok{=} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{ , }\BuiltInTok{len}\NormalTok{(Fake\_News\_Data))}
\end{Highlighting}
\end{Shaded}

Ahora vamos a ver de qué tipo son nuestras variables en \texttt{Python}
:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data.dtypes}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Fake      int64
title    object
text     object
date     object
dtype: object
\end{verbatim}

El tipo \texttt{object} es propio de variables no cuantitativos, como
categoricas o texto, y el tipo \texttt{int64} es propio de variables
enteras.

En este caso dejaremos los types como están, salvo el de la variable
\texttt{Fake} que es categorica y por tanto es más adecuado que su type
sea \texttt{object}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ Fake\_News\_Data[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{].astype(}\StringTok{\textquotesingle{}object\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Calculamos el numero de valores faltantes (NA) en cada una de las
variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data.isnull().}\BuiltInTok{sum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Fake     0
title    0
text     0
date     0
\end{verbatim}

Vamos a imprimir el data set para hacernos una mejor idea de su
contenido:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Fake                                              title  
      
0        1   Donald Trump Sends Out Embarrassing New Year’...   
1        1   Drunk Bragging Trump Staffer Started Russian ...   
2        1   Sheriff David Clarke Becomes An Internet Joke...   
3        1   Trump Is So Obsessed He Even Has Obama’s Name...   
4        1   Pope Francis Just Called Out Donald Trump Dur...   
...    ...                                                ...   
44893    0  'Fully committed' NATO backs new U.S. approach...   
44894    0  LexisNexis withdrew two products from Chinese ...   
44895    0  Minsk cultural hub becomes haven from authorities   
44896    0  Vatican upbeat on possibility of Pope Francis ...   
44897    0  Indonesia to buy $1.14 billion worth of Russia...   

                                                    text               date 
                                                    
0      Donald Trump just couldn t wish all Americans ...  December 31, 2017  
1      House Intelligence Committee Chairman Devin Nu...  December 31, 2017  
2      On Friday, it was revealed that former Milwauk...  December 30, 2017  
3      On Christmas day, Donald Trump announced that ...  December 29, 2017  
4      Pope Francis used his annual Christmas Day mes...  December 25, 2017  
...                                                  ...                ...  
44893  BRUSSELS (Reuters) - NATO allies on Tuesday we...   August 22, 2017   
44894  LONDON (Reuters) - LexisNexis, a provider of l...   August 22, 2017   
44895  MINSK (Reuters) - In the shadow of disused Sov...   August 22, 2017   
44896  MOSCOW (Reuters) - Vatican Secretary of State ...   August 22, 2017   
44897  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...   August 22, 2017  
\end{verbatim}

\hypertarget{descripciuxf3n-estadistica-de-los-datos-python}{%
\section{\texorpdfstring{Descripción estadistica de los datos
(\texttt{Python})}{Descripción estadistica de los datos (Python)}}\label{descripciuxf3n-estadistica-de-los-datos-python}}

Hacemos una breve descripción estadistica de las variables del data-set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data.describe(include}\OperatorTok{=}\StringTok{\textquotesingle{}all\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

              Fake                title       
count         44898               44898        
unique        2                   38729        
top           1                   Factbox: Trump fills top jobs for his administ...          
freq          23481               14         



              date                  text 
count         44898                 44898
unique        2397                  38646
top           December 20, 2017     (no se muestra por tamaño excesivo)
freq          182                   627
\end{verbatim}

Esta tabla nos da alguna informacion relevante, como que en el data-set
hay mas fake news que no fake news. Concretamente hay 44898 noticias, de
las cuales 23481 son fakes y 44898-23481 = 21417 son no fakes.

Vamos ahora a realizar un análisis descriptivo del data-set algo más
profundo.

\hypertarget{gruxe1fico-de-barras-de-la-variable-respuesta-fake}{%
\subsection{\texorpdfstring{Gráfico de barras de la variable respuesta
(\texttt{Fake})}{Gráfico de barras de la variable respuesta (Fake)}}\label{gruxe1fico-de-barras-de-la-variable-respuesta-fake}}

Importamos algunas librerias necesarias para realizar este análisis en
\texttt{Python}

Concretamente la libreria \texttt{numpy} da soporte para crear vectores
y matrices grandes multidimensionales, junto con una gran colección de
funciones matemáticas de alto nivel para operar con ellas. En general es
una de las librerias de \texttt{Python} más empleadas junto con
\texttt{pandas}

Tambien importamos las librerias \texttt{seaborn} y
\texttt{matplotlib}que son muy empleadas para visualización de datos
(creación de gráficos).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ matplotlib }\ImportTok{as}\NormalTok{ mpl}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{sns.}\BuiltInTok{set}\NormalTok{(rc}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}figure.figsize\textquotesingle{}}\NormalTok{:(}\DecValTok{8}\NormalTok{,}\DecValTok{8}\NormalTok{)\})}
\end{Highlighting}
\end{Shaded}

Vamos a calcular un gráfico de barras para la variable \texttt{Fake}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prop\_Fake\_yes }\OperatorTok{=} \BuiltInTok{len}\NormalTok{( Fake\_News\_Data.loc[ Fake\_News\_Data[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{]}\OperatorTok{==} \DecValTok{1}\NormalTok{ , :] ) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(Fake\_News\_Data)}

\NormalTok{prop\_Fake\_no }\OperatorTok{=} \BuiltInTok{len}\NormalTok{( Fake\_News\_Data.loc[ Fake\_News\_Data[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{]}\OperatorTok{==} \DecValTok{0}\NormalTok{ , :] ) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(Fake\_News\_Data)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data[}\StringTok{\textquotesingle{}proportion\_Fakes\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \DecValTok{0}


\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, }\BuiltInTok{len}\NormalTok{(Fake\_News\_Data)):}

    \ControlFlowTok{if}\NormalTok{ Fake\_News\_Data[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{][i] }\OperatorTok{==} \DecValTok{1}\NormalTok{ :}

\NormalTok{        Fake\_News\_Data[}\StringTok{\textquotesingle{}proportion\_Fakes\textquotesingle{}}\NormalTok{][i] }\OperatorTok{=}\NormalTok{ prop\_Fake\_yes}

    \ControlFlowTok{else}\NormalTok{ :}

\NormalTok{        Fake\_News\_Data[}\StringTok{\textquotesingle{}proportion\_Fakes\textquotesingle{}}\NormalTok{][i] }\OperatorTok{=}\NormalTok{ prop\_Fake\_no}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}


\NormalTok{p }\OperatorTok{=}\NormalTok{ sns.barplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}proportion\_Fakes\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{Fake\_News\_Data, palette}\OperatorTok{=}\StringTok{"Spectral"}\NormalTok{) }
\NormalTok{p.set\_yticks( np.arange(}\DecValTok{0}\NormalTok{, }\FloatTok{0.85}\NormalTok{, }\FloatTok{0.1}\NormalTok{)  )}
\NormalTok{p.set\_xticklabels([}\StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{])}
\NormalTok{p.axes.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}Fakes\textquotesingle{}}\NormalTok{, ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}proportion\textquotesingle{}}\NormalTok{)}

\NormalTok{fig.savefig(}\StringTok{\textquotesingle{}p.png\textquotesingle{}}\NormalTok{, }\BuiltInTok{format}\OperatorTok{=}\StringTok{\textquotesingle{}png\textquotesingle{}}\NormalTok{, dpi}\OperatorTok{=}\DecValTok{1200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{p.png}
\caption{Gráfico de barras de la variable \texttt{Fake}}
\end{figure}

Las proporciones exactas de fake y no fake news son:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[prop\_Fake\_no , prop\_Fake\_yes]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[0.47701456635039424, 0.5229854336496058]
\end{verbatim}

El número exacto de fake y no fake news es:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[prop\_Fake\_no}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(Fake\_News\_Data) , prop\_Fake\_yes}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(Fake\_News\_Data)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[21417.0, 23481.0]
\end{verbatim}

Eliminamos la columna \texttt{proportion\_Fakes} del data-set, que ha
sido creada solamente de manera auxiliar para poder generar el gráfico
de barras anterior:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data }\OperatorTok{=}\NormalTok{ Fake\_News\_Data.loc[ : , Fake\_News\_Data.columns }\OperatorTok{!=} \StringTok{\textquotesingle{}proportion\_Fakes\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\hypertarget{nuxfamero-de-palabras-por-noticia}{%
\subsection{Número de palabras por
noticia}\label{nuxfamero-de-palabras-por-noticia}}

Una forma de calcular en \texttt{Python} el número de palabras de cada
notica es la siguiente:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data[}\StringTok{\textquotesingle{}word\_count\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ Fake\_News\_Data[}\StringTok{\textquotesingle{}text\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.split().}\BuiltInTok{str}\NormalTok{.}\BuiltInTok{len}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Vamos a ver el data-set con la nueva columna \texttt{word\_count} que
contiene el nº de palabras por noticia

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Fake   title        
      
0        1   Donald Trump Sends Out Embarrassing New Year’...   
1        1   Drunk Bragging Trump Staffer Started Russian ...   
2        1   Sheriff David Clarke Becomes An Internet Joke...   
3        1   Trump Is So Obsessed He Even Has Obama’s Name...   
4        1   Pope Francis Just Called Out Donald Trump Dur...

...    ...                                                ...   

44893    0  'Fully committed' NATO backs new U.S. approach...   
44894    0  LexisNexis withdrew two products from Chinese ...   
44895    0  Minsk cultural hub becomes haven from authorities   
44896    0  Vatican upbeat on possibility of Pope Francis ...   
44897    0  Indonesia to buy $1.14 billion worth of Russia...   



       text                                               date               
       
0      Donald Trump just couldn t wish all Americans ...  December 31, 2017   
1      House Intelligence Committee Chairman Devin Nu...  December 31, 2017   
2      On Friday, it was revealed that former Milwauk...  December 30, 2017   
3      On Christmas day, Donald Trump announced that ...  December 29, 2017   
4      Pope Francis used his annual Christmas Day mes...  December 25, 2017   

...                                                  ...                ...   

44893  BRUSSELS (Reuters) - NATO allies on Tuesday we...   August 22, 2017    
44894  LONDON (Reuters) - LexisNexis, a provider of l...   August 22, 2017    
44895  MINSK (Reuters) - In the shadow of disused Sov...   August 22, 2017    
44896  MOSCOW (Reuters) - Vatican Secretary of State ...   August 22, 2017    
44897  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...   August 22, 2017    


       word_count
       
0             495  
1             305  
2             580  
3             444  
4             420  

...           ...  

44893         466  
44894         125  
44895         320  
44896         205  
44897         210  
\end{verbatim}

\hypertarget{numero-medio-de-palabras-por-noticia-en-funciuxf3n-de-si-son-fake-o-no}{%
\subsection{Numero medio de palabras por noticia en función de si son
fake o
no}\label{numero-medio-de-palabras-por-noticia-en-funciuxf3n-de-si-son-fake-o-no}}

Calculamos ahora la media de palabras de las fakes news y de la sno fake
news. Es decir, el nº medio de palabras en el cojuntos de las noticias
fake, y por otro lado en el conjutno de las no fake:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data.groupby(}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}word\_count\textquotesingle{}}\NormalTok{].mean()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Fake   Mean word_count

0        385.640099

1        423.197905
\end{verbatim}

\hypertarget{preprocesado-de-texto}{%
\section{Preprocesado de texto}\label{preprocesado-de-texto}}

En este apartado se vana a hacer una serie de operaciones orientadas al
preprocesado de texto, para poder posteriormente realizar analasis mas
profundos, y para poder implementar algoritmos de clasificación sobre
texto.

Este tipo de preprocesado es básico y fundamental en areas de la ciencia
de datos que trabajan con texto, como son la mineria de texto (text
minning), el procesamiento del lenguaje natural (PLN) y la recuperación
de información (information retrival).

Una de las operaciones centrales del preproceso de textos es la
\texttt{tokenización}.

\hypertarget{tokenizacion}{%
\subsection{Tokenizacion}\label{tokenizacion}}

Existen algunas librerias de \texttt{Python} que tienen funciones para
realizar operaciones de tokenizacion, como por ejemplo las librerias
\texttt{sklearn}, \texttt{nltk} o \texttt{spaCy}

En este caso no usaremos ninguna función de alguna de esas librerias,
sino que crearemos nuestra propia función para realizar la tokenización.

Esta función esta totalmente inspirada en la función creada por el
cientifico de datos \href{}{Joaquín Amat Rodrigo}, el cual es el creador
del excelente blog sobre ciencia de datos
\href{https://www.cienciadedatos.net/}{Cienciadedatos.net}. En este blog
Joaquin tiene un articulo sobre analisis de texto en \texttt{Python} en
el cual se encuentra la función que ahora vamos a presentar. Ademas
muchas otras partes de este trabajo estan basadas en dicho articulo, es
por ello que s ele hace una especial mención tanto aqui como en el
apartado de bibliografia.

La función \texttt{limpiar\_tokenizar} toma como input texto y devuelve
como output un vector de tokens asociado a ese texto, es decir, un
vector con las cadenas caracteres del texto, pero no con cualquier tipo,
sino que la función no considera signos de puntuación , palabras que
empiezan por ``http'', números, espacios en blancos múltiples, tokens
con longitud menor que 2.

Un token aqui es considerado como una cadena de caracteres, es decir,
una concatenacion de símbolos (sin considerar el espacio en blanco como
un símbolo).

Veamos un ejemplo de lo que consideramos tokens:

Dado el siguiente texto:

'' Esto es 1 ejemplo de l'limpieza de6 TEXTO
\url{https://t.co/rnHPgyhx4Z} @cienciadedatos \#textmining ''

Los tokens (en sentido estricto, no en el sentido restrictivo que
considera la función \texttt{limpiar\_tokenizar} ) asociados a dicho
texto son:

{[} Esto , es , 1 , ejemplo , de , l\textquotesingle limpieza , de6 ,
TEXTO , https://t.co/rnHPgyhx4Z , @cienciadedatos , \#textmining {]}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ limpiar\_tokenizar(texto):}

    \ImportTok{import}\NormalTok{ re}
    
    \CommentTok{\textquotesingle{}\textquotesingle{}\textquotesingle{}}
\CommentTok{    Esta función limpia y tokeniza el texto en palabras individuales.}
\CommentTok{    El orden en el que se va limpiando el texto no es arbitrario.}
\CommentTok{    El listado de signos de puntuación se ha obtenido de: print(string.punctuation)}
\CommentTok{    y re.escape(string.punctuation)}
\CommentTok{    \textquotesingle{}\textquotesingle{}\textquotesingle{}}
    
    \CommentTok{\# Se convierte todo el texto a minúsculas:}

\NormalTok{    nuevo\_texto }\OperatorTok{=}\NormalTok{ texto.lower()}
    
    
    \CommentTok{\# Eliminacion de paginas web (palabras que empiezan por "http"):}
    
    \CommentTok{\#\# Las cadenas de caracteres que sean enlaces a webs no serán consideradas como tokens}
    
\NormalTok{    nuevo\_texto }\OperatorTok{=}\NormalTok{ re.sub(}\StringTok{\textquotesingle{}http\textbackslash{}S+\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{, nuevo\_texto)}
    
    
    \CommentTok{\# Eliminacion de signos de puntuación:}
    
    \CommentTok{\#\# Si una cadena de caractrer contiene un signo de puntuacion estos serán eliminados y sustituidos por un   espacio en blanco. Si  por ejemplo tenemos las cadenas  \textquotesingle{}@FabioScielzo\textquotesingle{} y \textquotesingle{}Fabio@Scielzo\textquotesingle{} ,}
    \CommentTok{\#\# la funcion las transforma en  \textquotesingle{}FabioScielzo\textquotesingle{} en el primer caso  y en el par de cadenas \textquotesingle{}Fabio\textquotesingle{} , \textquotesingle{}Scielzo\textquotesingle{} en el segundo. Y si tenemos}
    \CommentTok{\#\# una cadena de signos d puntuacion   como \textquotesingle{}@\#!\textquotesingle{} la elimina directamente.}
    
\NormalTok{    regex }\OperatorTok{=} \StringTok{\textquotesingle{}[}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{!}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{\#}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{$}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{\%}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{\&}\CharTok{\textbackslash{}\textbackslash{}\textbackslash{}\textquotesingle{}\textbackslash{}\textbackslash{}}\StringTok{(}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{)}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{*}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{+}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{,}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{{-}}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{.}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{/}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{:}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{;}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{\textless{}}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{=}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{\textgreater{}}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{?}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{@}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{[}\CharTok{\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}}\StringTok{]}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{\^{}\_}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{\textasciigrave{}}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{\{}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{|}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{\}}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{\textasciitilde{}]\textquotesingle{}}
    
\NormalTok{    nuevo\_texto }\OperatorTok{=}\NormalTok{ re.sub(regex , }\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{, nuevo\_texto)}
    
    
    \CommentTok{\# Eliminacion de numeros:}
    
    \CommentTok{\#\# Si una cadena de caracter tiene numeros estos serán eliminados y sustituidos por un espacio en blanco. Si por ejemplo tenemos las cadenas \textquotesingle{}4FabioScielzo\textquotesingle{} y \textquotesingle{}Fabio44Scielzo\textquotesingle{} la funcion las transforma en \textquotesingle{}FabioScielzo\textquotesingle{} y \textquotesingle{}Fabio\textquotesingle{} , \textquotesingle{}Scielzo\textquotesingle{} , respectivamente. Ademas si una cadena solo contienen numeros, por ejemplo \textquotesingle{}123\textquotesingle{} la elimina directamente.}
    
\NormalTok{    nuevo\_texto }\OperatorTok{=}\NormalTok{ re.sub(}\StringTok{"\textbackslash{}d+"}\NormalTok{, }\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{, nuevo\_texto)}
    
    
    
    \CommentTok{\# Eliminacion de espacios en blanco multiples:}
    
    \CommentTok{\#\# Si tenemos en un texto dos o mas espacios en blanco consecutivos la funcion los transforma en un solo espacio en blanco. Por ejemplo si tenemos el texto "Fabio     es abogado" la funcion lo transforma en "Fabio es abogado".}
    
\NormalTok{    nuevo\_texto }\OperatorTok{=}\NormalTok{ re.sub(}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s+"}\NormalTok{, }\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{, nuevo\_texto)}
    
    
    \CommentTok{\# Una vez que a un texto se le han aplicado las operaciones anteriores ya solo quede considerar las cadenas de caracteres de ese texto como tokens, ya que son cadenas con buenas propiedades, a saber, sin signos de puntuacion, sin numeros, sin links de web. Ademas la eliminacion de espacios en blanco multiples es fundamental para que la siguiente operacion funcione bien, ya que en el texto final resultante todas las cadenas estan separadas entre si por un solo espacio, y la siguiente operacion utiliza esa propiedad para identificar a las cadenas, que ya serán considerados tokens en sentido estricto.}
    
    \CommentTok{\# Obtención de tokens:}
    
\NormalTok{    nuevo\_texto }\OperatorTok{=}\NormalTok{ nuevo\_texto.split(sep }\OperatorTok{=} \StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{)}
    
    
    \CommentTok{\# Eliminacion de tokens con una longitud menor que  2:}
    
    \CommentTok{\#\# Una ultima operacion es solo considerar los tokens obteenidos tras las operaciones anteriores que tengan un tamaño (nº de caracteres) igual o superior a 2 , es decir, dejar fuera tokens con solo un caracter.}
    
\NormalTok{    nuevo\_texto }\OperatorTok{=}\NormalTok{ [token }\ControlFlowTok{for}\NormalTok{ token }\KeywordTok{in}\NormalTok{ nuevo\_texto }\ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(token) }\OperatorTok{\textgreater{}=}  \DecValTok{2}\NormalTok{]}
    
    \ControlFlowTok{return}\NormalTok{(nuevo\_texto)}
\end{Highlighting}
\end{Shaded}

Probamos el funcionamiento de la función \texttt{limpiar\_tokenizar} con
el mismo texto que fue usado antes como ejemplo ilustrativo.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{test }\OperatorTok{=} \StringTok{"Esto es 1 ejemplo de l\textquotesingle{}limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos \#textmining"}

\BuiltInTok{print}\NormalTok{(limpiar\_tokenizar(texto}\OperatorTok{=}\NormalTok{test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['esto', 'es', 'ejemplo', 'de', 'limpieza', 'de', 'texto', 'cienciadedatos', 'textmining']
\end{verbatim}

Ahora probamos la función \texttt{limpiar\_tokenizar} con la primera
noticia del data-set \texttt{Fake\_News\_Data}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data[}\StringTok{\textquotesingle{}text\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.'
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(limpiar\_tokenizar(texto}\OperatorTok{=}\NormalTok{Fake\_News\_Data[}\StringTok{\textquotesingle{}text\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['donald', 'trump', 'just', 'couldn', 'wish', 'all', 'americans', 'happy', 'new', 'year', 'and', 'leave', 'it', 'at', 'that', 'instead', 'he', 'had', 'to', 'give', 'shout', 'out', 'to', 'his', 'enemies', 'haters', 'and', 'the', 'very', 'dishonest', 'fake', 'news', 'media', 'the', 'former', 'reality', 'show', 'star', 'had', 'just', 'one', 'job', 'to', 'do', 'and', 'he', 'couldn', 'do', 'it', 'as', 'our', 'country', 'rapidly', 'grows', 'stronger', 'and', 'smarter', 'want', 'to', 'wish', 'all', 'of', 'my', 'friends', 'supporters', 'enemies', 'haters', 'and', 'even', 'the', 'very', 'dishonest', 'fake', 'news', 'media', 'happy', 'and', 'healthy', 'new', 'year', 'president', 'angry', 'pants', 'tweeted', 'will', 'be', 'great', 'year', 'for', 'america', 'as', 'our', 'country', 'rapidly', 'grows', 'stronger', 'and', 'smarter', 'want', 'to', 'wish', 'all', 'of', 'my', 'friends', 'supporters', 'enemies', 'haters', 'and', 'even', 'the', 'very', 'dishonest', 'fake', 'news', 'media', 'happy', 'and', 'healthy', 'new', 'year', 'will', 'be', 'great', 'year', 'for', 'america', 'donald', 'trump', 'realdonaldtrump', 'december', 'trump', 'tweet', 'went', 'down', 'about', 'as', 'welll', 'as', 'you', 'expect', 'what', 'kind', 'of', 'president', 'sends', 'new', 'year', 'greeting', 'like', 'this', 'despicable', 'petty', 'infantile', 'gibberish', 'only', 'trump', 'his', 'lack', 'of', 'decency', 'won', 'even', 'allow', 'him', 'to', 'rise', 'above', 'the', 'gutter', 'long', 'enough', 'to', 'wish', 'the', 'american', 'citizens', 'happy', 'new', 'year', 'bishop', 'talbert', 'swan', 'talbertswan', 'december', 'no', 'one', 'likes', 'you', 'calvin', 'calvinstowell', 'december', 'your', 'impeachment', 'would', 'make', 'great', 'year', 'for', 'america', 'but', 'll', 'also', 'accept', 'regaining', 'control', 'of', 'congress', 'miranda', 'yaver', 'mirandayaver', 'december', 'do', 'you', 'hear', 'yourself', 'talk', 'when', 'you', 'have', 'to', 'include', 'that', 'many', 'people', 'that', 'hate', 'you', 'you', 'have', 'to', 'wonder', 'why', 'do', 'the', 'they', 'all', 'hate', 'me', 'alan', 'sandoval', 'alansandoval', 'december', 'who', 'uses', 'the', 'word', 'haters', 'in', 'new', 'years', 'wish', 'marlene', 'marlene', 'december', 'you', 'can', 'just', 'say', 'happy', 'new', 'year', 'koren', 'pollitt', 'korencarpenter', 'december', 'here', 'trump', 'new', 'year', 'eve', 'tweet', 'from', 'happy', 'new', 'year', 'to', 'all', 'including', 'to', 'my', 'many', 'enemies', 'and', 'those', 'who', 'have', 'fought', 'me', 'and', 'lost', 'so', 'badly', 'they', 'just', 'don', 'know', 'what', 'to', 'do', 'love', 'donald', 'trump', 'realdonaldtrump', 'december', 'this', 'is', 'nothing', 'new', 'for', 'trump', 'he', 'been', 'doing', 'this', 'for', 'years', 'trump', 'has', 'directed', 'messages', 'to', 'his', 'enemies', 'and', 'haters', 'for', 'new', 'year', 'easter', 'thanksgiving', 'and', 'the', 'anniversary', 'of', 'pic', 'twitter', 'com', 'fpae', 'kypa', 'daniel', 'dale', 'ddale', 'december', 'trump', 'holiday', 'tweets', 'are', 'clearly', 'not', 'presidential', 'how', 'long', 'did', 'he', 'work', 'at', 'hallmark', 'before', 'becoming', 'president', 'steven', 'goodine', 'sgoodine', 'december', 'he', 'always', 'been', 'like', 'this', 'the', 'only', 'difference', 'is', 'that', 'in', 'the', 'last', 'few', 'years', 'his', 'filter', 'has', 'been', 'breaking', 'down', 'roy', 'schulze', 'thbthttt', 'december', 'who', 'apart', 'from', 'teenager', 'uses', 'the', 'term', 'haters', 'wendy', 'wendywhistles', 'december', 'he', 'fucking', 'year', 'old', 'who', 'knows', 'rainyday', 'december', 'so', 'to', 'all', 'the', 'people', 'who', 'voted', 'for', 'this', 'hole', 'thinking', 'he', 'would', 'change', 'once', 'he', 'got', 'into', 'power', 'you', 'were', 'wrong', 'year', 'old', 'men', 'don', 'change', 'and', 'now', 'he', 'year', 'older', 'photo', 'by', 'andrew', 'burton', 'getty', 'images']
\end{verbatim}

Ahora aplicamos la función \texttt{limpiar\_tokenizar} a cada una de las
noticias del data-set \texttt{Fake\_News\_Data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data[}\StringTok{\textquotesingle{}text\_tokenizado\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ Fake\_News\_Data[}\StringTok{\textquotesingle{}text\textquotesingle{}}\NormalTok{].}\BuiltInTok{apply}\NormalTok{( limpiar\_tokenizar )}
\end{Highlighting}
\end{Shaded}

Creamos una columna que identifique las noticias:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data[}\StringTok{\textquotesingle{}id\_text\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, }\BuiltInTok{len}\NormalTok{(Fake\_News\_Data))}
\end{Highlighting}
\end{Shaded}

Vemos como queda tras estos cambios el data-set
\texttt{Fake\_News\_Data}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Fake                    title                                            
      
0        1   Donald Trump Sends Out Embarrassing New Year’...   
1        1   Drunk Bragging Trump Staffer Started Russian ...   
2        1   Sheriff David Clarke Becomes An Internet Joke...   
3        1   Trump Is So Obsessed He Even Has Obama’s Name...   
4        1   Pope Francis Just Called Out Donald Trump Dur...   

...    ...                     ...   

44893    0  'Fully committed' NATO backs new U.S. approach...   
44894    0  LexisNexis withdrew two products from Chinese ...   
44895    0  Minsk cultural hub becomes haven from authorities   
44896    0  Vatican upbeat on possibility of Pope Francis ...   
44897    0  Indonesia to buy $1.14 billion worth of Russia...   


                         text                                   date                

0      Donald Trump just couldn t wish all Americans ...  December 31, 2017   
1      House Intelligence Committee Chairman Devin Nu...  December 31, 2017   
2      On Friday, it was revealed that former Milwauk...  December 30, 2017   
3      On Christmas day, Donald Trump announced that ...  December 29, 2017   
4      Pope Francis used his annual Christmas Day mes...  December 25, 2017 

...                     ...                                      ...   

44893  BRUSSELS (Reuters) - NATO allies on Tuesday we...   August 22, 2017    
44894  LONDON (Reuters) - LexisNexis, a provider of l...   August 22, 2017    
44895  MINSK (Reuters) - In the shadow of disused Sov...   August 22, 2017    
44896  MOSCOW (Reuters) - Vatican Secretary of State ...   August 22, 2017    
44897  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...   August 22, 2017   


       word_count              text_tokenizado                        id_text  

0         495     [donald, trump, just, couldn, wish, all, ameri...      0  
1         305     [house, intelligence, committee, chairman, dev...      1  
2         580     [on, friday, it, was, revealed, that, former, ...      2  
3         444     [on, christmas, day, donald, trump, announced,...      3  
4         420     [pope, francis, used, his, annual, christmas, ...      4  

...       ...                        ...                                ...  

44893     466     [brussels, reuters, nato, allies, on, tuesday,...    44893  
44894     125     [london, reuters, lexisnexis, provider, of, le...    44894  
44895     320     [minsk, reuters, in, the, shadow, of, disused,...    44895  
44896     205     [moscow, reuters, vatican, secretary, of, stat...    44896  
44897     210     [jakarta, reuters, indonesia, will, buy, sukho...    44897  
\end{verbatim}

Creamos un nuevo data-frame solo con las columnas (variables)
\texttt{id\_text} , \texttt{text\_tokenizado} y \texttt{Fake}, en ell
que la columna \texttt{text\_tokenizado} esta expandida, es decir, al
ser una columna cuyos elementos son vectores, lo que se hace con la
operacion \texttt{explode} es expandir cada uno de esos vectores en un
nuevo data-frame, es decir, para cada uno de esos vectores se crean
tantas filas en el nuevo data-frame como elementos hay en el vector, y
en cada una de esas filas la columna \texttt{text\_tokenizado} contendra
un elemento del vector expandido. Visualmente es mas facil de entenderlo
como se verá a continuación:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Tokens }\OperatorTok{=}\NormalTok{ Fake\_News\_Data.loc[:, [}\StringTok{\textquotesingle{}id\_text\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}text\_tokenizado\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{] ].explode(column}\OperatorTok{=}\StringTok{\textquotesingle{}text\_tokenizado\textquotesingle{}}\NormalTok{)}


\CommentTok{\# Renombramos la columna \textasciigrave{}text\_tokenizado\textasciigrave{} como \textasciigrave{}token\textasciigrave{} :}

\NormalTok{Fake\_News\_Tokens }\OperatorTok{=}\NormalTok{ Fake\_News\_Tokens.rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}text\_tokenizado\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Imprimimos el nuevo data-frame creado \texttt{Fake\_News\_Tokens} al
expandir la columna \texttt{text\_tokenizado} del data-frame
\texttt{Fake\_News\_Data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Tokens}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       id_text       token    Fake
0            0      donald      1
0            0       trump      1
0            0        just      1
0            0      couldn      1
0            0        wish      1

...        ...         ...     ...

44897    44897  technology      0
44897    44897         and      0
44897    44897    aviation      0
44897    44897       among      0
44897    44897      others      0
\end{verbatim}

\newpage

\hypertarget{descripciuxf3n-estaduxedstica-de-los-datos-tras-la-tokenizaciuxf3n}{%
\section{\texorpdfstring{Descripción estadística de los datos tras la
\texttt{tokenización}}{Descripción estadística de los datos tras la tokenización}}\label{descripciuxf3n-estaduxedstica-de-los-datos-tras-la-tokenizaciuxf3n}}

\hypertarget{numero-de-tokens-del-conjunto-de-noticias-en-funcion-de-si-son-fake-o-no}{%
\subsection{Numero de tokens del conjunto de noticias en funcion de si
son fake o
no}\label{numero-de-tokens-del-conjunto-de-noticias-en-funcion-de-si-son-fake-o-no}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# nº de palabras (tokens) en el conjunto de textos clasificados como fake y en los no fake}

\NormalTok{Fake\_News\_Tokens.groupby(by}\OperatorTok{=}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{].count()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Fake
0    7891501
1    9611544
Name: token, dtype: int64
\end{verbatim}

\hypertarget{numero-de-tokens-uxfanicos-del-conjunto-de-noticias-en-funcion-de-si-son-fake-o-no}{%
\subsection{\texorpdfstring{Numero de tokens \emph{únicos} del conjunto
de noticias en funcion de si son fake o
no}{Numero de tokens únicos del conjunto de noticias en funcion de si son fake o no}}\label{numero-de-tokens-uxfanicos-del-conjunto-de-noticias-en-funcion-de-si-son-fake-o-no}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# nº de palabras (tokens) *unicos* en el conjunto de textos clasificados como fake y en los no fake}

\NormalTok{Fake\_News\_Tokens.groupby(by}\OperatorTok{=}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{].nunique()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Fake
0    78020
1    85642
Name: token, dtype: int64
\end{verbatim}

\hypertarget{numero-de-tokens-en-cada-una-de-las-noticias-individualmente}{%
\subsection{Numero de tokens en cada una de las noticias
individualmente}\label{numero-de-tokens-en-cada-una-de-las-noticias-individualmente}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# nº de palabras (tokens) en cada texto individual clasificados como fake y en los no fake}

\NormalTok{df1 }\OperatorTok{=}\NormalTok{ pd.DataFrame( Fake\_News\_Tokens.groupby(by }\OperatorTok{=}\NormalTok{ [}\StringTok{"id\_text"}\NormalTok{ , }\StringTok{"Fake"}\NormalTok{] )[}\StringTok{"token"}\NormalTok{].count().rename(}\StringTok{\textquotesingle{}nº\_tokens\textquotesingle{}}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                     nº_tokens
id_text   Fake           
  0         1           447
  1         1           294
  2         1           563
  3         1           426
  4         1           415

 ...       ...          ...

44893       0           433
44894       0           120
44895       0           307
44896       0           196
44897       0           197
\end{verbatim}

Hay noticias que no tienen tokens :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1.loc[df1[}\StringTok{\textquotesingle{}nº\_tokens\textquotesingle{}}\NormalTok{] }\OperatorTok{==} \DecValTok{0}\NormalTok{, :]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                 nº_tokens
id_text  Fake           

9358      1           0
10923     1           0
11041     1           0
11190     1           0
11225     1           0

 ...     ...         ...

21857     1           0
21869     1           0
21870     1           0
21873     1           0
32451     0           0
\end{verbatim}

Algunos ejemplos de estas noticias son los siguientes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data.loc[Fake\_News\_Data.id\_text }\OperatorTok{==} \DecValTok{9358}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Fake   title  
9358     1     https://100percentfedup.com/served-roy-moore-v...   


        text  
9358    https://100percentfedup.com/served-roy-moore-v...   


        date  word_count  
9358              1   


      text_tokenizado   id_text  
9358        []           9358  
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data.loc[Fake\_News\_Data.id\_text }\OperatorTok{==} \DecValTok{10923}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       Fake      title 
10923    1       TAKE OUR POLL: Who Do You Think President Trum...        

       text     date            word_count   text_tokenizado   id_text  
10923           May 10, 2017        0              []           10923
\end{verbatim}

Nos quedamos por tanto solo con las noticias que tienen algun token :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df2 }\OperatorTok{=}\NormalTok{ df1.loc[df1[}\StringTok{\textquotesingle{}nº\_tokens\textquotesingle{}}\NormalTok{] }\OperatorTok{!=} \DecValTok{0}\NormalTok{, :]}

\NormalTok{df2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                 nº_tokens
id_text   Fake           

0          1           447
1          1           294
2          1           563
3          1           426
4          1           415

...       ...          ...

44893      0           433
44894      0           120
44895      0           307
44896      0           196
44897      0           197
\end{verbatim}

Calculamos el numero medio de tokens para las noticas que tienen uno o
mas tokens en funcion se si son fake o no:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df2.groupby(}\StringTok{"Fake"}\NormalTok{)[}\StringTok{"nº\_tokens"}\NormalTok{].agg([}\StringTok{\textquotesingle{}mean\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            mean
Fake            
0     368.486225
1     422.169983
\end{verbatim}

Se puede interpretar como la longitud media de las noticas fake y de las
no fake

\vspace{0.5cm}

Hay diferencias entre lo obtenido mediante esta operación y lo obtenido
al usar el siguiente código, que fue visto anteriormente:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data[}\StringTok{\textquotesingle{}word\_count\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ Fake\_News\_Data[}\StringTok{\textquotesingle{}text\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.split().}\BuiltInTok{str}\NormalTok{.}\BuiltInTok{len}\NormalTok{()}

\NormalTok{Fake\_News\_Data.groupby(}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}word\_count\textquotesingle{}}\NormalTok{].mean()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Fake   Mean word_count
    
0        385.640099
    
1        423.197905
\end{verbatim}

Y esto es debido a que el código
\texttt{Fake\_News\_Data{[}\textquotesingle{}text\textquotesingle{}{]}.str.split()}
hace una operacion similar a la realizada por nuestra funcion
\texttt{limpiar\_tokenizar} pero \textbf{no exactamente igual}, y esto
lleva a que con la primera opcion se obtiene un conjunto de tokens
diferente al obtenido con la funcion \texttt{limpiar\_tokenizar}, en los
distintos documentos, y esto lleva a que la longitud de los documentos
sea diferente si se consideran los tokens obtenidos con
\texttt{Fake\_News\_Data{[}\textquotesingle{}text\textquotesingle{}{]}.str.split()}
a si se usan los obtenidos con \texttt{limpiar\_tokenizar}, llo que
lleva a diferencias en las longitudes medias obtenidas.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Data[}\StringTok{\textquotesingle{}text\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.split()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0        [Donald, Trump, just, couldn, t, wish, all, Am...
1        [House, Intelligence, Committee, Chairman, Dev...
2        [On, Friday,, it, was, revealed, that, former,...
3        [On, Christmas, day,, Donald, Trump, announced...
4        [Pope, Francis, used, his, annual, Christmas, ...
                               ...                        
44893    [BRUSSELS, (Reuters), -, NATO, allies, on, Tue...
44894    [LONDON, (Reuters), -, LexisNexis,, a, provide...
44895    [MINSK, (Reuters), -, In, the, shadow, of, dis...
44896    [MOSCOW, (Reuters), -, Vatican, Secretary, of,...
44897    [JAKARTA, (Reuters), -, Indonesia, will, buy, ...
\end{verbatim}

Como se pueden ver con el código anterior se obtiene por ejemplo que `-'
y `, Donald' son tokens , cuando con la función
\texttt{limpiar\_tokenizar} no serían considerados un tokens.

\vspace{0.5cm}

Otra forma de calcular lo anterior:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m0 }\OperatorTok{=}\NormalTok{ ( Fake\_News\_Tokens.loc[Fake\_News\_Tokens[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{]}\OperatorTok{==}\DecValTok{0}\NormalTok{].groupby(}\StringTok{\textquotesingle{}id\_text\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{].count() ).mean()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OperatorTok{=}\NormalTok{ ( Fake\_News\_Tokens.loc[Fake\_News\_Tokens[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{]}\OperatorTok{==}\DecValTok{1}\NormalTok{].groupby(}\StringTok{\textquotesingle{}id\_text\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{].count() ).mean()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.DataFrame(\{}\StringTok{\textquotesingle{}fake\_new\textquotesingle{}}\NormalTok{: [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{] , }\StringTok{\textquotesingle{}tokens\_mean\textquotesingle{}}\NormalTok{:[m0 , m1]\})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   fake_new  tokens_mean
0         0   368.469020
1         1   409.332822
\end{verbatim}

\hypertarget{nuxfamero-de-veces-que-aparece-cada-token-en-el-conjunto-de-las-noticias-en-funcion-de-si-es-fake-o-no}{%
\subsection{Número de veces que aparece cada token en el conjunto de las
noticias en funcion de si es fake o
no}\label{nuxfamero-de-veces-que-aparece-cada-token-en-el-conjunto-de-las-noticias-en-funcion-de-si-es-fake-o-no}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(  (Fake\_News\_Tokens.groupby(by }\OperatorTok{=}\NormalTok{ [}\StringTok{"Fake"}\NormalTok{, }\StringTok{"token"}\NormalTok{] )[}\StringTok{"token"}\NormalTok{].count().unstack(fill\_value}\OperatorTok{=}\DecValTok{0}\NormalTok{).stack().reset\_index(name}\OperatorTok{=}\StringTok{\textquotesingle{}frecuencia\_token\textquotesingle{}}\NormalTok{)))}

\CommentTok{\# .unstack(fill\_value=0).stack() para que tambien aparezcan los tokens con count = 0 , si no solo aprecerian los que tienen count \textgreater{} 0.}

\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Fake       token  frecuencia_token
0          0          aa                22
1          0         aaa                 7
2          0  aaaaaaaand                 0
3          0   aaaaackkk                 0
4          0  aaaaapkfhk                 0

...      ...         ...               ...

251605     1        ””it                 0
251606     1      ””when                 0
251607     1         •if                 0
251608     1      $emoji1$               0
251609     1      $emoji2$               0
\end{verbatim}

La salida anterior nos da para cada token el numero de veces que aparece
en el conjunto de las fake news por un lado (Fake = 1), y por otro lado
en el conjunto de las no fake (Fake=0)

Veamos algunos ejemplos para tokens concretos:

En la siguiente salida vemos el nº de veces que aparece el token `yes'
en eñ conjunto de las fake news (1775), asi como en el conjunto de las
no fake news (336).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.loc[df[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{]}\OperatorTok{==}\StringTok{\textquotesingle{}yes\textquotesingle{}}\NormalTok{ , ] }\CommentTok{\# El token \textquotesingle{}yes\textquotesingle{} aprece 1775 veces en el conjunto de las fake news y 336 en el de las no fake news}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Fake token  frecuencia_token
116577     0   yes               336
242382     1   yes              1775
\end{verbatim}

En la siguiente salida vemos el nº de veces que aparece el token `true'
en eñ conjunto de las fake news (2595), asi como en el conjunto de las
no fake news (412).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.loc[df[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{]}\OperatorTok{==}\StringTok{\textquotesingle{}true\textquotesingle{}}\NormalTok{ , ] }\CommentTok{\# El token \textquotesingle{}true\textquotesingle{} aparece 2595 veces en el conjunto de las fake news y 412 en el de las no fake news}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Fake token  frecuencia_token
106608     0  true               412
232413     1  true              2595
\end{verbatim}

En la siguiente salida podemos ver el nº de veces que aparece cada token
en el conjunto de las no fake news.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.loc[df[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{]}\OperatorTok{==}\DecValTok{0}\NormalTok{ , ] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         Fake       token  frecuencia_token
         
0          0          aa                22
1          0         aaa                 7
2          0  aaaaaaaand                 0
3          0   aaaaackkk                 0
4          0  aaaaapkfhk                 0

...      ...         ...               ...

125800     0        ””it                 1
125801     0      ””when                 1
125802     0         •if                 3
125803     0      $emoji1$               3
125804     0      $emoji2$               1
\end{verbatim}

Y en la siguiente salida podemos ver el nº de veces que aparece cada
token en el conjunto de las fake news.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.loc[df[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{]}\OperatorTok{==}\DecValTok{1}\NormalTok{ , ] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Fake       token  frecuencia_token
125805     1          aa                24
125806     1         aaa                 9
125807     1  aaaaaaaand                 1
125808     1   aaaaackkk                 1
125809     1  aaaaapkfhk                 1
...      ...         ...               ...
251605     1        ””it                 0
251606     1      ””when                 0
251607     1         •if                 0
251608     1      $emoji1$               0
251609     1      $emoji2$               0
\end{verbatim}

Ahora vamos a ordenar los dos data-frames anteriores en función de la
columna \texttt{frecuencia\_token} , de mayor a menor, para así poder
ver cuales son los tokens con mayor frecuencia tanto en el conjunto de
las fake news, como en el de las no fake news.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_fake\_sort }\OperatorTok{=}\NormalTok{ df.loc[df[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{]}\OperatorTok{==}\DecValTok{1}\NormalTok{ , ].sort\_values(by}\OperatorTok{=}\NormalTok{[}\StringTok{"frecuencia\_token"}\NormalTok{], ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).reset\_index(drop}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_no\_fake\_sort }\OperatorTok{=}\NormalTok{ df.loc[df[}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{]}\OperatorTok{==}\DecValTok{0}\NormalTok{ , ].sort\_values(by}\OperatorTok{=}\NormalTok{[}\StringTok{"frecuencia\_token"}\NormalTok{], ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).reset\_index(drop}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Imprimimos las primeras 15 filas de cada uno de los nuevos data-frames
ordenados:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_fake\_sort.head(}\DecValTok{15}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     index  Fake  token  frecuencia_token
     
0   229301     1    the            544521
1   230713     1     to            290882
2   199217     1     of            236735
3   129697     1    and            227349
4   174372     1     in            171433
5   229261     1   that            151789
6   176603     1     is            111278
7   162672     1    for             93538
8   176868     1     it             83693
9   199777     1     on             83661
10  232444     1  trump             79922
11  169936     1     he             79124
12  238650     1    was             67865
13  240547     1   with             63441
14  229776     1   this             58581
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_no\_fake\_sort.head(}\DecValTok{15}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     index  Fake token  frecuencia_token
     
0   103496     0   the            478548
1   104908     0    to            245378
2    73412     0    of            205193
3     3892     0   and            181715
4    48567     0    in            181082
5    73972     0    on            108459
6    90350     0  said             99054
7   103456     0  that             86723
8    36867     0   for             79705
9    50798     0    is             55298
10  114742     0  with             54327
11   44131     0    he             52605
12  112845     0   was             47892
13   14219     0    by             47871
14    5659     0    as             46935
\end{verbatim}

Se puede observar que en ambas tablas la mayoria de los 15 tokens mas
frecuentees se corresponden con artículos, preposiciones, pronombres,
etc. En general, palabras que no aportan información relevante sobre el
texto. A estas palabras se les conoce como \textbf{stopwords}. Para cada
idioma existen distintos listados de stopwords, además, dependiendo del
contexto, puede ser necesario adaptar el listado. Con frecuencia, a
medida que se realiza un análisis se encuentran palabras que deben
incluirse en el listado de stopwords.

\hypertarget{stop-words}{%
\subsection{Stop words}\label{stop-words}}

Vamos a obtener un listado de \textbf{stopwords} en ingles, ya que
nuestros textos (noticias) están en ingles. Si estuvieran en varios
idiosmas habra que formar un listado de stopwords para todos esos
idomas.

Para ontener el listado de stopwords usaremos la libreria \texttt{nltk}
(Natural Language Toolki) , una de las librerias mas importantes en
\texttt{Python} en el área de procesamiento de lenguaje natural.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# pip install nltk}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ nltk}
\NormalTok{nltk.download(}\StringTok{\textquotesingle{}stopwords\textquotesingle{}}\NormalTok{)}
\ImportTok{from}\NormalTok{ nltk.corpus }\ImportTok{import}\NormalTok{ stopwords}
\end{Highlighting}
\end{Shaded}

Obtenemos el listado de stopwords que provee \texttt{nltk} para el
idioma inglés, y ademas le añadimos una lista extra de palabras que
también vamos a considerar stopwords:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Obtencion de listado de stopwords del ingles}

\NormalTok{stop\_words }\OperatorTok{=}\NormalTok{ stopwords.words(}\StringTok{\textquotesingle{}english\textquotesingle{}}\NormalTok{) }\OperatorTok{+}\NormalTok{ [}\StringTok{"pic"}\NormalTok{ , }\StringTok{"getty"}\NormalTok{, }\StringTok{"quot"}\NormalTok{, }\StringTok{"acr"}\NormalTok{, }\StringTok{"filessupport"}\NormalTok{, }\StringTok{"flickr"}\NormalTok{, }\StringTok{"fjs"}\NormalTok{, }\StringTok{"js"}\NormalTok{, }\StringTok{"somodevilla"}\NormalTok{, }\StringTok{"var"}\NormalTok{, }\StringTok{"henningsen"}\NormalTok{,}
\StringTok{"ck"}\NormalTok{, }\StringTok{"cdata"}\NormalTok{, }\StringTok{"subscribing"}\NormalTok{, }\StringTok{"mcnamee"}\NormalTok{, }\StringTok{"amp"}\NormalTok{, }\StringTok{"wfb"}\NormalTok{, }\StringTok{"screenshot"}\NormalTok{, }\StringTok{"hesher"}\NormalTok{,}\StringTok{"nyp"}\NormalTok{, }\StringTok{"cking"}\NormalTok{, }\StringTok{"helton"}\NormalTok{, }\StringTok{"raedle"}\NormalTok{, }\StringTok{"donnell"}\NormalTok{,}
\StringTok{"getelementbyid"}\NormalTok{, }\StringTok{"src"}\NormalTok{, }\StringTok{"behar"}\NormalTok{, }\StringTok{"createelement"}\NormalTok{, }\StringTok{"getelementsbytagname"}\NormalTok{, }\StringTok{"parentnode"}\NormalTok{, }\StringTok{"wnd"}\NormalTok{,}\StringTok{"insertbefore"}\NormalTok{,}
\StringTok{"jssdk"}\NormalTok{, }\StringTok{"nowicki"}\NormalTok{, }\StringTok{"xfbml"}\NormalTok{, }\StringTok{"camerota"}\NormalTok{, }\StringTok{"sdk"}\NormalTok{,  }\StringTok{"“i"}\NormalTok{ , }\StringTok{"“the"}\NormalTok{, }\StringTok{"“we"}\NormalTok{, }\StringTok{"it’s"}\NormalTok{, }\StringTok{"don’t"}\NormalTok{, }\StringTok{"“this"}\NormalTok{, }\StringTok{"“it"}\NormalTok{, }\StringTok{"“a"}\NormalTok{,}
\StringTok{"“if"}\NormalTok{,  }\StringTok{"“it’s"}\NormalTok{, }\StringTok{"we’re"}\NormalTok{, }\StringTok{"that’s"}\NormalTok{,  }\StringTok{"“he"}\NormalTok{, }\StringTok{"“there"}\NormalTok{, }\StringTok{"i’m"}\NormalTok{,  }\StringTok{"he’s"}\NormalTok{,  }\StringTok{"“we’re"}\NormalTok{, }\StringTok{"doesn’t"}\NormalTok{, }\StringTok{"can’t"}\NormalTok{, }\StringTok{"“i’m"}\NormalTok{, }\StringTok{"“in"}\NormalTok{,}
\StringTok{"suu"}\NormalTok{, }\StringTok{"“they"}\NormalTok{, }\StringTok{"you’re"}\NormalTok{, }\StringTok{"“but"}\NormalTok{, }\StringTok{"didn’t"}\NormalTok{, }\StringTok{"“you"}\NormalTok{, }\StringTok{"they’re"}\NormalTok{, }\StringTok{"“no"}\NormalTok{, }\StringTok{"“as"}\NormalTok{, }\StringTok{"“very"}\NormalTok{ , }\StringTok{"there’s"}\NormalTok{, }\StringTok{"“what"}\NormalTok{,  }\StringTok{"“and"}\NormalTok{, }\StringTok{"won’t"}\NormalTok{,}
  \StringTok{"“to"}\NormalTok{, }\StringTok{"“that"}\NormalTok{, }\StringTok{"“one"}\NormalTok{, }\StringTok{"we’ve"}\NormalTok{, }\StringTok{"“when"}\NormalTok{ , }\StringTok{"“our"}\NormalTok{, }\StringTok{"“not"}\NormalTok{, }\StringTok{"’”"}\NormalTok{ ,}\StringTok{"“that’s"}\NormalTok{, }\StringTok{"“these"}\NormalTok{, }\StringTok{"“there’s"}\NormalTok{, }\StringTok{"“he’s"}\NormalTok{, }\StringTok{"we’ll"}\NormalTok{, }\StringTok{\textquotesingle{}one\textquotesingle{}}\NormalTok{,}
   \StringTok{\textquotesingle{}would\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}like\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}us\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}even\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}could\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}two\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}many\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}angerer\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}reilly\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Imprimimos la lista de stopwords que se van a considerar en este
trabajo:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(stop\_words)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't", 'pic', 'getty', 'quot', 'acr', 'filessupport', 'flickr', 'fjs', 'js', 'somodevilla', 'var', 'henningsen', 'ck', 'cdata', 'subscribing', 'mcnamee', 'amp', 'wfb', 'screenshot', 'hesher', 'nyp', 'cking', 'helton', 'raedle', 'donnell', 'getelementbyid', 'src', 'behar', 'createelement', 'getelementsbytagname', 'parentnode', 'wnd', 'insertbefore', 'jssdk', 'nowicki', 'xfbml', 'camerota', 'sdk', '“i', '“the', '“we', 'it’s', 'don’t', '“this', '“it', '“a', '“if', '“it’s', 'we’re', 'that’s', '“he', '“there', 'i’m', 'he’s', '“we’re', 'doesn’t', 'can’t', '“i’m', '“in', 'suu', '“they', 'you’re', '“but', 'didn’t', '“you', 'they’re', '“no', '“as', '“very', 'there’s', '“what', '“and', 'won’t', '“to', '“that', '“one', 'we’ve', '“when', '“our', '“not', '’”', '“that’s', '“these', '“there’s', '“he’s', 'we’ll', 'one', 'would', 'like', 'us', 'even', 'could', 'two', 'many', 'angerer', 'reilly']
    
\end{verbatim}

De los data-frames \texttt{df\_fake\_sort} y \texttt{df\_no\_fake\_sort}
eliminamos aquellos tokens que estan en la lista de stopwords:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_fake\_sort\_not\_StopWords }\OperatorTok{=}\NormalTok{ df\_fake\_sort[ }\OperatorTok{\textasciitilde{}}\NormalTok{ df\_fake\_sort[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{].isin(stop\_words) ] }\CommentTok{\# ranking de tokens para las fake news sin stop words}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_no\_fake\_sort\_not\_StopWords }\OperatorTok{=}\NormalTok{ df\_no\_fake\_sort[ }\OperatorTok{\textasciitilde{}}\NormalTok{ df\_no\_fake\_sort[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{].isin(stop\_words) ] }\CommentTok{\# ranking de tokens para las no fake news sin stop words}
\end{Highlighting}
\end{Shaded}

Imprimimos las primeras 15 filas de los nuevos data-frames creados:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_fake\_sort\_not\_StopWords.head(}\DecValTok{15}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     index  Fake      token  frecuencia_token
     
10  232444     1      trump             79922
31  216155     1       said             33763
34  206880     1  president             27801
35  203392     1     people             26591
56  144568     1    clinton             19209
59  198761     1      obama             18833
62  154174     1     donald             17789
67  128977     1       also             15420
69  196554     1       news             14688
73  196507     1        new             14414
75  171064     1    hillary             14184
77  230293     1       time             13854
79  224427     1      state             13471
82  239806     1      white             13194
84  237031     1        via             12830
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_no\_fake\_sort\_not\_StopWords.head(}\DecValTok{15}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     index  Fake       token  frecuencia_token
     
6    90350     0        said             99054
17  106639     0       trump             42755
26   87534     0     reuters             28880
28   81075     0   president             27128
36   98622     0       state             19912
41   41076     0  government             18484
44   70702     0         new             16849
47   46493     0       house             16480
48   98655     0      states             16380
49   86922     0  republican             16175
50    3172     0        also             15948
51  109089     0      united             15584
53   77587     0      people             14945
54  116463     0        year             14276
55  105051     0        told             14245
\end{verbatim}

\hypertarget{ranking-de-tokens-mas-frecuentes-en-el-conjunto-de-las-noticas-en-funcion-de-si-son-fake-y-no-fake-tras-eliminar-stopwords}{%
\subsection{Ranking de tokens mas frecuentes en el conjunto de las
noticas en funcion de si son fake y no fake tras eliminar
stopwords}\label{ranking-de-tokens-mas-frecuentes-en-el-conjunto-de-las-noticas-en-funcion-de-si-son-fake-y-no-fake-tras-eliminar-stopwords}}

Una vez eliminadas las stopwords vamos a crear unos graficos de barras
para representar el ranking de los 15 tokens mas frecuentes en el
conjunto de las fake news por un lado, y por otro las no fake news:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}

\NormalTok{sns.barplot(data}\OperatorTok{=}\NormalTok{df\_fake\_sort\_not\_StopWords.head(}\DecValTok{15}\NormalTok{), x}\OperatorTok{=}\StringTok{\textquotesingle{}frecuencia\_token\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tomato\textquotesingle{}}\NormalTok{).}\BuiltInTok{set}\NormalTok{(title}\OperatorTok{=}\StringTok{\textquotesingle{}Ranking 15 Tokens in Fake News\textquotesingle{}}\NormalTok{) }

\NormalTok{fig.savefig(}\StringTok{\textquotesingle{}p1.png\textquotesingle{}}\NormalTok{, }\BuiltInTok{format}\OperatorTok{=}\StringTok{\textquotesingle{}png\textquotesingle{}}\NormalTok{, dpi}\OperatorTok{=}\DecValTok{1200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{p1.png}
\caption{Ranking 15 Tokens in Fake News}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}

\NormalTok{sns.barplot(data}\OperatorTok{=}\NormalTok{df\_no\_fake\_sort\_not\_StopWords.head(}\DecValTok{15}\NormalTok{), x}\OperatorTok{=}\StringTok{\textquotesingle{}frecuencia\_token\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}cyan\textquotesingle{}}\NormalTok{).}\BuiltInTok{set}\NormalTok{(title}\OperatorTok{=}\StringTok{\textquotesingle{}Ranking 15 Tokens in Not Fake News\textquotesingle{}}\NormalTok{) }

\NormalTok{fig.savefig(}\StringTok{\textquotesingle{}p2.png\textquotesingle{}}\NormalTok{, }\BuiltInTok{format}\OperatorTok{=}\StringTok{\textquotesingle{}png\textquotesingle{}}\NormalTok{, dpi}\OperatorTok{=}\DecValTok{1200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{p2.png}
\caption{Ranking 15 Tokens in Not Fake News}
\end{figure}

\hypertarget{odds-ratio}{%
\subsection{Odds Ratio}\label{odds-ratio}}

A continuación, se estudia qué palabras se utilizan de forma más
diferenciada en cada tipo de noticia (fake / no fake), es decir,
palabras que utiliza mucho en las fake news y que no se utilizan tanto
en las no fakes, y viceversa.

Una forma de hacer este análisis es mediante el odds ratio de las
frecuencias.

Sea \(\hspace{0.2cm}p_k1 = \cfrac{n_{k1} + 1}{N_1 + 1}\hspace{0.2cm}\) y
\(\hspace{0.2cm}p_k0 = \cfrac{n_{k0} + 1}{N_0 +1}\)

\[OR(Fake|NoFake , k) = \dfrac{ p_{k1} }{ p_{k0} }\]

Donde:

\(n_{k1}\) el número de veces que aparece el token \(k\) en las
\textbf{fake news}.

\(n_{k0}\) el numero de veces que aparece el termino \(k\) en las
\textbf{no fake news}.

\(N_1\) es el número de tokens, contando repeticiones, que aparecen en
las \textbf{fake news}.

\(N_0\) es el número de tokens, contando repeticiones, que aparecen en
las \textbf{no fake news}

Por tanto:

\(p_{k1} \approx\) proporcion de apariciones del token \(k\) en las
\textbf{fake news}

\(p_{k0} \approx\) proporcion de apariciones del token \(k\) en las
\textbf{no fake news}

Si \(OddsRatio(k) = \dfrac{ p_k1 }{ p_k0 } = h\) , entonces:

Si \(h>1\) \(\Rightarrow\) el token \(k\) es \(h\) veces mas frecuente
en las \textbf{fake news} que en las \textbf{no fake news}, ya que
\(p_{k1} = h \cdot p_{k0}\)

Si \(h \in (0 , 1)\) \(\Rightarrow\) el token \(k\) es \(1/h\) veces mas
frecuente en las \textbf{no fake news} que en las \textbf{fake news}, ya
que \(p_{k0} = (1/h) \cdot p_{k1}\) , donde \((1/h)>1\)

Si \(h= 1\) \(\Rightarrow\) el token \(k\) es igual de frecuente en las
\textbf{fake news} que en las \textbf{no fake news}, ya que
\(p_{k1} = p_{k0}\)

A continuacion definimos funciones para calcular \(n_{k1}\) y \(n_{k0}\)
en \texttt{Python}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ n\_k1(token) : }

\NormalTok{    n\_k1 }\OperatorTok{=}\NormalTok{ df\_fake\_sort\_not\_StopWords.loc[ df\_fake\_sort\_not\_StopWords[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{]}\OperatorTok{==}\NormalTok{token , }\StringTok{\textquotesingle{}frecuencia\_token\textquotesingle{}}\NormalTok{]}

    \ControlFlowTok{return}\NormalTok{(n\_k1)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ n\_k0(token) : }

\NormalTok{    n\_k0 }\OperatorTok{=}\NormalTok{ df\_no\_fake\_sort\_not\_StopWords.loc[ df\_no\_fake\_sort\_not\_StopWords[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{]}\OperatorTok{==}\NormalTok{token , }\StringTok{\textquotesingle{}frecuencia\_token\textquotesingle{}}\NormalTok{]}

    \ControlFlowTok{return}\NormalTok{(n\_k0)}
\end{Highlighting}
\end{Shaded}

Probamos las funciones para algunos tokens concretos:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_k0(}\StringTok{\textquotesingle{}trump\textquotesingle{}}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
17    42755
Name: frecuencia_token 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_k1(}\StringTok{\textquotesingle{}trump\textquotesingle{}}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
10    79922
Name: frecuencia_token 
\end{verbatim}

Estas salidas nos indican que el nº de veces que aparece el token
`trump' en el conjunto de las fake news es 79922 , mientras que en el
conjunto de las no fake news es 42755.

\(N_0\) y \(N_1\) coinciden con el nº de tokens, contando repeticiones y
sin considerar las stopwords, que aparecen el las no fake y fake news,
respectivamente:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Tokens\_not\_StopWords }\OperatorTok{=}\NormalTok{ Fake\_News\_Tokens[ }\OperatorTok{\textasciitilde{}}\NormalTok{ Fake\_News\_Tokens[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{].isin(stop\_words) ]}

\NormalTok{Fake\_News\_Tokens\_not\_StopWords}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       id_text       token Fake
       
0            0      donald    1
0            0       trump    1
0            0        wish    1
0            0   americans    1
0            0       happy    1

...        ...         ...  ...

44897    44897      energy    0
44897    44897  technology    0
44897    44897    aviation    0
44897    44897       among    0
44897    44897      others    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fake\_News\_Tokens\_not\_StopWords.groupby(by}\OperatorTok{=}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{].count()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Fake
0    4782198
1    5396339

Name: token
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N0 }\OperatorTok{=}\NormalTok{ Fake\_News\_Tokens\_not\_StopWords.groupby(by}\OperatorTok{=}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{].count()[}\DecValTok{0}\NormalTok{]}

\NormalTok{N1 }\OperatorTok{=}\NormalTok{ Fake\_News\_Tokens\_not\_StopWords.groupby(by}\OperatorTok{=}\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{].count()[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
4782198
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
5396339
\end{verbatim}

Como ejemplo vamos a calcular el Odds Ratio fake - no fake para el toke
`trump' :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_k0(}\StringTok{\textquotesingle{}trump\textquotesingle{}}\NormalTok{) }\OperatorTok{/}\NormalTok{ N0 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
17    0.00894
Name: frecuencia_token, dtype: float64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_k1(}\StringTok{\textquotesingle{}trump\textquotesingle{}}\NormalTok{) }\OperatorTok{/}\NormalTok{ N1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
10    0.01481
Name: frecuencia_token, dtype: float64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Odds Ratio fake {-} no fake para el token \textquotesingle{}trump\textquotesingle{}}

\BuiltInTok{float}\NormalTok{( n\_k0(}\StringTok{\textquotesingle{}trump\textquotesingle{}}\NormalTok{) }\OperatorTok{/}\NormalTok{ N0 ) }\OperatorTok{/} \BuiltInTok{float}\NormalTok{( n\_k1(}\StringTok{\textquotesingle{}trump\textquotesingle{}}\NormalTok{) }\OperatorTok{/}\NormalTok{ N1 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1.6565622548396417
\end{verbatim}

Por tanto el token `trump' es 1.66 veces mas frecuente en las fake news
que en las no fake.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 }\OperatorTok{=}\NormalTok{ df\_fake\_sort\_not\_StopWords.sort\_values(by}\OperatorTok{=}\NormalTok{[}\StringTok{"token"}\NormalTok{]).reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{df1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         index  Fake       token  frecuencia_token
0       125805     1          aa                24
1       125806     1         aaa                 9
2       125807     1  aaaaaaaand                 1
3       125808     1   aaaaackkk                 1
4       125809     1  aaaaapkfhk                 1
...        ...   ...         ...               ...
125561  251605     1        ””it                 0
125562  251606     1      ””when                 0
125563  251607     1          if                 0
125564  251608     1      emoji1                 0
125565  251609     1      emoji2                 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df0 }\OperatorTok{=}\NormalTok{ df\_no\_fake\_sort\_not\_StopWords.sort\_values(by}\OperatorTok{=}\NormalTok{[}\StringTok{"token"}\NormalTok{]).reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{df0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         index  Fake       token  frecuencia_token
0            0     0          aa                22
1            1     0         aaa                 7
2            2     0  aaaaaaaand                 0
3            3     0   aaaaackkk                 0
4            4     0  aaaaapkfhk                 0
...        ...   ...         ...               ...
125561  125800     0        ””it                 1
125562  125801     0      ””when                 1
125563  125802     0          if                 3
125564  125803     0       emoji1                3
125565  125804     0       emoji2                1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{n\_k0\_vector }\OperatorTok{=}\NormalTok{ df0[}\StringTok{\textquotesingle{}frecuencia\_token\textquotesingle{}}\NormalTok{]}

\NormalTok{n\_k1\_vector }\OperatorTok{=}\NormalTok{ df1[}\StringTok{\textquotesingle{}frecuencia\_token\textquotesingle{}}\NormalTok{]}


\NormalTok{Odds\_ratio }\OperatorTok{=}\NormalTok{ ( ( n\_k1\_vector }\OperatorTok{+} \DecValTok{1}\NormalTok{ ) }\OperatorTok{/}\NormalTok{ ( N1 }\OperatorTok{+} \DecValTok{1}\NormalTok{) ) }\OperatorTok{/}\NormalTok{ ( ( n\_k0\_vector }\OperatorTok{+} \DecValTok{1}\NormalTok{ ) }\OperatorTok{/}\NormalTok{ ( N0 }\OperatorTok{+} \DecValTok{1}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df0[}\StringTok{\textquotesingle{}Odds\_ratio\_Fake\_NotFake\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ Odds\_ratio  }
\NormalTok{df1[}\StringTok{\textquotesingle{}Odds\_ratio\_Fake\_NotFake\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ Odds\_ratio  }

\NormalTok{df0[}\StringTok{\textquotesingle{}Odds\_ratio\_NotFake\_Fake\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \DecValTok{1} \OperatorTok{/}\NormalTok{ df0[}\StringTok{"Odds\_ratio\_Fake\_NotFake"}\NormalTok{] }
\NormalTok{df1[}\StringTok{\textquotesingle{}Odds\_ratio\_NotFake\_Fake\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \DecValTok{1} \OperatorTok{/}\NormalTok{ df1[}\StringTok{"Odds\_ratio\_Fake\_NotFake"}\NormalTok{]  }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         index  Fake       token  frecuencia_token  Odds_ratio_Fake_NotFake  
0            0     0          aa                22                 0.963253   
1            1     0         aaa                 7                 1.107741   
2            2     0  aaaaaaaand                 0                 1.772386   
3            3     0   aaaaackkk                 0                 1.772386   
4            4     0  aaaaapkfhk                 0                 1.772386   
...        ...   ...         ...               ...                      ...   
125561  125800     0        ””it                 1                 0.443097   
125562  125801     0      ””when                 1                 0.443097   
125563  125802     0          if                 3                 0.221548   
125564  125803     0    $emoji1$                 3                 0.221548   
125565  125804     0    $emoji2$                 1                 0.443097   

        Odds_ratio_NotFake_Fake  
0                      1.038149  
1                      0.902738  
2                      0.564211  
3                      0.564211  
4                      0.564211  
...                         ...  
125561                 2.256845  
125562                 2.256845  
125563                 4.513689  
125564                 4.513689  
125565                 2.256845  
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         index  Fake       token  frecuencia_token  Odds_ratio_Fake_NotFake  \
0       125805     1          aa                24                 0.963253   
1       125806     1         aaa                 9                 1.107741   
2       125807     1  aaaaaaaand                 1                 1.772386   
3       125808     1   aaaaackkk                 1                 1.772386   
4       125809     1  aaaaapkfhk                 1                 1.772386   
...        ...   ...         ...               ...                      ...   
125561  251605     1        ””it                 0                 0.443097   
125562  251606     1      ””when                 0                 0.443097   
125563  251607     1          if                 0                 0.221548   
125564  251608     1    $emoji1$                 0                 0.221548   
125565  251609     1    $emoji2$                 0                 0.443097   

        Odds_ratio_NotFake_Fake  
0                      1.038149  
1                      0.902738  
2                      0.564211  
3                      0.564211  
4                      0.564211  
...                         ...  
125561                 2.256845  
125562                 2.256845  
125563                 4.513689  
125564                 4.513689  
125565                 2.256845  
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df0.sort\_values(by}\OperatorTok{=}\NormalTok{[}\StringTok{"Odds\_ratio\_Fake\_NotFake"}\NormalTok{], ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    index  Fake            token  frecuencia_token  Odds_ratio_Fake_NotFake  
0   35830     0          finicum                 0               320.801884   
1  114264     0        wikimedia                 0               200.279629   
2  109040     0  uninterruptible                 0               189.645313   
3   78372     0     philosophers                 0               186.100540   
4   60711     0          lovable                 0               183.441961   

   Odds_ratio_NotFake_Fake  
0                 0.003117  
1                 0.004993  
2                 0.005273  
3                 0.005373  
4                 0.005451
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df0.sort\_values(by}\OperatorTok{=}\NormalTok{[}\StringTok{"Odds\_ratio\_NotFake\_Fake"}\NormalTok{], ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    index  Fake      token  frecuencia_token  Odds_ratio_Fake_NotFake  
0  106864     0    trump’s             11629                 0.000076   
1   72989     0    obama’s              2132                 0.000415   
2   18791     0  clinton’s              1604                 0.000552   
3   76630     0    party’s              1101                 0.000804   
4   98675     0    state’s              1010                 0.000877   

   Odds_ratio_NotFake_Fake  
0             13123.551362  
1              2406.924768  
2              1811.117793  
3              1243.521376  
4              1140.834946
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1.sort\_values(by}\OperatorTok{=}\NormalTok{[}\StringTok{"Odds\_ratio\_Fake\_NotFake"}\NormalTok{], ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    index  Fake            token  frecuencia_token  Odds_ratio_Fake_NotFake  
0  161635     1          finicum               361               320.801884   
1  240069     1        wikimedia               225               200.279629   
2  234845     1  uninterruptible               213               189.645313   
3  204177     1     philosophers               209               186.100540   
4  186516     1          lovable               206               183.441961   

   Odds_ratio_NotFake_Fake  
0                 0.003117  
1                 0.004993  
2                 0.005273  
3                 0.005373  
4                 0.005451
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1.sort\_values(by}\OperatorTok{=}\NormalTok{[}\StringTok{"Odds\_ratio\_NotFake\_Fake"}\NormalTok{], ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    index  Fake      token  frecuencia_token  Odds_ratio_Fake_NotFake  
0  232669     1    trump’s                 0                 0.000076   
1  198794     1    obama’s                 0                 0.000415   
2  144596     1  clinton’s                 0                 0.000552   
3  202435     1    party’s                 0                 0.000804   
4  224480     1    state’s                 0                 0.000877   

   Odds_ratio_NotFake_Fake  
0             13123.551362  
1              2406.924768  
2              1811.117793  
3              1243.521376  
4              1140.834946 
\end{verbatim}

Notese que en ambos data sets las columnas Odds\_ratio\_Fake\_NotFake y
Odds\_ratio\_NotFake\_Fake son las mismas, por tanto podemos construir
un nuevo data set solo con esas columnas y otra para los tokens, a
partir de cualquiera de esos dos data-sets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Odds\_ratio\_df }\OperatorTok{=}\NormalTok{ df1.loc[: , [}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Odds\_ratio\_Fake\_NotFake\textquotesingle{}}\NormalTok{ , }\StringTok{\textquotesingle{}Odds\_ratio\_NotFake\_Fake\textquotesingle{}}\NormalTok{]]  }

\NormalTok{Odds\_ratio\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             token  Odds_ratio_Fake_NotFake  Odds_ratio_NotFake_Fake
0               aa                 0.963253                 1.038149
1              aaa                 1.107741                 0.902738
2       aaaaaaaand                 1.772386                 0.564211
3        aaaaackkk                 1.772386                 0.564211
4       aaaaapkfhk                 1.772386                 0.564211
...            ...                      ...                      ...
125561        ””it                 0.443097                 2.256845
125562      ””when                 0.443097                 2.256845
125563          if                 0.221548                 4.513689
125564    $emoji2$                 0.221548                 4.513689
125565    $emoji1$                 0.443097                 2.256845
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Odds\_ratio\_df.sort\_values(by}\OperatorTok{=}\NormalTok{[}\StringTok{"Odds\_ratio\_Fake\_NotFake"}\NormalTok{], ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).head(}\DecValTok{15}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                  token  Odds_ratio_Fake_NotFake  Odds_ratio_NotFake_Fake
35775           finicum               320.801884                 0.003117
114071        wikimedia               200.279629                 0.004993
108870  uninterruptible               189.645313                 0.005273
78242      philosophers               186.100540                 0.005373
60612           lovable               183.441961                 0.005451
91113           savants               182.555768                 0.005478
67583         moralists               182.555768                 0.005478
97785             spore               182.555768                 0.005478
84324           rascals               181.669575                 0.005504
32976       evangelists               181.669575                 0.005504
63302        masochists               181.669575                 0.005504
11482            boiler               172.586096                 0.005794
13727             bundy               170.813710                 0.005854
92025        screengrab               167.490486                 0.005970
113747           whined               166.604293                 0.006002
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Odds\_ratio\_df.sort\_values(by}\OperatorTok{=}\NormalTok{[}\StringTok{"Odds\_ratio\_NotFake\_Fake"}\NormalTok{], ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).head(}\DecValTok{15}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                   token  Odds_ratio_Fake_NotFake  Odds_ratio_NotFake_Fake
106696           trump’s                 0.000076             13123.551362
72874            obama’s                 0.000415              2406.924768
18756          clinton’s                 0.000552              1811.117793
76500            party’s                 0.000804              1243.521376
98529            state’s                 0.000877              1140.834946
80975        president’s                 0.000979              1021.222183
83999            rakhine                 0.000987              1013.323226
1242    administration’s                 0.001157               864.371483
88673           rohingya                 0.001294               772.969276
117944              zuma                 0.001298               770.712432
82344         puigdemont                 0.001372               728.960807
17524            china’s                 0.001400               714.291317
89715           russia’s                 0.001439               695.108137
21888          country’s                 0.001541               648.842823
69047            myanmar                 0.001579               633.496280
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}

\NormalTok{sns.barplot(data}\OperatorTok{=}\NormalTok{Odds\_ratio\_df.sort\_values(by}\OperatorTok{=}\NormalTok{[}\StringTok{"Odds\_ratio\_Fake\_NotFake"}\NormalTok{], ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).head(}\DecValTok{15}\NormalTok{) ,}
\NormalTok{                 x}\OperatorTok{=}\StringTok{\textquotesingle{}Odds\_ratio\_Fake\_NotFake\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tomato\textquotesingle{}}\NormalTok{).}\BuiltInTok{set}\NormalTok{(title}\OperatorTok{=}\StringTok{\textquotesingle{}Ranking 15 most representative tokens in Fake News\textquotesingle{}}\NormalTok{) }

\NormalTok{fig.savefig(}\StringTok{\textquotesingle{}p3.png\textquotesingle{}}\NormalTok{, }\BuiltInTok{format}\OperatorTok{=}\StringTok{\textquotesingle{}png\textquotesingle{}}\NormalTok{, dpi}\OperatorTok{=}\DecValTok{1200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{p3.png}
\caption{Ranking de los 15 tokens mas representativos de las Fake News}
\end{figure}

Vamos a hacer un pequeño analisis de los tokens que son los mas
representativos para las fake news, es decir, aquellos tokens con mayor
odds ratio fake - no fake, esto es, aquellos que son mucho mas
frecuentes en las fake news que en las no fake news.

\begin{itemize}
\item
  El token mas representativa de las fake news analizadas es `finicum'

  \begin{itemize}
  \tightlist
  \item
    El token `finicum' podria hace referencia a Robert LaVoy Finicum,
    que según
    \href{https://en.wikipedia.org/wiki/LaVoy_Finicum}{Wikipedia} fue
    uno de los militantes estadounidenses que organizaron una ocupación
    armada del Refugio Nacional de Vida Silvestre Malheur en enero de
    2016. Después de que comenzó, la fuerza de ocupación se organizó
    como Ciudadanos por la Libertad Constitucional. , de la que Finicum
    fue portavoz. Fue la única víctima mortal de la ocupación. El 26 de
    enero de 2016, agentes del orden público intentaron arrestar a
    Finicum y a otros líderes de la ocupación mientras viajaban por una
    carretera remota para reunirse con simpatizantes en el condado
    vecino. Cuando el camión de Finicum finalmente fue detenido por una
    barricada, salió del vehículo hacia la nieve profunda y le
    dispararon, pero los oficiales fallaron. Finicum hizo dos
    movimientos a su chaqueta mientras le gritaba a la policía que
    tendrían que dispararle. Luego, Finicum fue asesinado a tiros. Más
    tarde, los oficiales encontraron un arma cargada en su bolsillo.
  \end{itemize}
\item
  El segundo token mas representativo de las fakes news es `wikipedia'

  \begin{itemize}
  \tightlist
  \item
    La Fundación Wikimedia (en inglés: Wikimedia Foundation, Inc.) es
    una organización sin ánimo de lucro. Es la organización matriz de
    Wikipedia
  \end{itemize}
\item
  El tercer token mas representativo de las fakes news es
  `uninterruptible' que significa `ininterrumpible' en español.
\item
  El cuarto token mas representativo de las fakes news es `philosophers'
  que significa `filosofos' en español.
\item
  El quinto token mas representativo de las fakes news es `lovable' que
  significa `amable' en español.
\item
  El sexto token mas representativo de las fakes news es `savants' que
  significa `sabios' en español.
\item
  El septimo token mas representativo de las fakes news es `moralist'
  que significa `moralistas' en español.
\item
  El octavo token mas representativo de las fakes news es `spore' que
  significa `espora' en español.
\item
  El noveno token mas representatico de las fake news es `rascals' que
  podria hacer alusion a la pelicula `The Little Rascals' en la cual
  participo como parte del elenco de actores el ex-presidente de Estados
  Unidos Donald Trump.
\item
  El decimo token mas representativo es `evangelist' que significa
  `evangelistas' en español.
\item
  El undecimo token mas representativo es `masochist' que significa
  `masoquista' en español.
\item
  El duodecimo token mas representativo es `boiler' que significa
  `caldera' en español.
\item
  El decimotercero token mas representativo es `bundy' que significa
  `paquete' en español.
\item
  El decimocuarto token mas representativo es `screengrab' que significa
  `captura de pantalla' en español.
\item
  El decimoquinto token mas representativo es `whined' que significa
  `quejarse' en español.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}

\NormalTok{sns.barplot(data}\OperatorTok{=}\NormalTok{Odds\_ratio\_df.sort\_values(by}\OperatorTok{=}\NormalTok{[}\StringTok{"Odds\_ratio\_NotFake\_Fake"}\NormalTok{], ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).head(}\DecValTok{15}\NormalTok{) ,}
\NormalTok{                 x}\OperatorTok{=}\StringTok{\textquotesingle{}Odds\_ratio\_NotFake\_Fake\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}cyan\textquotesingle{}}\NormalTok{).}\BuiltInTok{set}\NormalTok{(title}\OperatorTok{=}\StringTok{\textquotesingle{}Ranking 15 most representative tokens in Not Fake News\textquotesingle{}}\NormalTok{) }
                 
\NormalTok{fig.savefig(}\StringTok{\textquotesingle{}p4.png\textquotesingle{}}\NormalTok{, }\BuiltInTok{format}\OperatorTok{=}\StringTok{\textquotesingle{}png\textquotesingle{}}\NormalTok{, dpi}\OperatorTok{=}\DecValTok{1200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{p4.png}
\caption{Ranking de los 15 tokens mas representativos de las No Fake
News}
\end{figure}

Vamos a hacer un pequeño analisis de los tokens que son los mas
representativos para las fake news, es decir, aquellos tokens con mayor
odds ratio fake - no fake, esto es, aquellos que son mucho mas
frecuentes en las fake news que en las no fake news.

\begin{itemize}
\item
  El token mas representativa de las fake news analizadas es `trump's'

  \begin{itemize}
  \tightlist
  \item
    Este token hace referencia a Donald John Trump es un empresario,
    director ejecutivo, inversor en bienes inmuebles, personalidad
    televisiva y político estadounidense que ejerció como el 45.º
    presidente de los Estados Unidos de América desde el 20 de enero de
    2017 hasta el 20 de enero de 2021
  \end{itemize}
\item
  El segundo token mas representativo de las fakes news es `obama's'

  \begin{itemize}
  \tightlist
  \item
    Este token hace referencia a Barack Hussein Obama es un político
    estadounidense que ejerció como el 44.º presidente de los Estados
    Unidos de América desde el 20 de enero de 2009 hasta el 20 de enero
    de 2017
  \end{itemize}
\item
  El tercer token mas representativo de las fakes news es `clinton's'

  \begin{itemize}
  \tightlist
  \item
    William Jefferson Clinton es un político y abogado estadounidense
    que ejerció como el 42.º presidente de los Estados Unidos de América
    de 1993 a 2001
  \end{itemize}
\item
  El cuarto token mas representativo de las fakes news es `party's' que
  hace referencia a `partidos politicos' en español.
\item
  El quinto token mas representativo de las fakes news es `state's' que
  significa `estados' en español.
\item
  El sexto token mas representativo de las fakes news es `president's'
  que significa `presidentes' en español.
\item
  El septimo token mas representativo de las fakes news es `rakhine'

  \begin{itemize}
  \tightlist
  \item
    Rakhine es un estado de Birmania.
  \end{itemize}
\item
  El octavo token mas representativo de las fakes news es
  `administration's' que significa `gobierno' en español.
\item
  El noveno token mas representatico de las fake news es `rohingya'

  \begin{itemize}
  \tightlist
  \item
    Los rohingya son un grupo étnico musulmán de Birmania (Myanmar) que
    desde 2017 fue objeto, según la ONU, de una limpieza étnica por
    parte de las autoridades birmanas que obligó a la mayoría de sus
    integrantes a refugiarse en la vecina Bangladés.
  \end{itemize}
\item
  El decimo token mas representativo es `zuma' que podria hacer
  referencia a Jacob Zuma

\begin{verbatim}
- Jacob Gedleyihlekisa Zuma  es un político sudafricano que ejerció como el cuarto Presidente de la República de Sudáfrica. Fue también presidente del Congreso Nacional Africano (ANC), partido político en el poder en la República de Sudáfrica, elegido por la Conferencia Nacional el 18 de diciembre de 2007, puesto que mantuvo hasta diciembre de 2017. Zuma fue vicepresidente de la República de 1999 a 2005. El 14 de febrero de 2018 renunció a su cargo como presidente.
\end{verbatim}
\item
  El undecimo token mas representativo es `puigdemont' que hace
  referencia a Carles Puigdemont

\begin{verbatim}
     - Carles Puigdemont i Casamajón es un político y periodista español, diputado al Parlamento Europeo y presidente de la Generalidad de Cataluña entre 2016 y 2017. Durante su mandato como presidente de la Generalidad se impulsó la celebración ilegal del referéndum de independencia de Cataluña el 1 de octubre de 2017 y se efectuó una pretendida declaración unilateral de independencia del territorio el día 27 del mismo mes. Así, fue cesado en el cargo el 28 de octubre de 2017, al amparo de la Orden PRA/1034/2017, de 27 de octubre, en aplicación del artículo 155 de la Constitución española de 1978. Fue puesto entonces en busca y captura en territorio español acusado de presuntos delitos de rebelión, sedición y malversación de caudales públicos por actos que se le imputan en la organización del referéndum y la declaración unilateral de independencia
\end{verbatim}
\item
  El duodecimo token mas representativo es `china's' que hace referencia
  a China.
\item
  El decimotercero token mas representativo es `russia's' que hace
  referencia a Rusia.
\item
  El decimocuarto token mas representativo es `county's' que significa
  `país' en español.
\item
  El decimoquinto token mas representativo es `myanmar'

\begin{verbatim}
 - Birmania o Myanmar  denominada oficialmente República de la Unión de Myanmar, es un Estado soberano del Sudeste Asiático. 
\end{verbatim}
\end{itemize}

\begin{quote}
La interpretacion de estos términos no es una tarea en la que
necesariamente el cientifico de datos que los ha obtenido pueda aportar
un gran valor. Desde mi punto de vista, es mas adecuado que la
interpretacion de estas palabras se haga por parte de expertos en el
contexto en el que se encuentran las noticias analizadas, a saber, en el
contexto politico, economico y social que rodeaba a Estados Unidos entre
los años 2015 y 2018.
\end{quote}

\hypertarget{term-frequency-inverse-document-frequency-tf---idf}{%
\section{Term frequency -- Inverse document frequency (Tf -
Idf)}\label{term-frequency-inverse-document-frequency-tf---idf}}

Siguiendo a
\href{https://www.linkedin.com/in/joaquin-amat-rodrigo/?originalSubdomain=es}{Joaquin
Amat Rodrigo} , creador de
\href{https://www.cienciadedatos.net/}{Cienciadedatos.net} y la entrada
de \href{https://es.wikipedia.org/wiki/Tf-idf}{Wikipedia}

Uno de los principales intereses en \textbf{text mining},
\textbf{natural language processing} e \textbf{information retrieval} es
cuantificar la temática de un texto, así como la importancia de cada
término que lo forma. Una manera sencilla de medir la importancia de un
término dentro de un documento es utilizando la frecuencia con la que
aparece (tf, term-frequency). Esta aproximación, aunque simple, tiene la
limitación de atribuir mucha importancia a aquellas palabras que
aparecen muchas veces aunque no aporten información selectiva. Por
ejemplo, si la palabra matemáticas aparece 5 veces en un documento y la
palabra página aparece 50, la segunda tendrá 10 veces más peso a pesar
de que no aporte tanta información sobre la temática del documento.

Para solucionar este problema se pueden ponderar los valores tf
multiplicándolos por la inversa de la frecuencia con la que el término
en cuestión aparece en el resto de documentos (idf). Asi se obtienee el
estadístico \textbf{tf-idf}, que se consigue reducir el valor de
aquellos términos que aparecen en muchos documentos y que, por lo tanto,
no aportan información selectiva.

El estadístico \textbf{tf-idf} es una medida numérica que expresa cuán
relevante es un término para un documento dentro de una colección de
documentos. Esta medida se utiliza a menudo como un factor de
ponderación en la recuperación de información y la minería de texto. El
valor \textbf{tf-idf} aumenta proporcionalmente al número de veces que
una palabra aparece en el documento, pero es compensada por la
frecuencia de la palabra en la colección de documentos, lo que permite
manejar el hecho de que algunas palabras son generalmente más comunes
que otras.

Variaciones del esquema de peso \textbf{tf-idf} son empleadas
frecuentemente por los motores de búsqueda como herramienta fundamental
para medir la relevancia de un documento dada una consulta del usuario,
estableciendo así una ordenación o ranking de los mismos.
\textbf{Tf-idf} también puede utilizarse exitosamente para el filtrado
de las denominadas stop-words (palabras que suelen usarse en casi todos
los documentos).

\hypertarget{definiciuxf3n-formal-del-estadistico-tf-idf}{%
\subsection{Definición formal del estadistico
tf-idf}\label{definiciuxf3n-formal-del-estadistico-tf-idf}}

\textbf{Term Frequency (tf)}

\begin{itemize}
\tightlist
\item
  Versión simple:
\end{itemize}

\[tf (k, d)= \dfrac{n(k,d)}{size(d)}\]

\begin{quote}
Donde:
\end{quote}

\begin{quote}
\(n(k, d)\) es el número de veces que aparece el término \(k\) en el
documento \(d\)
\end{quote}

\begin{quote}
\(size(d)\) es el nº de terminos del documento \(d\)
\end{quote}

\begin{itemize}
\tightlist
\item
  Versión normalizada (para evitar una predisposición hacia los
  documentos largos):
\end{itemize}

\[tf_{norm} (k, d)= \dfrac{tf(k,d)}{Max\lbrace tf(k,d) \hspace{0.12cm} /\hspace{0.12cm} k \in d  \rbrace}\]

\textbf{Inverse Document Frequency (idf)}

\begin{itemize}
\tightlist
\item
  Versión simple:
\end{itemize}

\[idf (k)=log\left(\dfrac{n(D)}{n(k, D)}\right)\]

\begin{quote}
Donde :
\end{quote}

\begin{quote}
\(n(D)= \# D\) es el número total de documentos , donde \(D\) es el
conjunto de los documentos
\end{quote}

\begin{quote}
\(n(D, k)=\# \lbrace d\in D / k \in d \rbrace\) el número de documentos
que contienen el término \(k\)
\end{quote}

\begin{quote}
\(log()\) es la función logaritmo en base \(e\)
\end{quote}

\begin{itemize}
\tightlist
\item
  Versión \texttt{sklearn} si smooth\_idf = True
\end{itemize}

\[idf (k)=log\left(\dfrac{n(D)}{n(k, D)}\right) + 1\]

\begin{itemize}
\tightlist
\item
  Versión \texttt{sklearn} si smooth\_idf = False
\end{itemize}

\[idf (k)=log\left(\dfrac{n(D) + 1}{n(k, D) + 1}\right) + 1\]

\textbf{Estadístico tf-idf}

\begin{itemize}
\tightlist
\item
  Versión simple:
\end{itemize}

\[tfidf(k, d)=tf(k, d) \cdot idf (k)\]

\begin{itemize}
\tightlist
\item
  Versión normalizada :
\end{itemize}

\[tfidf_{norm}(k, d) = tf_{norm}(k, d) \cdot idf (k)\]

\begin{itemize}
\tightlist
\item
  Versión \texttt{sklearn}
\end{itemize}

\[tfidf(k, d) = \dfrac{tfidf(k, d)}{ \sum_{k\in T(D)} tfidf(k, d)^2 }\]

\begin{quote}
Donde:
\end{quote}

\begin{quote}
\(T(D)\) es el conjunto de términos del conjunto de documentos \((D)\).
\end{quote}

\begin{quote}
Sea \(tfidf(k\in T, d) = ( tfidf(k, d) )_{k\in T}\) el vector que
contiene como componentes los valores de tf-idf para los terminos
\(k\in T\) en el documento \(d\) , entonces:
\end{quote}

\begin{quote}
\(|| tfidf(k\in T, d) ||_2 = \sqrt{ \sum_{k\in T(D)} tfidf(k, d)^2 }\),
es decir, es la norma euclidea del vector \(tfidf(k\in T, d)\)
\end{quote}

\begin{quote}
Notese que si \(k\notin d\) , entonces \(tfidf(k, d) = 0\), por tanto ,
\(\sum_{k\in T} tfidf(k, d)^2 = \sum_{k\in T(d)} tfidf(k, d)^2\) , donde
\(T(d)\) es el conjunto de los terminos del documento \(d \in D\)
\end{quote}

\hypertarget{cuxe1lculo-de-tf-idf-en-python}{%
\subsection{\texorpdfstring{Cálculo de tf-idf en
\texttt{Python}}{Cálculo de tf-idf en Python}}\label{cuxe1lculo-de-tf-idf-en-python}}

\hypertarget{cuxe1lculo-de-tf}{%
\subsubsection{Cálculo de tf}\label{cuxe1lculo-de-tf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# nº de veces que aparece cada termino (token) en cada noticia (n\_k)}

\NormalTok{df\_tf }\OperatorTok{=}\NormalTok{ pd.DataFrame( Fake\_News\_Tokens\_not\_StopWords.groupby([}\StringTok{\textquotesingle{}id\_text\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{])[}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{].count().reset\_index(name}\OperatorTok{=}\StringTok{\textquotesingle{}n\_k\textquotesingle{}}\NormalTok{) )}

\CommentTok{\# nº de terminos (tokens) en cada noticia (size(d))}

\NormalTok{df\_tf[}\StringTok{\textquotesingle{}size(d)\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df\_tf.groupby(}\StringTok{\textquotesingle{}id\_text\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}n\_k\textquotesingle{}}\NormalTok{].transform(}\BuiltInTok{sum}\NormalTok{)}

\CommentTok{\# Calculo de term{-}frequency (tf)}

\NormalTok{df\_tf[}\StringTok{\textquotesingle{}tf\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df\_tf[}\StringTok{\textquotesingle{}n\_k\textquotesingle{}}\NormalTok{] }\OperatorTok{/}\NormalTok{ df\_tf[}\StringTok{\textquotesingle{}size(d)\textquotesingle{}}\NormalTok{]}


\CommentTok{\# Calculo de term{-}frequency normalizado (tf\_norm)}

\NormalTok{df\_tf[}\StringTok{\textquotesingle{}max\_tf\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df\_tf.groupby(}\StringTok{\textquotesingle{}id\_text\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}tf\textquotesingle{}}\NormalTok{].transform(}\BuiltInTok{max}\NormalTok{)}

\NormalTok{df\_tf[}\StringTok{\textquotesingle{}tf\_norm\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df\_tf[}\StringTok{\textquotesingle{}tf\textquotesingle{}}\NormalTok{] }\OperatorTok{/}\NormalTok{ df\_tf[}\StringTok{\textquotesingle{}max\_tf\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Veamos como queda el data-frame creado:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_tf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         id_text         token  n_k      size(d)        tf    max_tf   tf_norm
0              0        accept    1          251  0.003984  0.059761  0.066667
1              0          alan    1          251  0.003984  0.059761  0.066667
2              0  alansandoval    1          251  0.003984  0.059761  0.066667
3              0         allow    1          251  0.003984  0.059761  0.066667
4              0          also    1          251  0.003984  0.059761  0.066667
...          ...           ...  ...          ...       ...       ...       ...
7155407    44897         union    1          132  0.007576  0.030303  0.250000
7155408    44897        volume    1          132  0.007576  0.030303  0.250000
7155409    44897         wants    1          132  0.007576  0.030303  0.250000
7155410    44897         worth    2          132  0.015152  0.030303  0.500000
7155411    44897         years    1          132  0.007576  0.030303  0.250000
\end{verbatim}

\hypertarget{cuxe1lculo-de-idf}{%
\subsubsection{Cálculo de idf}\label{cuxe1lculo-de-idf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculo del nº de documentos en los que aparece cada termino (token) (n(D,k))}

\NormalTok{df\_Idf }\OperatorTok{=}\NormalTok{ pd.DataFrame( Fake\_News\_Tokens\_not\_StopWords.groupby([}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{])[}\StringTok{\textquotesingle{}id\_text\textquotesingle{}}\NormalTok{].nunique().reset\_index(name}\OperatorTok{=}\StringTok{\textquotesingle{}n\_D\_k\textquotesingle{}}\NormalTok{) )}

\CommentTok{\# Ojo, si se usa count en lugar de nunique no se estaria contando el nº de documentos}
\CommentTok{\# en los que aparece cada termino, si no el nº de veces en total (contando repeticiones) que}
\CommentTok{\# aparece un termino en el conjunto de documentos. Por ejemplo, dado un termino k que aparece 10 veces en el documento d1 y 3 veces en el d3 , usando count() la cuenta sale 13 , que es el  nº de veces que aparece k en el conjunto de los documentos, en cambio usando nunique()  la cuenta sale 2 que es el nº de documentos en los que aparece el termino k, que es lo que buscamos}

\CommentTok{\# Calculo del nº total de documentos (n\_D)}

\NormalTok{df\_Idf[}\StringTok{\textquotesingle{}n\_D\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(Fake\_News\_Data)}

\CommentTok{\# Calculo de Idf}

\NormalTok{df\_Idf[}\StringTok{\textquotesingle{}Idf\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.log( (df\_Idf[}\StringTok{\textquotesingle{}n\_D\textquotesingle{}}\NormalTok{] ) }\OperatorTok{/}\NormalTok{ (df\_Idf[}\StringTok{\textquotesingle{}n\_d\_k\textquotesingle{}}\NormalTok{]) ) }\OperatorTok{+} \DecValTok{1} 
\end{Highlighting}
\end{Shaded}

Vemos como queda el data-frame creado:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_Idf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             token  n_d_k    n_d        Idf
0               aa     28  44898   8.379944
1              aaa     11  44898   9.314253
2       aaaaaaaand      1  44898  11.712149
3        aaaaackkk      1  44898  11.712149
4       aaaaapkfhk      1  44898  11.712149
...            ...    ...    ...        ...
125561        ””it      1  44898  11.712149
125562      ””when      1  44898  11.712149
125563          if      1  44898  11.712149
125564    $emoji1$      1  44898  11.712149
125565    $emoji2$      1  44898  11.712149
\end{verbatim}

\hypertarget{cuxe1lculo-de-tf-idf}{%
\subsubsection{Cálculo de tf-idf}\label{cuxe1lculo-de-tf-idf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_tf\_Idf }\OperatorTok{=}\NormalTok{ pd.merge(df\_tf, df\_Idf, on}\OperatorTok{=}\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{)}

\NormalTok{df\_tf\_Idf[}\StringTok{\textquotesingle{}tf\_Idf\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df\_tf\_Idf[}\StringTok{\textquotesingle{}tf\textquotesingle{}}\NormalTok{] }\OperatorTok{*}\NormalTok{ df\_tf\_Idf[}\StringTok{\textquotesingle{}Idf\textquotesingle{}}\NormalTok{] }

\NormalTok{df\_tf\_Idf[}\StringTok{\textquotesingle{}tf\_Idf\_norm\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df\_tf\_Idf[}\StringTok{\textquotesingle{}tf\_norm\textquotesingle{}}\NormalTok{] }\OperatorTok{*}\NormalTok{ df\_tf\_Idf[}\StringTok{\textquotesingle{}Idf\textquotesingle{}}\NormalTok{]}

\NormalTok{df\_tf\_Idf }\OperatorTok{=}\NormalTok{ df\_tf\_Idf.sort\_values(by}\OperatorTok{=}\StringTok{"id\_text"}\NormalTok{)}

\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}

\KeywordTok{def}\NormalTok{ euclidean\_norm( v ):}

  \ControlFlowTok{return}\NormalTok{ np.sqrt( (v}\OperatorTok{**}\DecValTok{2}\NormalTok{).}\BuiltInTok{sum}\NormalTok{() )}
  
\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}

\NormalTok{df\_tf\_Idf[}\StringTok{\textquotesingle{}euclidean\_norm\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df\_tf\_Idf.groupby(}\StringTok{\textquotesingle{}id\_text\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}tf\_Idf\textquotesingle{}}\NormalTok{].transform( euclidean\_norm)}

\NormalTok{df\_tf\_Idf[}\StringTok{\textquotesingle{}tf\_Idf\_sklearn\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df\_tf\_Idf[}\StringTok{\textquotesingle{}tf\_Idf\textquotesingle{}}\NormalTok{] }\OperatorTok{/}\NormalTok{ df\_tf\_Idf[}\StringTok{\textquotesingle{}euclidean\_norm\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Vemos con es el nuevo data-frame creado:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_tf\_Idf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         id_text         token  n_k      size(d)        tf    max_tf  
         
0              0        accept    1          251  0.003984  0.059761   
304262         0       pollitt    1          251  0.003984  0.059761   
304263         0         power    1          251  0.003984  0.059761   
309320         0     president    3          251  0.011952  0.059761   
332891         0  presidential    1          251  0.003984  0.059761   

...          ...           ...  ...          ...       ...       ...   

5516302    44897          amid    1          132  0.007576  0.030303   
5350014    44897        string    1          132  0.007576  0.030303   
2043348    44897         state    2          132  0.015152  0.030303   
1423490    44897     delivered    1          132  0.007576  0.030303   
7155411    44897        suhkoi    1          132  0.007576  0.030303   


          tf_norm  n_d_k    n_d        Idf    tf_Idf  tf_Idf_sklearn  
          
0        0.066667   1395  44898   4.471499  0.017815     0.298100  
304262   0.066667      1  44898  11.712149  0.046662     0.780810  
304263   0.066667   5057  44898   3.183620  0.012684     0.212241  
309320   0.200000  23571  44898   1.644376  0.019654     0.328875  
332891   0.066667   8823  44898   2.627031  0.010466     0.175135  

...           ...    ...    ...        ...       ...          ...  

5516302  0.250000   1316  44898   4.529796  0.034317     1.132449  
5350014  0.250000    404  44898   5.710734  0.043263     1.427683  
2043348  0.500000  14226  44898   2.149322  0.032565     1.074661  
1423490  0.250000    882  44898   4.929956  0.037348     1.232489  
7155411  0.250000      1  44898  11.712149  0.088728     2.928037  
\end{verbatim}

\hypertarget{matriz-tf-idf}{%
\subsection{Matriz Tf-Idf}\label{matriz-tf-idf}}

Para poder aplicar algoritmos de clasificación a un texto, es necesario
crear una representación numérica del mismo. Para ello se utiliza una
matriz que tiene como filas los documentos y como columnas los tokens.
Existen diferentes criterios para definir los elementos internos de esta
matriz. Sea \((i,j)\) el elemento de la fila \$i y columna j
distinguimos varias aproximaciones. Una es que (i,j) sea la frecuencia
del token j en el documento i , es decir, tf(j,i) , otra aproximacion es
que (i, j) sea 0 si el token j no aparece en el documento i y 1 en el
caso de que si aparece. Otra aprozimacion es que (i,j) sea tfidf(j,i).

El criterio seguido en esta seccion del trabajo es que
\((i,j) = tfidf(j,i)\) , ya que es el criterio seguido por la libreria
\texttt{sklearn}, la cual será empleada para calcular la matriz tf-idf.
Además es uno de los criterios mas habituales para definir dicha matriz.

Se ha intentado construir esta matriz a traves de bucles, pero dado que
es una matriz con 44898 filas (documentos) y 125565 columnas (tokens) ,
la sola operacion de crear la primera fila no ha podido ser ejecutada
por el computador por sobrepasar la memoria necesaria para ello. Por
tanto deben usarse métodos de programación mas eficientes, o usar
opciones eficientes ya implementadas por desarrolladores profesionales,
como el equipo de \texttt{sklearn}. Esta segunda opción es la que
seguiresmos, es decir, usaremos dicha funciones de dicha libreria para
crear la matriz tf-idf.

Para crear la matriz tf-idf con \texttt{sklearn} necesitamos construir
por un un vector con los documentos (en este caso noticias). Además
también vamos a crear otro con la variable respuesta (en este caso la
variable binaria \texttt{Fake} que indica si las noticias son o no
fakes), que será necesario para la parte de clasificación de texto.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_data }\OperatorTok{=}\NormalTok{ Fake\_News\_Data.loc[ : , }\StringTok{\textquotesingle{}text\textquotesingle{}}\NormalTok{]}

\NormalTok{X\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0        Donald Trump just couldn t wish all Americans ...
1        House Intelligence Committee Chairman Devin Nu...
2        On Friday, it was revealed that former Milwauk...
3        On Christmas day, Donald Trump announced that ...
4        Pope Francis used his annual Christmas Day mes...
                               
                               ... 
                               
44893    BRUSSELS (Reuters) - NATO allies on Tuesday we...
44894    LONDON (Reuters) - LexisNexis, a provider of l...
44895    MINSK (Reuters) - In the shadow of disused Sov...
44896    MOSCOW (Reuters) - Vatican Secretary of State ...
44897    JAKARTA (Reuters) - Indonesia will buy 11 Sukh...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y\_data }\OperatorTok{=}\NormalTok{ Fake\_News\_Data.loc[ : , }\StringTok{\textquotesingle{}Fake\textquotesingle{}}\NormalTok{]}

\NormalTok{Y\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0        1
1        1
2        1
3        1
4        1

        ...

44893    0
44894    0
44895    0
44896    0
44897    0
\end{verbatim}

Importamos la funcion \texttt{TfidfVectorizer} de \texttt{sklearn} la
cual nos permitirá generar la matriz tf-idf.

Es recomendable ver la documentación de \texttt{sklearn} para esta
función
\href{https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html}{documentación}

En este caso usaremos como funcion de tokenizacion la anteriormente
creada \texttt{limpiar\_tokenizar} , podria usarse la que usa por
defecto \texttt{sklearn}, además la lista de stopwords que también se ha
usado anteriormente. Ademas usamos los argumentos min\_df = 0 , lo cual
significa que se van a considerar todos los tokens generados por la
funcion tokenizer, si min\_df=h la función solo consideraria los tokens
que aparecen en mas de h documentos. Por ultimo usaremos el argumento
smooth\_idf=False , el cual ya fue mencionado en la seccion
\emph{definición formal del estadistico tf-idf}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.feature\_extraction.text }\ImportTok{import}\NormalTok{ TfidfVectorizer}

\NormalTok{tfidf\_vectorizador }\OperatorTok{=}\NormalTok{ TfidfVectorizer(tokenizer  }\OperatorTok{=}\NormalTok{ limpiar\_tokenizar, min\_df }\OperatorTok{=} \DecValTok{0}\NormalTok{, stop\_words }\OperatorTok{=}\NormalTok{ stop\_words, smooth\_idf}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Ahora necesitamos usar el método \texttt{fit} con el vector de
documentos \texttt{X\_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_vectorizador.fit(X\_data)}
\end{Highlighting}
\end{Shaded}

Creamos la matriz tf-idf con el metodo \texttt{transform}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_matrix }\OperatorTok{=}\NormalTok{ tfidf\_vectorizador.transform(X\_data)}
\end{Highlighting}
\end{Shaded}

Comprobamos el tamaño de la matriz

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_matrix.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(44898, 125566)
\end{verbatim}

Podemos obtener los nombres de las columnas de la matriz, a saber, los
tokens, con el metodo \texttt{get\_feature\_names\_out} , en este caso
imprimimos los 50 primeros:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(tfidf\_vectorizador.get\_feature\_names\_out()[}\DecValTok{0}\NormalTok{:}\DecValTok{50}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['aa' 'aaa' 'aaaaaaaand' 'aaaaackkk' 'aaaaapkfhk' 'aaaahhhh' 'aaaand'
 'aaaarrgh' 'aaab' 'aaarf' 'aab' 'aaba' 'aabfsv' 'aabge' 'aabo'
 'aaccording' 'aachen' 'aacnr' 'aadhaar' 'aadhar' 'aadl' 'aaf' 'aafn'
 'aag' 'aahd' 'aahwuhvvnh' 'aai' 'aaj' 'aaja' 'aal' 'aalberg' 'aalberts'
 'aaldef' 'aaliyah' 'aamer' 'aamin' 'aammir' 'aamom' 'aamrrd' 'aan'
 'aaofj' 'aapa' 'aapi' 'aapl' 'aapxim' 'aar' 'aardal' 'aardvark'
 'aardvarks' 'aargh'] 
\end{verbatim}

\texttt{sklearn} no permite imprimir la matriz tf-idf obtenida debido a
sus dimension excesiva, pero si podemos acceder a sus elementos del
siguiente modo:

Por ejemplo, podemos acceder al tf-idf del token asociado a la columna
645 y el documento asociado a la columna 0

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_matrix[}\DecValTok{0}\NormalTok{, }\DecValTok{645}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 0.034889784479772486
 
\end{verbatim}

La salida anterior nos indicaa que el valor del estadistico tf-idf para
el token asociado a la columna 645 en el documento (noticia) asociada a
la fila 0 (la fila 0 en Python es la fila 1) es 0.0349

Vamos a crear un data-frame para identificar cada columna con su token
asociado:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_index\_token }\OperatorTok{=}\NormalTok{ pd.DataFrame( \{}\StringTok{\textquotesingle{}index\textquotesingle{}}\NormalTok{ : }\BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{,}\BuiltInTok{len}\NormalTok{(tfidf\_vectorizador.get\_feature\_names\_out())) , }\StringTok{\textquotesingle{}token\textquotesingle{}}\NormalTok{ : tfidf\_vectorizador.get\_feature\_names\_out() \})}

\NormalTok{df\_index\_token}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         index       token

0            0          aa
1            1         aaa
2            2  aaaaaaaand
3            3   aaaaackkk
4            4  aaaaapkfhk

...        ...         ...

125561  125561        ””it
125562  125562      ””when
125563  125563          if
125564  125564    $emoji1$
125565  125565    $emoji2$
\end{verbatim}

Utilizando este da-taframe vamos a comparar algunos valores de la matriz
tf-idf obtenida con \texttt{sklearn}con los valores que calculamos
nosotros en las secciones anteriores y que se encuentran registrados en
el data-frame \texttt{df\_tf\_Idf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_tf\_Idf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

         id_text         token  n_k  longitud(d)        tf    max_tf  

0              0        accept    1          251  0.003984  0.059761   
304262         0       pollitt    1          251  0.003984  0.059761   
304263         0         power    1          251  0.003984  0.059761   
309320         0     president    3          251  0.011952  0.059761   
332891         0  presidential    1          251  0.003984  0.059761   
...          ...           ...  ...          ...       ...       ...   
5516302    44897          amid    1          132  0.007576  0.030303   
5350014    44897        string    1          132  0.007576  0.030303   
2043348    44897         state    2          132  0.015152  0.030303   
1423490    44897     delivered    1          132  0.007576  0.030303   
7155411    44897        suhkoi    1          132  0.007576  0.030303   

          tf_norm  n_d_k    n_d        Idf    tf_Idf  tf_Idf_norm  
0        0.066667   1395  44898   4.471499  0.017815     0.298100   
304262   0.066667      1  44898  11.712149  0.046662     0.780810   
304263   0.066667   5057  44898   3.183620  0.012684     0.212241   
309320   0.200000  23571  44898   1.644376  0.019654     0.328875   
332891   0.066667   8823  44898   2.627031  0.010466     0.175135   
...           ...    ...    ...        ...       ...          ...   
5516302  0.250000   1316  44898   4.529796  0.034317     1.132449   
5350014  0.250000    404  44898   5.710734  0.043263     1.427683   
2043348  0.500000  14226  44898   2.149322  0.032565     1.074661   
1423490  0.250000    882  44898   4.929956  0.037348     1.232489   
7155411  0.250000      1  44898  11.712149  0.088728     2.928037   

         euclidean_norm  tf_Idf_sklearn  
0              0.510600        0.034890  
304262         0.510600        0.091386  
304263         0.510600        0.024841  
309320         0.510600        0.038492  
332891         0.510600        0.020498  
...                 ...             ...  
5516302        0.585168        0.058644  
5350014        0.585168        0.073933  
2043348        0.585168        0.055651  
1423490        0.585168        0.063825  
7155411        0.585168        0.151629 
\end{verbatim}

Vamos a comparar concretamente los tf-idf obtenidos ``manualmente'' y
con \texttt{sklearn} para los tokens `accept' , `pollit' , `string',
`never', e `investigation' :

Primero tenemos que identificar la columna asociada al token `accept'

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_index\_token.loc[df\_index\_token.token }\OperatorTok{==} \StringTok{\textquotesingle{}accept\textquotesingle{}}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 index   token
\end{verbatim}

645 645 accept

Ahora vemos cual es el valor del estadistico tf-idf calculado por
\texttt{sklearn} para el token `accept' en el documento asociado a la
fila 0.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_matrix[}\DecValTok{0}\NormalTok{, }\DecValTok{645}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

0.034889784479772486

Ahora comprobamos el valor del estadistico tf-idf calculado
``manualmente'' por nosotros para el token `accept' y la noticia con
identificador 0 (notese que la noticia con identificador \(i\) es la que
esta asociada a la fila \(i\) de la matriz creada por \texttt{sklearn} ,
para \(i =0,1,2,...\))

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_tf\_Idf.loc[ ( df\_tf\_Idf.id\_text }\OperatorTok{==} \DecValTok{0}\NormalTok{ ) }\OperatorTok{\&}\NormalTok{  ( df\_tf\_Idf.token }\OperatorTok{==} \StringTok{\textquotesingle{}accept\textquotesingle{}}\NormalTok{ ) , ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   id_text   token  n_k  longitud(d)        tf    max_tf   tf_norm  n_d_k  
0        0  accept    1          251  0.003984  0.059761  0.066667   1395   

     n_d       Idf    tf_Idf  tf_Idf_norm  euclidean_norm  tf_Idf_sklearn  
0  44898  4.471499  0.017815       0.2981          0.5106         0.03489 
\end{verbatim}

Podemos ver que de todos los estadisticos tf-idf calculados (el simple,
el normalizado y la versión de sklearn) el único que coincide con el
obtenido al usar sklearn es justanmente \texttt{tf\_Idf\_sklearn} , como
cabria esperar.

En este punto hay que hacer una mención especial a la entrada de
\href{https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/}{analyticsvidhya}
, la cual me permitio resolver una disparidad de resultados que obtuve
inicialmente, al no ser consciente de que \texttt{sklearn} normalizaba
el tf-idf (la version simple) dividiendolo entre la norma euclidea, tal
y como se ha explicado anteriormente con más detalle.

Ahora repetimos el mismo proceso para el token `pollitt'

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_index\_token.loc[df\_index\_token.token }\OperatorTok{==} \StringTok{\textquotesingle{}pollitt\textquotesingle{}}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       index    token
79741  79741  pollitt
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_matrix[}\DecValTok{0}\NormalTok{, }\DecValTok{79741}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   0.09138643507615184
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_tf\_Idf.loc[ ( df\_tf\_Idf.id\_text }\OperatorTok{==} \DecValTok{0}\NormalTok{ ) }\OperatorTok{\&}\NormalTok{  ( df\_tf\_Idf.token }\OperatorTok{==} \StringTok{\textquotesingle{}pollitt\textquotesingle{}}\NormalTok{ ) , ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        id_text    token  n_k  longitud(d)        tf    max_tf   tf_norm  \
304262        0  pollitt    1          251  0.003984  0.059761  0.066667   

        n_d_k    n_d        Idf    tf_Idf  tf_Idf_norm  euclidean_norm  \
304262      1  44898  11.712149  0.046662      0.78081          0.5106   

        tf_Idf_sklearn  
304262        0.091386  
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_index\_token.loc[df\_index\_token.token }\OperatorTok{==} \StringTok{\textquotesingle{}string\textquotesingle{}}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   index   token
\end{verbatim}

99546 99546 string

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_matrix[}\DecValTok{44897}\NormalTok{, }\DecValTok{99546}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

0.07393279140214064

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_tf\_Idf.loc[ ( df\_tf\_Idf.id\_text }\OperatorTok{==} \DecValTok{44897}\NormalTok{ ) }\OperatorTok{\&}\NormalTok{  ( df\_tf\_Idf.token }\OperatorTok{==} \StringTok{\textquotesingle{}string\textquotesingle{}}\NormalTok{ ) , ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     id_text   token  n_k  longitud(d)        tf    max_tf  tf_norm  \
\end{verbatim}

5350014 44897 string 1 132 0.007576 0.030303 0.25

\begin{verbatim}
     n_d_k    n_d       Idf    tf_Idf  tf_Idf_norm  euclidean_norm  \
\end{verbatim}

5350014 404 44898 5.710734 0.043263 1.427683 0.585168

\begin{verbatim}
     tf_Idf_sklearn  
\end{verbatim}

5350014 0.073933

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_index\_token.loc[df\_index\_token.token }\OperatorTok{==} \StringTok{\textquotesingle{}never\textquotesingle{}}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   index  token
\end{verbatim}

70560 70560 never

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_matrix[}\DecValTok{3}\NormalTok{, }\DecValTok{70560}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

0.022176846230040667

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_tf\_Idf.loc[ ( df\_tf\_Idf.id\_text }\OperatorTok{==} \DecValTok{3}\NormalTok{ ) }\OperatorTok{\&}\NormalTok{  ( df\_tf\_Idf.token }\OperatorTok{==} \StringTok{\textquotesingle{}never\textquotesingle{}}\NormalTok{ ) , ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    id_text  token  n_k  longitud(d)        tf    max_tf   tf_norm  n_d_k  \
\end{verbatim}

990208 3 never 1 249 0.004016 0.044177 0.090909 6077

\begin{verbatim}
      n_d       Idf    tf_Idf  tf_Idf_norm  euclidean_norm  tf_Idf_sklearn  
\end{verbatim}

990208 44898 2.999882 0.012048 0.272717 0.543257 0.022177

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_index\_token.loc[df\_index\_token.token }\OperatorTok{==} \StringTok{\textquotesingle{}investigation\textquotesingle{}}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   index          token
\end{verbatim}

50314 50314 investigation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_matrix[}\DecValTok{1522}\NormalTok{, }\DecValTok{50314}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

0.2598673157066844

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_tf\_Idf.loc[ ( df\_tf\_Idf.id\_text }\OperatorTok{==} \DecValTok{1522}\NormalTok{ ) }\OperatorTok{\&}\NormalTok{  ( df\_tf\_Idf.token }\OperatorTok{==} \StringTok{\textquotesingle{}investigation\textquotesingle{}}\NormalTok{ ) , ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    id_text          token  n_k  longitud(d)        tf    max_tf  \
\end{verbatim}

635701 1522 investigation 7 210 0.033333 0.052381

\begin{verbatim}
     tf_norm  n_d_k    n_d       Idf    tf_Idf  tf_Idf_norm  \
\end{verbatim}

635701 0.636364 3753 44898 3.481838 0.116061 2.215715

\begin{verbatim}
    euclidean_norm  tf_Idf_sklearn  
\end{verbatim}

635701 0.446617 0.259867

\newpage

\hypertarget{muxe9todos-naive-bayes}{%
\section{Métodos Naive Bayes}\label{muxe9todos-naive-bayes}}

Los métodos de naive Bayes son un conjunto de algoritmos de aprendizaje
supervisado basados en aplicar el teorema de Bayes con el supuesto
``naive'' de independencia condicional entre cada par de predictores
dada una clase de la variable respuesta (que debe ser categorica).

Sean \(Y,X_1,...,X_p\) la respuesta categorica y los predictores, y sean
\(x_i = (x_i1 , x_i2, ..., x_ip)^t\) y \(y_i\) la \(i\)-esima
observación de los predictores y de la respuesta, respectivamente.

Si consideramos \(Y,X_1,...,X_p\) como variables aleatorias, por el
teorema de Bayes tenemos que :

\[P(Y=y_i | X_1 = x_{i1} ,..., X_p=x_{ip}) = \dfrac{P(Y=y_i)\cdot P(X_1=x_{i1} ,..., X_p=x_{ip} | Y=y_i)}{P(X_1=x_{i1} ,..., X_p=x_{ip})}\]

Usando el supuesto naive de independencia entre cada par de predictores

\[X_r \perp X_j , \forall r\neq j = 1,...,p\]

tenemos que:

\[P(X_1=x_{i1} ,..., X_p=x_{ip} | Y=y_i) = \prod_{j=1}^{p} P(X_j=x_{ij} | Y=y_i)\]

Por tanto, podemos reformular el teorema de Bayes como:

\[P(Y=y_i | X_1 = x_1 ,..., X_p=x_p) = \dfrac{P(Y=y_i)\cdot\prod_{j=1}^{p} P(X_j=x_{ij} | Y=y_i)}{P(X_1=x_{i1} ,..., X_p=x_{ip})}\]

\[P(Y=y_i | X_1 = x_1 ,..., X_p=x_p) \propto \P(Y=y_i)\cdot\prod_{j=1}^{p} P(X_j=x_{ij} | Y=y_i)\]

El algoritmo de naive Bayes predice la respuesta \(Y\) para un vector de
observaciones de los predictores \(x_i = (x_i1 , x_i2, ..., x_ip)^t\)
como la solucion del siguiente problema de optimización:

\[\underset{Max}{y} P(Y=y | X_1 = x_1 ,..., X_p=x_p) = \underset{Max}{y} \dfrac{P(Y=y_i)\cdot\prod_{j=1}^{p} P(X_j=x_{ij} | Y=y_i)}{P(X_1=x_{i1} ,..., X_p=x_{ip})} =   \underset{Max}{y}  P(Y=y) \cdot\prod_{j=1}^{p} P(X_j=x_{ij} | Y=y)\]

Notese que \(P(X_1=x_{i1} ,..., X_p=x_{ip})\) no depende del valor de
\(y\) por lo que puede sacarse del problema de maximización.

Es decir, la prediccion de \(Y\) para el vector de observaciones de los
predictores \(x_i = (x_i1 , x_i2, ..., x_ip)^t\) es

\[\hat{y_i} = arg \underset{Max}{y} P(Y=y | X_1 = x_1 ,..., X_p=x_p) = arg \underset{Max}{y} P(Y=y) \cdot\prod_{j=1}^{p} P(X_j=x_{ij} | Y=y)\]

Es decir, la observación \(i\)-esima
\(x_i = (x_i1 , x_i2, ..., x_ip)^t\) se clasifica en la
clase/categoria/grupo de maxima probabilidad para esa observación.

\textbf{Problemas}

Dados y y x\_\{ij\}

\begin{itemize}
\tightlist
\item
  ¿Cómo estimar P(Y=y) ?
\end{itemize}

\[\widehat{P}(Y=y) = \dfrac{\# \lbrace r=1,...,n / y_r = y  \rbrace}{n}\]

Es decir, P(Y=y) se estima como la proporcion de observaciones (del
conjunto de entrenamiento) que pertenecen a la clase/categoria/grupo
\(y\) , es decir, se estima como la proporción de observaciones para las
que la respuesta toma la categoria \(y\)

\begin{itemize}
\tightlist
\item
  ¿ Cómo estimar P(X\_j=x\_\{ij\} \textbar{} Y=y) ?
\end{itemize}

Podria seguirse la solución del problema anterior, a saber:

\[\widehat{P}(X_j=x_{ij} | Y=y) = \dfrac{ \# \lbrace r=1,...,n \  y_r = y  \text{y} x_{rj} = x_{ij} \rbrace }{ \# \lbrace  r=1,...,n \  y_r = y  \rbrace}\]

\textbf{Problema:} en la practica en cuanto haya algun predictor \(X_j\)
tal que el valor observado \(x_{ij}\) no este en el set de observaciones
de entrenamiento con \(Y=y\) se tendrá
\(\widehat{P}(X_j=x_{ij} | Y=y) = 0\) , lo que conducirá a
\(P(Y=y) \cdot\prod_{j=1}^{p} P(X_j=x_{ij} | Y=y) = 0\) , y esto llevará
a no clasificar la observacion \(x_i = (x_i1 , x_i2, ..., x_ip)^t\) en
la clase \(y\) , independientemente de los valores observados para el
resto de predictores. Por lo que es razonable pensar que esta
aproximacion no conduciria a buenas predicciones de la respuesta.

Ademas si nos enfocamos en un problema de clasificación de texto, en el
que se usa la matriz tf-idf como matriz de predictores, esta
aproximacion queda en clara evidencia, ya que para cualquier categoria
\(y\) habria algunas palabras (tokens) que tienen para cada documento un
valor del estadistico tf-idf diferente al valor correspondiente de la
observacion \(x_i = (x_i1 , x_i2, ..., x_ip)^t\), es decir, para cada
\(y\) habria algun predictor \(X_j\) tal que
\(\widehat{P}(X_j=x_{ij} | Y=y) = 0\) , por lo que la observacion
\(x_i = (x_i1 , x_i2, ..., x_ip)^t\) seria clasificada indistintamente
en cualquier categoria de la respuesta, lo cual no es razonable en
absoluto.

Si en lugar de una matriz tf-idf se usase un matriz con el conteo de
ocurrencia de cada palabra en los textos la situacion seria similar,
puesto que para cada categoria \(y\) de la respuesta habria algunas
palabras que aparecen 0 veces en los textos de dicha categoria (es
decir, habria algunos predictores \(X_j\) tales que
\(\widehat{P}(X_j=x_{ij} | Y=y) = 0\) ), y esto llevaria a la misma
conclusion que antes.

La solución estandar a este problema pasa por estimar
\(\widehat{P}(X_j=x_{ij} | Y=y)\) usando la funcion de
probabilidad/densidad de una distribución conocida. En este trabajo
distinguiremos dos casos (los estandar), uno en el que se usa la
distribucion normal Gaussiana (Gaussian Naive Bayes) y otro en el que se
usa la distribución multinomial (multinomial naive Bayes).

\hypertarget{gaussian-naive-bayes}{%
\subsection{Gaussian Naive Bayes}\label{gaussian-naive-bayes}}

Este algoritmo naive Bayes es apropiado para el caso en el que tenemos
predictores cuantitativos continuos.

En nuestro caso al trabajar con la matriz tf-idf como matriz de
predictores, se pueden considerar nuestros predictores como
cuantitativos continuos, por lo que es un caso en el que es adecuado
usar Gaussian naive Bayes.

Gaussian Naive Bayes es un naive Bayes classifier en el que se toma el
siguiente supuesto:

Par cada predictor \(X_j\) continuo , con \(j=1,...,p\)

\(\widehat{P}(X_j=x_{ij} | Y=y) = \dfrac{1}{\sqrt{2\pi \sigma^2(x_{ij}| y)}} \cdot exp \lbrace - \dfrac{(x_{ij} - \mu(x_{ij}|y))^2}{2 \sigma^2(x_{ij}| y)} \rbrace\)

Donde:

\(x_{ij} \in \mathbb{R}\)

\(\mu(x_{ij}| y) = Mean( x_{rj} / r=1,..,n ; y_r = y )\) , es decir, es
la media de los valores de \(X_j\) asociados a la clase \(y\) de la
variable respuesta \(Y\)

\(\sigma^2(x_{ij}| y) = Var( x_{rj} / r=1,..,n ; y_r = y )\) , es decir,
es la varianza de los valores de \(X_j\) asociados a la clase y de la
variable respuesta Y

\textbf{Observación:}

Esta aproximación permite solventar el problema de que en la practica en
cuanto haya algun predictor \(X_j\) tal que el valor observado
\(x_{ij}\) no este en el set de observaciones de entrenamiento con
\(Y=y\) se tendrá \(\widehat{P}(X_j=x_{ij} | Y=y) = 0\) , lo que
conducirá a \(P(Y=y) \cdot\prod_{j=1}^{p} P(X_j=x_{ij} | Y=y) = 0\) , y
esto llevará a no clasificar la observacion
\(x_i = (x_i1 , x_i2, ..., x_ip)^t\) en la clase \(y\) ,
independientemente de los valores observados para el resto de
predictores. Ya que
\(\dfrac{1}{\sqrt{2\pi \sigma^2(x_{ij}| y)}} \cdot exp \lbrace - \dfrac{(x_{ij} - \mu(x_{ij}|y))^2}{2 \sigma^2(x_{ij}| y)} \neq 0\)
inclusio si el valor observado \(x_{ij}\) no esta en el set de
observaciones de entrenamiento con \(Y=y\)

Para ver como funciona este algoritmo con un ejemplo de juguete es
recomendable ver la siguiente entrada de
\href{https://es.wikipedia.org/wiki/Clasificador_bayesiano_ingenuo\#Entrenamiento}{Wikipedia}

\hypertarget{multinomial-naive-bayes}{%
\subsection{Multinomial Naive Bayes}\label{multinomial-naive-bayes}}

Este naive Bayes es adecuado cuando se tienen predictores cuantitativos
discretos de conteo, es decir, predictores definidos como el nº de veces
que ocurre un evento.

Si estamos en el caso de clasificación de textos y trabajamos con una
matriz de conteo de apareciones de las palabras (tokens) en los
documentos, por lo que es un caso en el que es adecuado usar Multinomial
naive Bayes.

Multinomial Naive Bayes es un naive Bayes classifier en el que se toma
el siguiente supuesto:

Para cada predictor \(X_j\) de conteo , con \(j=1,...,p\)

\$\widehat{P}(X\_j=x\_\{ij\} \textbar{} Y=y) =
\dfrac{ \# \lbrace r=1,...,n \  y_r = y  \text{y} x_{rj} = x_{ij} \rbrace + \alpha}{ \# \lbrace  r=1,...,n \  y_r = y  \rbrace + \alpha\cdot n}
\$

Donde: \(x_{ij}=0,1,2,3,...\)

\textbf{Observación:}

Esta aproximacion es muy similar ala propuesta inicialmente, que
mencionamos que tenia problemas, especialmente para predictores
cuantitativos . Pero tiene algunas diferencias. Por un lado que en este
caso solo se aplicaria a variables de conteo. Y especialmente la
incorporacion de un termino \(\alpha > 0\) en el numerador que va a
permitir solventar el problema de que en la practica en cuanto haya
algun predictor \(X_j\) tal que el valor observado \(x_{ij}\) no este en
el set de observaciones de entrenamiento con \(Y=y\) se tendrá
\(\widehat{P}(X_j=x_{ij} | Y=y) = 0\) , lo que conducirá a
\(P(Y=y) \cdot\prod_{j=1}^{p} P(X_j=x_{ij} | Y=y) = 0\) , y esto llevará
a no clasificar la observacion \(x_i = (x_i1 , x_i2, ..., x_ip)^t\) en
la clase \(y\) , independientemente de los valores observados para el
resto de predictores.

Si \(\alpha = 1\) es llamado suavizado de Laplace, mientras que si
\(\alpha \neq 1\) es llamado suavizado de Lidstone.

\hypertarget{categorical-naive-bayes}{%
\subsection{Categorical Naive Bayes}\label{categorical-naive-bayes}}

Este naive Bayes es adecuado cuando se tienen predictores categoricos.

Multinomial Naive Bayes es un naive Bayes classifier en el que se toma
el siguiente supuesto:

Para cada predictor \(X_j\) categorico
\(\lbrace 0 , 1,..., c_j-1 \rbrace\) , con \(j=1,...,p\)

\$\widehat{P}(X\_j=x\_\{ij\} \textbar{} Y=y) =
\dfrac{ \# \lbrace r=1,...,n \  y_r = y  \text{y} x_{rj} = x_{ij} \rbrace + \alpha}{ \# \lbrace  r=1,...,n \  y_r = y  \rbrace + \alpha\cdot n}
\$

Donde: \(x_{ij} \in 0,1,..., c_j-1\)

\hypertarget{gaussian-naive-bayes-aplicado-con-python}{%
\subsection{\texorpdfstring{Gaussian Naive Bayes aplicado con
\texttt{Python}}{Gaussian Naive Bayes aplicado con Python}}\label{gaussian-naive-bayes-aplicado-con-python}}

Vamos a importar los las funciones \texttt{GaussianNB} y
\texttt{train\_test\_split}. La primera para implementar el modelo de
naive Bayes Gaussiano, y el segundo para separar el data-set en parte de
entrenamiento y parte de test.

\begin{Shaded}
\begin{Highlighting}[]

\ImportTok{from}\NormalTok{ sklearn.naive\_bayes }\ImportTok{import}\NormalTok{ GaussianNB}

\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\end{Highlighting}
\end{Shaded}

Aplicaremos una estrategia de validación simple para validar la
capacidad predictiva del modelo que implementaremos, para ello separamos
el vector de documentos (noticias) \texttt{X\_data} y el vector de la
respuesta \texttt{Y\_data} en dos partes, una para entrenar el modelo
(train) y otra para testearlo (test), para ello usamos la función
\texttt{train\_test\_split} :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_train, X\_test, Y\_train, Y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(}
\NormalTok{    X\_data,}
\NormalTok{    Y\_data,}
\NormalTok{    test\_size }\OperatorTok{=} \FloatTok{0.3}\NormalTok{,}
\NormalTok{    random\_state }\OperatorTok{=} \DecValTok{123}    
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Inicializamos de nuevo la función TfidfVectorizer que se sirve para
generar la matriz tf-idf, usando los mismos argumentos que antes
tokenizer = limpiar\_tokenizar, min\_df = 5, stop\_words = stop\_words,
smooth\_idf=False , a excepcion de min\_df que antes pusimos 0 pero
ahora 5, para que al construir la matriz solo se consideren los tokens
que aparecen en mas de 5 documentos.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_vectorizador }\OperatorTok{=}\NormalTok{ TfidfVectorizer(tokenizer  }\OperatorTok{=}\NormalTok{ limpiar\_tokenizar, min\_df }\OperatorTok{=} \DecValTok{5}\NormalTok{, stop\_words }\OperatorTok{=}\NormalTok{ stop\_words, smooth\_idf}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Aplicamos el metodo \texttt{fit} a \texttt{tfidf\_vectorizador} con los
documentos de entrenamiento. Esto es importante porque es lo que deja
fijado los tokens que se van a incluir como columnas de la matriz
tf-idf.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{tfidf\_vectorizador.fit(X\_train)}
\end{Highlighting}
\end{Shaded}

Aplicando el método \texttt{transform}a \texttt{tfidf\_vectorizador} con
los documentos de entrenamiento. Con ello generamos la matriz tf-idf
para los documentos de train , que llamaremos
\texttt{tfidf\_matrix\_train}. Es decir, se calcula el estadistico
tf-idf para cada token obtenido con
\texttt{tfidf\_vectorizador.fit(X\_train)} y cada documento del vector
de entrenamiento \texttt{X\_train}.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{tfidf\_matrix\_train }\OperatorTok{=}\NormalTok{ tfidf\_vectorizador.transform(X\_train)}
\end{Highlighting}
\end{Shaded}

Podemos ver las dimensiones de la matriz tf-idf de train:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_matrix\_train.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(31428, 34571)
\end{verbatim}

Cambiamos el type en \texttt{Python}del vector \texttt{Y\_train} que
antes ha sido creado, para que sea type \texttt{ìnt} , puesto que para
entrenar el modelo de Gaussian naive Bayes será necesario:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y\_train }\OperatorTok{=}\NormalTok{ Y\_train.astype(}\StringTok{\textquotesingle{}int\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Entrenamos el modelo Gaussian naive Bayes en \texttt{Python} usando la
función \texttt{GaussianNB} de \texttt{sklearn} con el método
\texttt{fit}. Para ello usamos como argumentos la matriz tf-idf de
entrenamiento (en formato array , por ello usamos el comando
\texttt{toarray()}) y la respuesta para los documentos de entrenamiento:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gnb\_fit }\OperatorTok{=}\NormalTok{ GaussianNB().fit(tfidf\_matrix\_train.toarray(), Y\_train)}
\end{Highlighting}
\end{Shaded}

Ahora generamos la matriz tf-idf para los documentos de test, es decir,
vamos a gener una matriz cuyas columnas sean los tokens generados con
los documentos de entrenamiento , es decir, los generados con la
instruccion \texttt{tfidf\_vectorizador.fit(X\_train)} , y como filas
los documentos de test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_matrix\_test }\OperatorTok{=}\NormalTok{ tfidf\_vectorizador.transform(X\_test)}
\end{Highlighting}
\end{Shaded}

Podemos ver las dimensiones de la matriz tf-idf de test

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfidf\_matrix\_test.shape}
\end{Highlighting}
\end{Shaded}

(13470, 34571)

Como vemos tiene las mismas columnas que la matriz tf-idf de train
(34571), puesto que las columnas estan asociada a los tokens obtenidos a
partir de los documentos de train, asi que es coeherente que tenga el
mismo numero de columnas que \texttt{tfidf\_matrix\_train}.

Por otro lado el numero de filas difiere con el de la matriz tf-idf de
train, y esto es coherente porque las filas de la matriz tf-idf estan
asociadas a los documentos, y el numero de documentos de test (13470) es
diferente del de train (31428) .

Predecimos la variable respuesta para los documentos de test aplicando a
\texttt{gnb\_fit} el método \texttt{predict} con la matriz tf-idf de
test como argumento:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y\_pred }\OperatorTok{=}\NormalTok{ gnb\_fit.predict(tfidf\_matrix\_test.toarray())}

\NormalTok{Y\_pred}
\end{Highlighting}
\end{Shaded}

array({[}0, 1, 0, \ldots, 1, 0, 0{]})

Como vemos las predicciones de la respuesta son un vector de ceros y
unos. Si la componente \(i\)-esima del vector es \(j\) significa que el
documentos \(i\)-esimo del vector de test pertenece a la clase \(j\) ,
que en este caso significa que el modelo predice que la notica
\(i\)-esima es una fake new, si \(j=1\), y una no fake new, si \(j=0\).

Ahora vamos a calcular el error de clasificación de test, es decir, la
proporcion de documentos (noticias) de test que han sido clasificadas
erroneamente por parte del modelo implementado:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y\_test }\OperatorTok{=}\NormalTok{ Y\_test.astype(}\StringTok{\textquotesingle{}int\textquotesingle{}}\NormalTok{)}

\NormalTok{TEC\_test }\OperatorTok{=}\NormalTok{ (Y\_test }\OperatorTok{!=}\NormalTok{ Y\_pred).}\BuiltInTok{sum}\NormalTok{()}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(Y\_test)}

\NormalTok{TEC\_test}
\end{Highlighting}
\end{Shaded}

0.0593170007423905

Se obtiene un error de clasificación por validación simple del 5.90\% ,
o lo que es lo mismo una tasa de acierto del 95.10\%

\newpage

\hypertarget{bibliografuxeda}{%
\section{Bibliografía}\label{bibliografuxeda}}

\url{https://scikit-learn.org/stable/modules/naive_bayes.html}

\url{https://en.wikipedia.org/wiki/Naive_Bayes_classifier}

\url{https://www.cienciadedatos.net/documentos/py25-text-mining-python.html}

\url{https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html}

\url{https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/}

\url{https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/feature_extraction/text.py\#L1717}

\url{https://es.wikipedia.org/wiki/Tf-idf}

web sklearn, numpy , pandas , wikipedia

\end{document}
