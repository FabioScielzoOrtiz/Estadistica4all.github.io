<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 KNN como algoritmo de clasificación supervisada | Algoritmo K vecinos más cercanos</title>
  <meta name="description" content="Esta es una introducción al algoritmo de regresión y clasificación supervisada KNN." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 KNN como algoritmo de clasificación supervisada | Algoritmo K vecinos más cercanos" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Esta es una introducción al algoritmo de regresión y clasificación supervisada KNN." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 KNN como algoritmo de clasificación supervisada | Algoritmo K vecinos más cercanos" />
  
  <meta name="twitter:description" content="Esta es una introducción al algoritmo de regresión y clasificación supervisada KNN." />
  

<meta name="author" content="Fabio Scielzo Ortiz" />


<meta name="date" content="2023-03-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="problema-de-clasificación-supervisada.html"/>
<link rel="next" href="problema-de-regresión.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">KNN</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="problema-de-clasificación-supervisada.html"><a href="problema-de-clasificación-supervisada.html"><i class="fa fa-check"></i><b>2</b> Problema de clasificación supervisada</a></li>
<li class="chapter" data-level="3" data-path="knn-como-algoritmo-de-clasificación-supervisada.html"><a href="knn-como-algoritmo-de-clasificación-supervisada.html"><i class="fa fa-check"></i><b>3</b> KNN como algoritmo de clasificación supervisada</a>
<ul>
<li class="chapter" data-level="3.1" data-path="knn-como-algoritmo-de-clasificación-supervisada.html"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#knn-para-clasificación-supervisada-con-sklearn"><i class="fa fa-check"></i><b>3.1</b> KNN para clasificación supervisada con <code>sklearn</code></a></li>
<li class="chapter" data-level="3.2" data-path="knn-como-algoritmo-de-clasificación-supervisada.html"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#knn-para-clasificación-supervisada-programado-en-python"><i class="fa fa-check"></i><b>3.2</b> KNN para clasificación supervisada programado en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="problema-de-regresión.html"><a href="problema-de-regresión.html"><i class="fa fa-check"></i><b>4</b> Problema de regresión</a></li>
<li class="chapter" data-level="5" data-path="knn-como-algoritmo-de-regresión.html"><a href="knn-como-algoritmo-de-regresión.html"><i class="fa fa-check"></i><b>5</b> KNN como algoritmo de regresión</a>
<ul>
<li class="chapter" data-level="5.1" data-path="knn-como-algoritmo-de-regresión.html"><a href="knn-como-algoritmo-de-regresión.html#knn-para-regresión-con-sklearn"><i class="fa fa-check"></i><b>5.1</b> KNN para regresión con <code>sklearn</code></a></li>
<li class="chapter" data-level="5.2" data-path="knn-como-algoritmo-de-regresión.html"><a href="knn-como-algoritmo-de-regresión.html#knn-para-regresión-programado-en-python"><i class="fa fa-check"></i><b>5.2</b> KNN para regresión programado en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i><b>6</b> Bibliografía</a></li>
<li class="divider"></li>
<li><a href="https://estadistica4all.com" target="blank">Estadistica4all.com</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Algoritmo K vecinos más cercanos</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="knn-como-algoritmo-de-clasificación-supervisada" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> KNN como algoritmo de clasificación supervisada<a href="knn-como-algoritmo-de-clasificación-supervisada.html#knn-como-algoritmo-de-clasificación-supervisada" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Un algoritmo de clasificación supervisada es un algoritmo que permite predecir una variable respuesta <strong>categórica</strong> usando para ello información sobre una serie de predictores y de la propia respuesta, es decir, es una modelo que permite resolver un problema de clasificación supervisada.</p>
<ul>
<li><p>Se consideran <span class="math inline">\(\hspace{0.02cm} p\hspace{0.02cm}\)</span> predictores <span class="math inline">\(\hspace{0.02cm}\mathcal{X}_1,...,\mathcal{X}_p\hspace{0.02cm}\)</span> y una variable respuesta <strong>categórica</strong> <span class="math inline">\(\hspace{0.02cm}\mathcal{Y}\)</span> .</p></li>
<li><p>La variable respuesta <strong>categorica</strong> tiene <span class="math inline">\(\hspace{0.02cm}g\hspace{0.02cm}\)</span> categorías, ya que <span class="math inline">\(\hspace{0.03cm}R(\mathcal{Y})=\lbrace 0,1,...,g-1 \rbrace\)</span> .</p></li>
<li><p>Se tiene una muestra de observaciones <span class="math inline">\(\hspace{0.02cm}X_r = (x_{1r},...,x_{nr})^t\hspace{0.02cm}\)</span> de la variable <span class="math inline">\(\hspace{0.02cm}\mathcal{X}_r\hspace{0.02cm}\)</span> , para cada <span class="math inline">\(\hspace{0.02cm} r \in \lbrace 1,...,p \rbrace\)</span> .</p></li>
<li><p>Se tiene una muestra de observaciones <span class="math inline">\(\hspace{0.02cm}Y = (y_1,...,y_n)^t\hspace{0.035cm}\)</span> de la variable <span class="math inline">\(\hspace{0.02cm}\mathcal{Y}\)</span> .</p></li>
<li><p>En conclusión, se tiene una muestra de observaciones de los predictores y la respuesta.</p></li>
</ul>
<p><span class="math display">\[D=[\hspace{0.12cm}X_1,...,X_p,Y \hspace{0.12cm}]\hspace{0.12cm}=\begin{pmatrix}
    x_{11}&amp;x_{12}&amp;...&amp;x_{1p}&amp; y_1\\
    x_{21}&amp;x_{22}&amp;...&amp;x_{2p} &amp; y_2\\
    ... &amp;...&amp; ...&amp; .... &amp; ...\\
    x_{n1}&amp;x_{n2}&amp;...&amp;x_{np}&amp; y_n
    \end{pmatrix} = \begin{pmatrix}
    x_{1}&amp; y_1\\
    x_{2}&amp; y_2 \\
     ...&amp;... \\
     x_{n} &amp; y_n
    \end{pmatrix}\]</span></p>
<p><br></p>
<p>A continuación vamos a hacer una exposición teórica del algoritmo de KNN para clasificación supervisada.</p>
<p>El algoritmo <strong>KNN</strong> para <strong>clasificación supervisada</strong> tiene los siguientes pasos:</p>
<div class="warning" style="background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;">
<p><span></p>
<p style="margin-left:1em;">
<ul>
<li><p>Se define una medida de distancia entre pares de observaciones de variables estadisticas <span class="math inline">\(\hspace{0.15cm} \Rightarrow \hspace{0.15cm}\)</span> <span class="math inline">\(\delta\)</span> .</p></li>
<li><p>Dada una nueva observación <span class="math inline">\(\hspace{0.02cm}x_{*}\hspace{0.05cm}=\hspace{0.05cm}(x_{*1} \hspace{0.05cm},\hspace{0.05cm} x_{*2} \hspace{0.05cm},\dots ,\hspace{0.05cm} x_{*p})\hspace{0.02cm}\)</span> de los predictores <span class="math inline">\(\hspace{0.02cm}(\mathcal{X}_1 ,...,\mathcal{X}_p)\hspace{0.02cm}\)</span> , es decir, una observación que no está en la muestra de datos <span class="math inline">\(\hspace{0.02cm}D\hspace{0.02cm}\)</span>, se calculan las distancias entre cada par de observaciones <span class="math inline">\(\hspace{0.02cm}(x_{*} \hspace{0.05cm} , \hspace{0.05cm} x_i)\)</span> <span class="math inline">\(\hspace{0.2cm} \Rightarrow \hspace{0.2cm} \delta(x_{*},x_i) \hspace{0.2cm},\hspace{0.2cm} i = 1,...,n\)</span></p>
<p>Aquí no se va a entrar en este asunto, pero <span class="math inline">\(\hspace{0.02cm}D\hspace{0.02cm}\)</span> está jugando el papel de muestra de entrenamiento.</p></li>
<li><p>Se seleccionan las <span class="math inline">\(\hspace{0.02cm}k\hspace{0.02cm}\)</span> observaciones <span class="math inline">\(\hspace{0.02cm}x_1,...,x_n\hspace{0.02cm}\)</span> que son más cercanas a la nueva observación <span class="math inline">\(\hspace{0.02cm}x_{*}\hspace{0.02cm}\)</span>, según la medida de distancia <span class="math inline">\(\hspace{0.02cm}\delta\hspace{0.02cm}\)</span>.</p>
<p>El conjunto de estas <span class="math inline">\(\hspace{0.02cm}k\hspace{0.02cm}\)</span> observaciones son los <strong>k vecinos más cercanos de <span class="math inline">\(\hspace{0.02cm}x_{*}\hspace{0.02cm}\)</span></strong> y se denota por <span class="math inline">\(\hspace{0.02cm}K_{x_{*}}\)</span></p></li>
<li><p>Se calcula la frecuencia relativa de cada categoría de la variable respuesta en el conjunto <span class="math inline">\(\hspace{0.02cm}K_{x_{*}}\)</span>.</p>
<p>Denotamos por <span class="math inline">\(\hspace{0.02cm}P\hspace{0.05cm}[\hspace{0.02cm} K_{x_{*}} \hspace{0.1cm},\hspace{0.1cm} r\hspace{0.1cm}]\hspace{0.03cm}\)</span> a la frecuencia relativa (proporción) de observaciones del conjunto <span class="math inline">\(\hspace{0.02cm}K_{x_{*}}\hspace{0.02cm}\)</span> tales que <span class="math inline">\(\hspace{0.02cm}\mathcal{Y}=r\hspace{0.02cm}\)</span>.</p>
<p>Es decir:</p>
<p><span class="math display">\[P\hspace{0.05cm}[\hspace{0.02cm} K_{x_{*}} \hspace{0.1cm},\hspace{0.1cm} r \hspace{0.02cm}] \hspace{0.15cm}=\hspace{0.15cm} \dfrac{ \# \hspace{0.1cm}\left\lbrace\hspace{0.1cm} i=1,...,n \hspace{0.15cm}:\hspace{0.15cm} x_i \in K_{x_{*}} \hspace{0.3cm}\text{y}\hspace{0.3cm}  y_i = r \hspace{0.1cm} \right\rbrace  }{\# \hspace{0.1cm} K_{x_{*}} } \\\]</span></p>
<p>Donde <span class="math inline">\(\hspace{0.04cm}\# \hspace{0.1cm} K_{x_{*}} \hspace{0.05cm}=\hspace{0.05cm} k\hspace{0.02cm}\)</span>.</p></li>
<li><p>Para la nueva observación de los predictores <span class="math inline">\(\hspace{0.02cm} x_{*} \hspace{0.02cm}\)</span> se predice la variable respuesta como la categoría más frecuente en el conjunto <span class="math inline">\(\hspace{0.02cm}K_{x_{*}}\)</span></p>
<p>Es decir:</p>
<p><span class="math display">\[\text{Si} \hspace{0.1 cm} r^*  \hspace{0.07 cm}= \hspace{0.07 cm}  arg \hspace{0.15 cm} \underset{r \hspace{0.02 cm}\in \hspace{0.02 cm} R(\mathcal{Y})}{Max} \hspace{0.2cm} P\hspace{0.05cm}[\hspace{0.1cm} K_{x_{*}} \hspace{0.1cm},\hspace{0.1cm} r \hspace{0.1cm}]  \hspace{0.3cm}  \Rightarrow \hspace{0.3cm} \widehat{y}_{*} \hspace{0.05 cm}=\hspace{0.05 cm} r^*\]</span></p>
<p>Donde <span class="math inline">\(\hspace{0.04cm}\widehat{y}_{*}\hspace{0.04cm}\)</span> es el valor de la variable respuesta que el modelo predice para la observación <span class="math inline">\(\hspace{0.03cm}x_{*}\hspace{0.03cm}\)</span> de los predictores.</p></li>
</ul>
</p>
</p>
<p></span></p>
</div>
<p><br></p>
<div id="knn-para-clasificación-supervisada-con-sklearn" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> KNN para clasificación supervisada con <code>sklearn</code><a href="knn-como-algoritmo-de-clasificación-supervisada.html#knn-para-clasificación-supervisada-con-sklearn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Importamos algunos de los paquetes y modulos que vamos a utilizar:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> resample</span>
<span id="cb1-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math <span class="im">as</span> math</span></code></pre></div>
<p>Cargamos los datos con los que vamos a trabajar, los cuales han sido descritos a nivel conceptual en el artículo de regresión lineal.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb2-1" aria-hidden="true" tabindex="-1"></a>Data <span class="op">=</span> pd.read_csv(<span class="st">&#39;House_Price_Regression.csv&#39;</span>)</span></code></pre></div>
<p>Seleccionamos las variables con las que vamos a trabajar:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb3-1" aria-hidden="true" tabindex="-1"></a>Data_Mixed <span class="op">=</span> Data.loc[:, [<span class="st">&#39;latitude&#39;</span>, <span class="st">&#39;longitude&#39;</span>, <span class="st">&#39;price&#39;</span>, <span class="st">&#39;size_in_m_2&#39;</span>, <span class="st">&#39;balcony_recode&#39;</span>, <span class="st">&#39;private_garden_recode&#39;</span>, <span class="st">&#39;quality_recode&#39;</span>]]</span>
<span id="cb3-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb3-2" aria-hidden="true" tabindex="-1"></a>Data_Mixed.head()</span></code></pre></div>
<div style="overflow-x: scroll;">
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
latitude
</th>
<th>
longitude
</th>
<th>
price
</th>
<th>
size_in_m_2
</th>
<th>
balcony_recode
</th>
<th>
private_garden_recode
</th>
<th>
quality_recode
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
25.113208
</td>
<td>
55.138932
</td>
<td>
2700000
</td>
<td>
100.242337
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
25.106809
</td>
<td>
55.151201
</td>
<td>
2850000
</td>
<td>
146.972546
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
25.063302
</td>
<td>
55.137728
</td>
<td>
1150000
</td>
<td>
181.253753
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
3
</th>
<td>
25.227295
</td>
<td>
55.341761
</td>
<td>
2850000
</td>
<td>
187.664060
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
4
</th>
<td>
25.114275
</td>
<td>
55.139764
</td>
<td>
1729200
</td>
<td>
47.101821
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<p>Dividimos el data set inicial en dos partes. Una parte jugará el rol de datos de entrenamiento, con ellos se entrenará el modelo. La otra parte jugara el rol de datos nuevos que no han sido usados para entrenar el modelo, y para los que se quiere predecir la variable respuesta.</p>
<p>En la vida real es conjunto de datos nuevos serían exclusivamente de los predictores, y en base a esa información y el modelo entrenado se predecirian los valores de la respuesta para esos predictores, los cuales serían desconocidos de partida. Pero en este ejemplo no realista disponemos tambien de datos sobre la respuesta en el conjunto de datos nuevos. Esto permitirá medir de algun modo el error que comete el modelo al predecir la respuesta para las nuevas observaciones de los predictores, comparando las predicciones con los valores reales disponibles en la nueva muestra. Este principio es el que está detras de las técnicas de validación cruzada que serán estudiadas con detalle en futuros artículos.</p>
<p>En este caso la variable respuesta será la calidad de la vivienda (quality), que es la variable que se va a predecir. Es decir, dada una vivienda vamos a predecir su calidad, esto es, vamos a clasificarla en una de las categorias definidas por la variable quality.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TRAIN DATA (Datos usados para entrenar el modelo)</span></span>
<span id="cb4-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb4-2" aria-hidden="true" tabindex="-1"></a>Data_Mixed_train <span class="op">=</span> Data_Mixed.sample(frac<span class="op">=</span><span class="fl">0.8</span>, replace<span class="op">=</span><span class="va">False</span>, weights<span class="op">=</span><span class="va">None</span>, random_state<span class="op">=</span><span class="dv">123</span>, axis<span class="op">=</span><span class="va">None</span>, ignore_index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb4-3" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> Data_Mixed_train.loc[: , Data_Mixed_train.columns <span class="op">!=</span> <span class="st">&#39;quality_recode&#39;</span>]</span>
<span id="cb4-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb4-4" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> Data_Mixed_train.loc[: , <span class="st">&#39;quality_recode&#39;</span>]</span>
<span id="cb4-5"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">## NEW DATA (Nuevos datos de los predictores)</span></span>
<span id="cb4-7"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb4-7" aria-hidden="true" tabindex="-1"></a>Data_Mixed_new <span class="op">=</span>  Data_Mixed.drop( Data_Mixed_train.index , )</span>
<span id="cb4-8"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb4-8" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> Data_Mixed_new.loc[: , Data_Mixed_new.columns <span class="op">!=</span> <span class="st">&#39;quality_recode&#39;</span>]</span>
<span id="cb4-9"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># En la práctica real no se tienen datos sobre la respuesta asociado a las nuevas observaciones de los predictores</span></span>
<span id="cb4-10"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Eso es justo lo que se quiere predecir.</span></span>
<span id="cb4-11"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Pero en este ejemplo al usar como &quot;nuevos&quot; datos una parte del data set original, si que tenemos esa información.</span></span>
<span id="cb4-12"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb4-12" aria-hidden="true" tabindex="-1"></a>Y_new <span class="op">=</span> Data_Mixed_new.loc[: , <span class="st">&#39;quality_recode&#39;</span>] </span></code></pre></div>
<p>Veamos la pinta que tienen los data-frames que acabamos de crear:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb5-1" aria-hidden="true" tabindex="-1"></a>X_train.head()</span></code></pre></div>
<div style="overflow-x: scroll;">
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
latitude
</th>
<th>
longitude
</th>
<th>
price
</th>
<th>
size_in_m_2
</th>
<th>
balcony_recode
</th>
<th>
private_garden_recode
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
382
</th>
<td>
25.196489
</td>
<td>
55.272126
</td>
<td>
15800000
</td>
<td>
488.019459
</td>
<td>
0.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
732
</th>
<td>
25.107984
</td>
<td>
55.244923
</td>
<td>
1700000
</td>
<td>
138.704179
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
1888
</th>
<td>
25.071504
</td>
<td>
55.128579
</td>
<td>
1300000
</td>
<td>
171.220229
</td>
<td>
1.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
679
</th>
<td>
25.054336
</td>
<td>
55.203423
</td>
<td>
999000
</td>
<td>
116.035847
</td>
<td>
1.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
1004
</th>
<td>
25.087251
</td>
<td>
55.145574
</td>
<td>
2990000
</td>
<td>
162.208638
</td>
<td>
1.0
</td>
<td>
0.0
</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb6-1" aria-hidden="true" tabindex="-1"></a>X_new.head()</span></code></pre></div>
<div style="overflow-x: scroll;">
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
latitude
</th>
<th>
longitude
</th>
<th>
price
</th>
<th>
size_in_m_2
</th>
<th>
balcony_recode
</th>
<th>
private_garden_recode
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
25.106809
</td>
<td>
55.151201
</td>
<td>
2850000
</td>
<td>
146.972546
</td>
<td>
1.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
25.063302
</td>
<td>
55.137728
</td>
<td>
1150000
</td>
<td>
181.253753
</td>
<td>
1.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
3
</th>
<td>
25.227295
</td>
<td>
55.341761
</td>
<td>
2850000
</td>
<td>
187.664060
</td>
<td>
1.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
8
</th>
<td>
25.106668
</td>
<td>
55.149275
</td>
<td>
2100000
</td>
<td>
203.085958
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
16
</th>
<td>
25.132021
</td>
<td>
55.151405
</td>
<td>
3499000
</td>
<td>
151.710599
</td>
<td>
1.0
</td>
<td>
0.0
</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb7-1" aria-hidden="true" tabindex="-1"></a>Y_train.head()</span></code></pre></div>
<pre><code>382     2.0
732     1.0
1888    2.0
679     0.0
1004    2.0
Name: quality_recode, dtype: float64</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb9-1" aria-hidden="true" tabindex="-1"></a>Y_new.head()</span></code></pre></div>
<pre><code>1     2.0
2     2.0
3     1.0
8     1.0
16    2.0
Name: quality_recode, dtype: float64</code></pre>
<p><br></p>
<p>Importamos la libreria <code>sklearn</code>y los modulos necesarios para aplicar KNN para clasificación supervisada en <code>Python</code>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb11-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> NearestNeighbors</span></code></pre></div>
<p>En la siguiente caja de codigo podemos ver todos los parametros disponibles para la función <code>KNeighborsClassifier</code>, la cual permite implementar el algoritmo KNN para clasificación supervisada.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sklearn.neighbors.KNeighborsClassifier(n_neighbors=10, *, weights=&#39;uniform&#39;, algorithm=&#39;auto&#39;, leaf_size=30, p=2, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None) </span></span></code></pre></div>
<p><br></p>
<p>Inicializamos algunos parametros del algoritmo. En concreto usaremos <span class="math inline">\(k=10\)</span> vecinos y la distancia de Minkowski con <span class="math inline">\(p=2\)</span>, es decir, la distancia <strong>Euclidea</strong>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb13-1" aria-hidden="true" tabindex="-1"></a>knn_classification <span class="op">=</span> sklearn.neighbors.KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">10</span>, p<span class="op">=</span><span class="dv">2</span>, metric<span class="op">=</span><span class="st">&#39;minkowski&#39;</span>)</span></code></pre></div>
<p>Entrenamos el modelo usando los data-frames <strong>X_train</strong> y <strong>Y_train</strong>, que contienen observaciones de los predictores y la respuesta, respectivamente.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb14-1" aria-hidden="true" tabindex="-1"></a>knn_classification.fit(X_train, Y_train)</span></code></pre></div>
<p>Usando el método <code>predict</code> podemos predecir la respuesta para cada una de las observaciones de los predictores que consideremos. Para ello tenemos que pasarle al método los vectores de observaciones como filas de un array, en este caso esto ya lo tenemos en el data-frame X_new, que contiene las nuevas observaciones de los predictores como filas.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb15-1" aria-hidden="true" tabindex="-1"></a>knn_classification.predict(X_new) </span></code></pre></div>
<pre><code>array([2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2.,
       1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,
       2., 2., 2., 0., 2., 2., 3., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2.,
       2., 2., 2., 1., 2., 2., 2., 1., 2., 2., 2., 2., 1., 2., 2., 1., 1.,
       2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2., 2., 2., 2.,
       2., 2., 1., 2., 2., 1., 2., 2., 2., 2., 2., 2., 1., 1., 1., 2., 2.,
       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2.,
       1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 2.,
       2., 1., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2.,
       1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1.,
       2., 2., 2., 2., 1., 1., 2., 2., 2., 2., 2., 1., 2., 2., 1., 1., 2.,
       2., 1., 2., 2., 2., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2., 2.,
       2., 1., 2., 2., 2., 3., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,
       3., 1., 2., 2., 2., 2., 2., 3., 2., 2., 3., 2., 2., 2., 2., 1., 2.,
       1., 1., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1.,
       2., 2., 2., 2., 2., 1., 2., 2., 2., 1., 2., 2., 2., 2., 0., 2., 2.,
       2., 1., 2., 0., 2., 2., 1., 2., 1., 2., 2., 2., 1., 1., 1., 2., 2.,
       2., 1., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 1., 2., 2., 2., 1.,
       1., 1., 2., 2., 2., 2., 1., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2.,
       2., 2., 2., 1., 2., 2., 2., 2., 1., 2., 2., 2., 1., 2., 2., 2., 2.,
       2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 1., 2., 2., 2., 1., 2., 2.,
       2., 2., 1., 1., 2., 2., 2., 1., 2., 2., 1., 2., 2., 1., 2., 1., 2.,
       2., 2., 2., 2., 2., 2., 1.])</code></pre>
<p><br></p>
<p>Como disponemos de los valores reales de la respuesta para las nuevas obserciones de los predictores, podemos calcular la tasa de acierto en la clasificación como la proporción de clasificaciones correctas :</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb17-1" aria-hidden="true" tabindex="-1"></a>TA <span class="op">=</span> <span class="bu">sum</span>( knn_classification.predict( X_new ) <span class="op">==</span> Y_new.reset_index().quality_recode ) <span class="op">/</span> <span class="bu">len</span>(Y_new)</span>
<span id="cb17-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb17-2" aria-hidden="true" tabindex="-1"></a>TA</span></code></pre></div>
<pre><code>0.5249343832020997</code></pre>
<p><br></p>
<p>Podemos probar el algoritmo con otras funciones de distancia.</p>
<p>Por ejemplo con la distancia de Minkowski con <span class="math inline">\(p=1\)</span>, es decir, con la distancia <strong>Manhattan</strong>:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb19-1" aria-hidden="true" tabindex="-1"></a>knn_classification <span class="op">=</span> sklearn.neighbors.KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">10</span> , p<span class="op">=</span><span class="dv">1</span>,  metric<span class="op">=</span><span class="st">&#39;minkowski&#39;</span>)</span>
<span id="cb19-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb19-2" aria-hidden="true" tabindex="-1"></a>knn_classification.fit(X_train, Y_train)</span></code></pre></div>
<p>Calculamos la tasa de acierto de la clasificación:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb20-1" aria-hidden="true" tabindex="-1"></a>TA <span class="op">=</span> <span class="bu">sum</span>( knn_classification.predict( X_new ) <span class="op">==</span> Y_new.reset_index().quality_recode ) <span class="op">/</span> <span class="bu">len</span>(Y_new)</span>
<span id="cb20-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb20-2" aria-hidden="true" tabindex="-1"></a>TA</span></code></pre></div>
<pre><code>0.5275590551181102</code></pre>
<p><br></p>
<p>Probamos ahora con la distancia <strong>coseno</strong>:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb22-1" aria-hidden="true" tabindex="-1"></a>knn_classification <span class="op">=</span> sklearn.neighbors.KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">10</span> ,   metric<span class="op">=</span><span class="st">&#39;cosine&#39;</span>)</span>
<span id="cb22-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb22-3" aria-hidden="true" tabindex="-1"></a>knn_classification.fit(X_train, Y_train)</span></code></pre></div>
<p>Calculamos la tasa de acierto de la clasificación:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb23-1" aria-hidden="true" tabindex="-1"></a>TA <span class="op">=</span> <span class="bu">sum</span>( knn_classification.predict( X_new ) <span class="op">==</span> Y_new.reset_index().quality_recode ) <span class="op">/</span> <span class="bu">len</span>(Y_new)</span>
<span id="cb23-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb23-2" aria-hidden="true" tabindex="-1"></a>TA</span></code></pre></div>
<pre><code>0.5643044619422573</code></pre>
<p><br></p>
</div>
<div id="knn-para-clasificación-supervisada-programado-en-python" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> KNN para clasificación supervisada programado en <code>Python</code><a href="knn-como-algoritmo-de-clasificación-supervisada.html#knn-para-clasificación-supervisada-programado-en-python" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Vamos a crear una función que replique el algoritmo KNN para clasificación supervisada que anteriormente fue descrito. Este es un buen ejercicio para entender mejor como funciona realmente el algoritmo, y para practicar nuestra programación.</p>
<p><strong>Primero definimos algunas de las funciones de distancia que usará nuestro algoritmo:</strong></p>
<p>Para ver con más detalle estas funciones y conceptos se recomienda visitar el artículo dedicado a este tema que tenemos en nuestro blog: <a href="https://fabioscielzoortiz.github.io/Statisitcal_Distances_Bookdown/">distancias estadísticas</a></p>
<ul>
<li>Definimos la distancia Euclidea y la matriz de distancias Euclideas:</li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Dist_Euclidea(x_i, x_r):</span>
<span id="cb25-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb25-2" aria-hidden="true" tabindex="-1"></a>        Dist_Euclidea <span class="op">=</span> ( ( x_i <span class="op">-</span> x_r )<span class="op">**</span><span class="dv">2</span> ).<span class="bu">sum</span>()</span>
<span id="cb25-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb25-3" aria-hidden="true" tabindex="-1"></a>        Dist_Euclidea <span class="op">=</span> np.sqrt(Dist_Euclidea)</span>
<span id="cb25-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb25-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> Dist_Euclidea</span></code></pre></div>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Matrix_Dist_Euclidea(Data):</span>
<span id="cb26-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb26-2" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(Data)</span>
<span id="cb26-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb26-3" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span>  np.empty((n , n))</span>
<span id="cb26-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n):</span>
<span id="cb26-5"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb26-5" aria-hidden="true" tabindex="-1"></a>         <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n):</span>
<span id="cb26-6"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb26-6" aria-hidden="true" tabindex="-1"></a>             <span class="cf">if</span> i <span class="op">&gt;=</span> r :</span>
<span id="cb26-7"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb26-7" aria-hidden="true" tabindex="-1"></a>                 M[i,r] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb26-8"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb26-8" aria-hidden="true" tabindex="-1"></a>             <span class="cf">else</span> :</span>
<span id="cb26-9"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb26-9" aria-hidden="true" tabindex="-1"></a>                 M[i,r] <span class="op">=</span> Dist_Euclidea(Data[i,:] , Data[r,:])</span>
<span id="cb26-10"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb26-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> M </span></code></pre></div>
<p><br></p>
<ul>
<li>Definimos la distancia Minkowski y la matriz de distancias Minkowski:</li>
</ul>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Dist_Minkowski(x_i, x_r, q):</span>
<span id="cb27-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb27-2" aria-hidden="true" tabindex="-1"></a>    Dist_Minkowski <span class="op">=</span> ( ( ( <span class="bu">abs</span>( x_i <span class="op">-</span> x_r) )<span class="op">**</span>q ).<span class="bu">sum</span>() )<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span>q)</span>
<span id="cb27-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Dist_Minkowski</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Matrix_Dist_Minkowski(Data, q):</span>
<span id="cb28-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb28-2" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(Data)</span>
<span id="cb28-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb28-3" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span>  np.empty((n , n))</span>
<span id="cb28-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n):</span>
<span id="cb28-6"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb28-6" aria-hidden="true" tabindex="-1"></a>         <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n):</span>
<span id="cb28-7"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb28-7" aria-hidden="true" tabindex="-1"></a>             <span class="cf">if</span> i <span class="op">&gt;=</span> r :</span>
<span id="cb28-8"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb28-8" aria-hidden="true" tabindex="-1"></a>                 M[i,r] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb28-9"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb28-9" aria-hidden="true" tabindex="-1"></a>             <span class="cf">else</span> :</span>
<span id="cb28-10"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb28-10" aria-hidden="true" tabindex="-1"></a>                 M[i,r] <span class="op">=</span> Dist_Minkowski(Data[i,:] , Data[r,:], q)   </span>
<span id="cb28-11"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> M </span></code></pre></div>
<p><br></p>
<ul>
<li>Definimos la distancia Canberra y la matriz de distancias Canberra:</li>
</ul>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Dist_Canberra(x_i, x_r):</span>
<span id="cb29-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb29-2" aria-hidden="true" tabindex="-1"></a>    numerator <span class="op">=</span>  <span class="bu">abs</span>( x_i <span class="op">-</span> x_r )</span>
<span id="cb29-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb29-3" aria-hidden="true" tabindex="-1"></a>    denominator <span class="op">=</span>  ( <span class="bu">abs</span>(x_i) <span class="op">+</span> <span class="bu">abs</span>(x_r) )</span>
<span id="cb29-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb29-4" aria-hidden="true" tabindex="-1"></a>    numerator<span class="op">=</span>np.array([numerator], dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb29-5"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb29-5" aria-hidden="true" tabindex="-1"></a>    denominator<span class="op">=</span>np.array([denominator], dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb29-6"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb29-6" aria-hidden="true" tabindex="-1"></a>    Dist_Canberra <span class="op">=</span> ( np.divide( numerator , denominator , out<span class="op">=</span>np.zeros_like(numerator), where<span class="op">=</span>denominator<span class="op">!=</span><span class="dv">0</span>) ).<span class="bu">sum</span>() </span>
<span id="cb29-7"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Dist_Canberra</span></code></pre></div>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Matrix_Dist_Canberra(Data):</span>
<span id="cb30-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb30-2" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(Data)</span>
<span id="cb30-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb30-3" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span>  np.empty((n , n))</span>
<span id="cb30-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n):</span>
<span id="cb30-5"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb30-5" aria-hidden="true" tabindex="-1"></a>         <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n):</span>
<span id="cb30-6"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb30-6" aria-hidden="true" tabindex="-1"></a>             <span class="cf">if</span> i <span class="op">&gt;=</span> r :</span>
<span id="cb30-7"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb30-7" aria-hidden="true" tabindex="-1"></a>                 M[i,r] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-8"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb30-8" aria-hidden="true" tabindex="-1"></a>             <span class="cf">else</span> :</span>
<span id="cb30-9"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb30-9" aria-hidden="true" tabindex="-1"></a>                 M[i,r] <span class="op">=</span> Dist_Canberra(Data[i,:] , Data[r,:]</span>
<span id="cb30-10"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb30-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> M </span></code></pre></div>
<p><br></p>
<ul>
<li>Definimos la distancia Mahalanobis y la matriz de distancias de Mahalanobis:</li>
</ul>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Dist_Mahalanobis_2(x_i, x_r, S_inv):</span>
<span id="cb31-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb31-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> (x_i <span class="op">-</span> x_r)</span>
<span id="cb31-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb31-3" aria-hidden="true" tabindex="-1"></a>    Dist_Maha <span class="op">=</span> np.sqrt( x <span class="op">@</span> S_inv <span class="op">@</span> x.T )  </span>
<span id="cb31-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb31-4" aria-hidden="true" tabindex="-1"></a>    Dist_Maha <span class="op">=</span> <span class="bu">float</span>(Dist_Maha)</span>
<span id="cb31-5"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Dist_Maha</span></code></pre></div>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Dist_Mahalanobis_3(x, S_inv):  <span class="co">#</span></span>
<span id="cb32-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb32-2" aria-hidden="true" tabindex="-1"></a>    Dist_Maha <span class="op">=</span> np.sqrt( x <span class="op">@</span> S_inv <span class="op">@</span> x.T ) </span>
<span id="cb32-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb32-3" aria-hidden="true" tabindex="-1"></a>    Dist_Maha <span class="op">=</span> <span class="bu">float</span>(Dist_Maha)</span>
<span id="cb32-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Dist_Maha</span></code></pre></div>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Matrix_Dist_Mahalanobis_3(Data):</span>
<span id="cb33-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb33-2" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(Data)</span>
<span id="cb33-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb33-3" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span>  np.empty((n , n))</span>
<span id="cb33-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb33-4" aria-hidden="true" tabindex="-1"></a>    S_inv<span class="op">=</span>np.linalg.inv( np.cov(Data , rowvar<span class="op">=</span><span class="va">False</span>) )</span>
<span id="cb33-5"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n):</span>
<span id="cb33-6"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb33-6" aria-hidden="true" tabindex="-1"></a>         <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n):</span>
<span id="cb33-7"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb33-7" aria-hidden="true" tabindex="-1"></a>             <span class="cf">if</span> i <span class="op">&gt;=</span> r :</span>
<span id="cb33-8"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb33-8" aria-hidden="true" tabindex="-1"></a>                 M[i,r] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb33-9"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb33-9" aria-hidden="true" tabindex="-1"></a>             <span class="cf">else</span> :</span>
<span id="cb33-10"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb33-10" aria-hidden="true" tabindex="-1"></a>                 M[i,r] <span class="op">=</span> Dist_Mahalanobis_3(x <span class="op">=</span> np.array([Data[i,:] <span class="op">-</span> Data[r,:]]) , S_inv<span class="op">=</span>S_inv ) </span>
<span id="cb33-11"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb33-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-12"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb33-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> M </span></code></pre></div>
<p><br></p>
<ul>
<li>Definimos la matriz de distancias de Gower:</li>
</ul>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> a_b_c_d_Matrix(Data):</span>
<span id="cb34-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb34-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> Data</span>
<span id="cb34-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb34-3" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> X <span class="op">@</span> X.T</span>
<span id="cb34-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb34-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb34-5"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb34-5" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb34-6"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb34-6" aria-hidden="true" tabindex="-1"></a>    ones_matrix <span class="op">=</span> np.ones((n, p)) </span>
<span id="cb34-7"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb34-7" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> (ones_matrix <span class="op">-</span> X) <span class="op">@</span> X.T</span>
<span id="cb34-8"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb34-8" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> b.T</span>
<span id="cb34-9"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb34-9" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> (ones_matrix <span class="op">-</span> X) <span class="op">@</span> (ones_matrix <span class="op">-</span> X).T</span>
<span id="cb34-10"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb34-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a , b , c , d , p</span></code></pre></div>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> alpha(x_i, x_r):</span>
<span id="cb35-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb35-2" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> <span class="bu">sum</span>(x_i <span class="op">==</span> x_r)</span>
<span id="cb35-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(alpha)</span></code></pre></div>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Matrix_Gower_Similarity(Data, p1, p2, p3 ):</span>
<span id="cb36-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-2" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(Data)</span>
<span id="cb36-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-3" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span>  np.empty((n , n))</span>
<span id="cb36-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-4" aria-hidden="true" tabindex="-1"></a>    G_vector <span class="op">=</span> np.repeat(<span class="fl">0.5</span>, p1)</span>
<span id="cb36-5"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, p1):</span>
<span id="cb36-6"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-6" aria-hidden="true" tabindex="-1"></a>        G_vector[k] <span class="op">=</span> Data[:,k].<span class="bu">max</span>() <span class="op">-</span> Data[:,k].<span class="bu">min</span>()</span>
<span id="cb36-7"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-7" aria-hidden="true" tabindex="-1"></a>    ones <span class="op">=</span> np.repeat(<span class="dv">1</span>, p1)</span>
<span id="cb36-8"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-8" aria-hidden="true" tabindex="-1"></a>    Quant_Data <span class="op">=</span> Data[: , <span class="dv">0</span>:p1]</span>
<span id="cb36-9"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-9" aria-hidden="true" tabindex="-1"></a>    Binary_Data <span class="op">=</span> Data[: , (p1):(p1<span class="op">+</span>p2)]</span>
<span id="cb36-10"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-10" aria-hidden="true" tabindex="-1"></a>    Multiple_Data <span class="op">=</span> Data[: , (p1<span class="op">+</span>p2):(p1<span class="op">+</span>p2<span class="op">+</span>p3) ]</span>
<span id="cb36-11"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-11" aria-hidden="true" tabindex="-1"></a>    a, b, c, d, p <span class="op">=</span> a_b_c_d_Matrix(Binary_Data)</span>
<span id="cb36-12"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n):</span>
<span id="cb36-13"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-13" aria-hidden="true" tabindex="-1"></a>         <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n):</span>
<span id="cb36-14"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">&gt;</span> r :</span>
<span id="cb36-15"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-15" aria-hidden="true" tabindex="-1"></a>                 M[i,r] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-16"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> i <span class="op">==</span> r :</span>
<span id="cb36-17"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-17" aria-hidden="true" tabindex="-1"></a>                 M[i,r] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb36-18"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> :</span>
<span id="cb36-19"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-19" aria-hidden="true" tabindex="-1"></a>                numerator_part_1 <span class="op">=</span> ( ones <span class="op">-</span> ( <span class="bu">abs</span>(Quant_Data[i,:] <span class="op">-</span> Quant_Data[r,:]) <span class="op">/</span> G_vector ) ).<span class="bu">sum</span>() </span>
<span id="cb36-20"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-20" aria-hidden="true" tabindex="-1"></a>                numerator_part_2 <span class="op">=</span> a[i,r] <span class="op">+</span> alpha(Multiple_Data[i,:], Multiple_Data[r,:])</span>
<span id="cb36-21"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-21" aria-hidden="true" tabindex="-1"></a>                numerator <span class="op">=</span> numerator_part_1 <span class="op">+</span> numerator_part_2</span>
<span id="cb36-22"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> p1 <span class="op">+</span> (p2 <span class="op">-</span> d[i,r]) <span class="op">+</span> p3 <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb36-23"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-23" aria-hidden="true" tabindex="-1"></a>                    M[i,r] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-24"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb36-25"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-25" aria-hidden="true" tabindex="-1"></a>                    denominator <span class="op">=</span> p1 <span class="op">+</span> (p2 <span class="op">-</span> d[i,r]) <span class="op">+</span> p3</span>
<span id="cb36-26"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-26" aria-hidden="true" tabindex="-1"></a>                    M[i,r] <span class="op">=</span> numerator <span class="op">/</span> denominator</span>
<span id="cb36-27"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb36-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> M  </span></code></pre></div>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Matrix_Gower_Distance(Data, p1, p2, p3 ):</span>
<span id="cb37-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb37-2" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> Matrix_Gower_Similarity(Data, p1, p2, p3)</span>
<span id="cb37-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb37-3" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> M <span class="op">+</span> M.T <span class="op">-</span> np.diag(np.repeat(<span class="dv">1</span> , <span class="bu">len</span>(M)), k<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb37-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb37-4" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> np.sqrt( <span class="dv">1</span> <span class="op">-</span> M )</span>
<span id="cb37-5"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb37-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> M</span></code></pre></div>
<p><br></p>
<p><br></p>
<ul>
<li><strong>Programamos el algoritmo KNN para clasificación supervisada:</strong></li>
</ul>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> KNNClassification:</span>
<span id="cb38-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># k es el hiper-parametro del algoritmo KNN.</span></span>
<span id="cb38-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co"># &#39;Distance_Matrix_New_Data&#39; debe contenere las distancias entre las nuevas observaciones de los predictores y las de train.</span></span>
<span id="cb38-4"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Su fila i-esima debe contener las distancias entre la i-esima nueva observacion de los predictores y las observaciones de train de los predictores.</span></span>
<span id="cb38-5"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co"># X_train tiene que ser un data-frame con las observaciones de los predictores con las que se va a entrenar el modelo.</span></span>
<span id="cb38-6"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Y_train tiene que ser un numpy array que contenga las observaciones de la respuesta con las que se va a entrenar el modelo.</span></span>
<span id="cb38-7"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-8"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, k, X_train, Y_train, distance_matrix_new_data):</span>
<span id="cb38-9"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> k</span>
<span id="cb38-10"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.X_train <span class="op">=</span> X_train</span>
<span id="cb38-11"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Y_train <span class="op">=</span> Y_train</span>
<span id="cb38-12"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.distance_matrix_new_data <span class="op">=</span> distance_matrix_new_data</span>
<span id="cb38-13"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.predictions <span class="op">=</span> <span class="va">None</span></span>
<span id="cb38-14"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-15"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>):</span>
<span id="cb38-16"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-16" aria-hidden="true" tabindex="-1"></a>        Y_predict_x_new_i_LIST <span class="op">=</span> []</span>
<span id="cb38-17"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(<span class="va">self</span>.distance_matrix_new_data)):</span>
<span id="cb38-18"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-18" aria-hidden="true" tabindex="-1"></a>            distancias_x_new_i <span class="op">=</span> pd.DataFrame({<span class="st">&#39;id_x_train&#39;</span>: <span class="va">self</span>.X_train.index, <span class="st">&#39;distancias&#39;</span>: <span class="va">self</span>.distance_matrix_new_data[i,:]})</span>
<span id="cb38-19"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-19" aria-hidden="true" tabindex="-1"></a>            distancias_x_new_i_sort <span class="op">=</span> distancias_x_new_i.sort_values(by<span class="op">=</span>[<span class="st">&quot;distancias&quot;</span>]).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-20"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-20" aria-hidden="true" tabindex="-1"></a>            knn_x_new_i <span class="op">=</span> distancias_x_new_i_sort.iloc[<span class="dv">0</span>:<span class="va">self</span>.k, <span class="dv">0</span>]</span>
<span id="cb38-21"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-21" aria-hidden="true" tabindex="-1"></a>            categorias_knn_x_new_i <span class="op">=</span> []</span>
<span id="cb38-22"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> knn_x_new_i :</span>
<span id="cb38-23"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-23" aria-hidden="true" tabindex="-1"></a>               categorias_knn_x_new_i.append(<span class="va">self</span>.Y_train[j])</span>
<span id="cb38-24"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-24" aria-hidden="true" tabindex="-1"></a>            unique, counts <span class="op">=</span> np.unique(categorias_knn_x_new_i , return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-25"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-25" aria-hidden="true" tabindex="-1"></a>            unique_Y , counts_Y <span class="op">=</span> np.unique(<span class="va">self</span>.Y_train , return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-26"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(unique) <span class="op">==</span> <span class="bu">len</span>(unique_Y) :</span>
<span id="cb38-27"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-27" aria-hidden="true" tabindex="-1"></a>               proporciones_categorias_knn_x_new_i <span class="op">=</span> pd.DataFrame({<span class="st">&#39;prop_categoria&#39;</span>: counts<span class="op">/</span><span class="va">self</span>.k, <span class="st">&#39;categoria&#39;</span>: unique_Y })</span>
<span id="cb38-28"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="bu">len</span>(unique) <span class="op">&lt;</span> <span class="bu">len</span>(unique_Y) :</span>
<span id="cb38-29"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-29" aria-hidden="true" tabindex="-1"></a>               proporciones_categorias_knn_x_new_i <span class="op">=</span> pd.DataFrame({<span class="st">&#39;prop_categoria&#39;</span>: counts<span class="op">/</span><span class="va">self</span>.k, <span class="st">&#39;categoria&#39;</span>: unique })</span>
<span id="cb38-30"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-30" aria-hidden="true" tabindex="-1"></a>            Y_predict_x_new_i <span class="op">=</span> proporciones_categorias_knn_x_new_i.sort_values(by<span class="op">=</span>[<span class="st">&quot;prop_categoria&quot;</span>], ascending<span class="op">=</span><span class="va">False</span>).iloc[<span class="dv">0</span> , :][<span class="st">&#39;categoria&#39;</span>]  </span>
<span id="cb38-31"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-31" aria-hidden="true" tabindex="-1"></a>            Y_predict_x_new_i_LIST.append(Y_predict_x_new_i)</span>
<span id="cb38-32"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df_predictions <span class="op">=</span> pd.DataFrame({<span class="st">&#39;id_x_new&#39;</span>:<span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(<span class="va">self</span>.distance_matrix_new_data)) , <span class="st">&#39;Y_predict&#39;</span>: Y_predict_x_new_i_LIST})</span>
<span id="cb38-33"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb38-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.predictions <span class="op">=</span> <span class="va">self</span>.df_predictions[<span class="st">&#39;Y_predict&#39;</span>]</span></code></pre></div>
<p><br></p>
<p>Probaremos el algoritmo que acabamos de programar con la distancia de <strong>Gower</strong>, ya que es una distancia que no está disponible en la implementación de <code>sklearn</code>.</p>
<p>Para ello necesitamos calcular la matriz de distancias de Gower entre las nuevas observaciones y las observaciones de entrenamiento de los predictores. Esto lo podemos conseguir como sigue:</p>
<p>Primero obtenemos el conjunto total de observaciones de los predicitores. Es decir, un data-frame que se puede construir a partir de la concatenación por filas de los data-frames <code>X_train</code> y <code>X_new</code>.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb39-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.concat([X_train , X_new])</span>
<span id="cb39-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb39-2" aria-hidden="true" tabindex="-1"></a>X.head()</span></code></pre></div>
<div style="overflow-x: scroll;">
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
latitude
</th>
<th>
longitude
</th>
<th>
price
</th>
<th>
size_in_m_2
</th>
<th>
balcony_recode
</th>
<th>
private_garden_recode
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
382
</th>
<td>
25.196489
</td>
<td>
55.272126
</td>
<td>
15800000
</td>
<td>
488.019459
</td>
<td>
0.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
732
</th>
<td>
25.107984
</td>
<td>
55.244923
</td>
<td>
1700000
</td>
<td>
138.704179
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
1888
</th>
<td>
25.071504
</td>
<td>
55.128579
</td>
<td>
1300000
</td>
<td>
171.220229
</td>
<td>
1.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
679
</th>
<td>
25.054336
</td>
<td>
55.203423
</td>
<td>
999000
</td>
<td>
116.035847
</td>
<td>
1.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
1004
</th>
<td>
25.087251
</td>
<td>
55.145574
</td>
<td>
2990000
</td>
<td>
162.208638
</td>
<td>
1.0
</td>
<td>
0.0
</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<p>Calculamos la matriz de distancias de gower entre las observaciones totales (de train y new) de los predictores.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb40-1" aria-hidden="true" tabindex="-1"></a>M_Gower <span class="op">=</span> Matrix_Gower_Distance(Data<span class="op">=</span>X.to_numpy(), p1<span class="op">=</span><span class="dv">4</span>, p2<span class="op">=</span><span class="dv">2</span>, p3<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb40-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb40-2" aria-hidden="true" tabindex="-1"></a>M_Gower</span></code></pre></div>
<pre><code>array([[0.        , 0.64817837, 0.76118906, ..., 0.70588672, 0.62389777,
        0.76544317],
       [0.64817837, 0.        , 0.5387377 , ..., 0.51390107, 0.61030246,
        0.53192923],
       [0.76118906, 0.5387377 , 0.        , ..., 0.38695255, 0.48555238,
        0.20821652],
       ...,
       [0.70588672, 0.51390107, 0.38695255, ..., 0.        , 0.477898  ,
        0.33432523],
       [0.62389777, 0.61030246, 0.48555238, ..., 0.477898  , 0.        ,
        0.45160134],
       [0.76544317, 0.53192923, 0.20821652, ..., 0.33432523, 0.45160134,
        0.        ]])</code></pre>
<p>Por un lado, nos quedamos con las filas de la 1524 (<em>len(X_train)</em>) a la 1904 (<em>len(X)</em>) , que son las correspondientes a las nuevas observaciones de los predictores.</p>
<p>Por otro lado, nos quedamos con las columnas de la 0 a la 1524, que son las correspondientes a las observaciones de train de los predictores.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb42-1" aria-hidden="true" tabindex="-1"></a>M_Gower_new_data <span class="op">=</span> M_Gower[ <span class="bu">len</span>(X_train):<span class="bu">len</span>(X) , <span class="dv">0</span>:<span class="bu">len</span>(X_train) ]  </span></code></pre></div>
<p><strong>M_Gower_new_data</strong> contiene las distancias (de Gower) entre las nuevas observaciones de los predictores y las de entrenamiento (train).</p>
<p>Su fila <span class="math inline">\(i\)</span>-esima contiene las distancias entre la <span class="math inline">\(i\)</span>-esima nueva observación de los predictores y cada una de las observaciones de entrenamiento.</p>
<p>Podemos comprobar las dimensiones de esta matriz.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb43-1" aria-hidden="true" tabindex="-1"></a>M_Gower_new_data.shape</span></code></pre></div>
<pre><code>(381, 1524)</code></pre>
<p><br></p>
<p>Ejecutamos el algoritmo KNN que hemos programado con la distancia de <strong>Gower</strong>, es decir, usamos la matriz de distancias de Gower entre las observaciones de train y las nuevas observaciones de los predictores. Fijaremos, además, <span class="math inline">\(k=10\)</span> :</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb45-1" aria-hidden="true" tabindex="-1"></a>KNNClassification_init <span class="op">=</span> KNNClassification(k<span class="op">=</span><span class="dv">10</span>, X_train<span class="op">=</span>X_train, Y_train<span class="op">=</span>Y_train, distance_matrix_new_data<span class="op">=</span>M_Gower_new_data)</span>
<span id="cb45-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb45-2" aria-hidden="true" tabindex="-1"></a>KNNClassification_init.predict()</span></code></pre></div>
<p><br></p>
<p><code>KNNClassification_init.df_predictions</code> almacena un data-frame que contiene la predicción de la variable respuesta para cada una de las nuevas observaciones de los predictores, es decir, para cada nuevo “individuo”.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb46-1" aria-hidden="true" tabindex="-1"></a>KNNClassification_init.df_predictions</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right"></th>
<th align="right">id_x_new</th>
<th align="right">Y_predict</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">0</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">2</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">3</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">4</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">…</td>
<td align="right">…</td>
<td align="right">…</td>
</tr>
<tr class="odd">
<td align="right">376</td>
<td align="right">376</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">377</td>
<td align="right">377</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">378</td>
<td align="right">378</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">379</td>
<td align="right">379</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="right">380</td>
<td align="right">380</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>También podemos acceder directamente a las predicciones generadas por el algoritmo con <code>KNNClassification_init.predictions</code>.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb47-1" aria-hidden="true" tabindex="-1"></a>KNNClassification_init.predictions</span></code></pre></div>
<pre><code>0      2.0
1      2.0
2      2.0
3      1.0
4      2.0
      ... 
376    2.0
377    2.0
378    2.0
379    0.0
380    2.0
Name: Y_predict, Length: 381, dtype: float64</code></pre>
<p><br></p>
<p>Podemos calcular la tasa de acierto en la clasificación obtenida con el algoritmo usando nuestra función con la distancia de <strong>Gower</strong>:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb49-1" aria-hidden="true" tabindex="-1"></a>TA <span class="op">=</span> <span class="bu">sum</span>( KNNClassification_init.predictions <span class="op">==</span> Y_new.reset_index().quality_recode ) <span class="op">/</span> <span class="bu">len</span>(Y_new)</span>
<span id="cb49-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb49-2" aria-hidden="true" tabindex="-1"></a>TA</span></code></pre></div>
<pre><code>0.7611548556430446</code></pre>
<p><br></p>
<p>Ahora probamos el algoritmo con la distancia <strong>Euclidea</strong>:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb51-1" aria-hidden="true" tabindex="-1"></a>M_Euclidea <span class="op">=</span> Matrix_Dist_Euclidea(Data<span class="op">=</span>X.to_numpy())</span>
<span id="cb51-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb51-2" aria-hidden="true" tabindex="-1"></a>M_Euclidea  <span class="op">=</span> M_Euclidea <span class="op">+</span> M_Euclidea.T</span>
<span id="cb51-3"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb51-3" aria-hidden="true" tabindex="-1"></a>M_Euclidea_new_data <span class="op">=</span> M_Euclidea[ <span class="bu">len</span>(X_train):<span class="bu">len</span>(X) , <span class="dv">0</span>:<span class="bu">len</span>(X_train) ]  </span></code></pre></div>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb52-1" aria-hidden="true" tabindex="-1"></a>KNNClassification_init <span class="op">=</span> KNNClassification(k<span class="op">=</span><span class="dv">10</span>, X_train<span class="op">=</span>X_train, Y_train<span class="op">=</span>Y_train, distance_matrix_new_data<span class="op">=</span>M_Euclidea_new_data)</span>
<span id="cb52-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb52-2" aria-hidden="true" tabindex="-1"></a>KNNClassification_init.predict()</span></code></pre></div>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb53-1" aria-hidden="true" tabindex="-1"></a>KNNClassification_init.df_predictions</span></code></pre></div>
<div style="overflow-x: scroll;">
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
id_x_new
</th>
<th>
Y_predict
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
2
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
3
</th>
<td>
3
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
4
</th>
<td>
4
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
376
</th>
<td>
376
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
377
</th>
<td>
377
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
378
</th>
<td>
378
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
379
</th>
<td>
379
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
380
</th>
<td>
380
</td>
<td>
2.0
</td>
</tr>
</tbody>
</table>
<p>
381 rows × 2 columns
</p>
</div>
<p><br></p>
<p>Calculamos la tasa de acierto de la clasificación:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb54-1" aria-hidden="true" tabindex="-1"></a>TA <span class="op">=</span> <span class="bu">sum</span>( KNNClassification_init.predictions <span class="op">==</span> Y_new.reset_index().quality_recode ) <span class="op">/</span> <span class="bu">len</span>(Y_new)</span>
<span id="cb54-2"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#cb54-2" aria-hidden="true" tabindex="-1"></a>TA</span></code></pre></div>
<pre><code>0.5958005249343832</code></pre>
<p>Como se puede comprobar se obtiene la misma tasa de acierto que con el algoritmo aplicado con <code>sklearn</code>, usando también la distancia Euclidea.</p>
<p><br></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="problema-de-clasificación-supervisada.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="problema-de-regresión.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://estadistica4all.com/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
