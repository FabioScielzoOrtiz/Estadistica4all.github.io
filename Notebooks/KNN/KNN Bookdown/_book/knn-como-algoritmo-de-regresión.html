<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 KNN como algoritmo de regresión | Algoritmo K vecinos más cercanos</title>
  <meta name="description" content="Esta es una introducción al algoritmo de regresión y clasificación supervisada KNN." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 KNN como algoritmo de regresión | Algoritmo K vecinos más cercanos" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Esta es una introducción al algoritmo de regresión y clasificación supervisada KNN." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 KNN como algoritmo de regresión | Algoritmo K vecinos más cercanos" />
  
  <meta name="twitter:description" content="Esta es una introducción al algoritmo de regresión y clasificación supervisada KNN." />
  

<meta name="author" content="Fabio Scielzo Ortiz" />


<meta name="date" content="2023-03-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="problema-de-regresión.html"/>
<link rel="next" href="bibliografía.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">KNN</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="problema-de-clasificación-supervisada.html"><a href="problema-de-clasificación-supervisada.html"><i class="fa fa-check"></i><b>2</b> Problema de clasificación supervisada</a></li>
<li class="chapter" data-level="3" data-path="knn-como-algoritmo-de-clasificación-supervisada.html"><a href="knn-como-algoritmo-de-clasificación-supervisada.html"><i class="fa fa-check"></i><b>3</b> KNN como algoritmo de clasificación supervisada</a>
<ul>
<li class="chapter" data-level="3.1" data-path="knn-como-algoritmo-de-clasificación-supervisada.html"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#knn-para-clasificación-supervisada-con-sklearn"><i class="fa fa-check"></i><b>3.1</b> KNN para clasificación supervisada con <code>sklearn</code></a></li>
<li class="chapter" data-level="3.2" data-path="knn-como-algoritmo-de-clasificación-supervisada.html"><a href="knn-como-algoritmo-de-clasificación-supervisada.html#knn-para-clasificación-supervisada-programado-en-python"><i class="fa fa-check"></i><b>3.2</b> KNN para clasificación supervisada programado en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="problema-de-regresión.html"><a href="problema-de-regresión.html"><i class="fa fa-check"></i><b>4</b> Problema de regresión</a></li>
<li class="chapter" data-level="5" data-path="knn-como-algoritmo-de-regresión.html"><a href="knn-como-algoritmo-de-regresión.html"><i class="fa fa-check"></i><b>5</b> KNN como algoritmo de regresión</a>
<ul>
<li class="chapter" data-level="5.1" data-path="knn-como-algoritmo-de-regresión.html"><a href="knn-como-algoritmo-de-regresión.html#knn-para-regresión-con-sklearn"><i class="fa fa-check"></i><b>5.1</b> KNN para regresión con <code>sklearn</code></a></li>
<li class="chapter" data-level="5.2" data-path="knn-como-algoritmo-de-regresión.html"><a href="knn-como-algoritmo-de-regresión.html#knn-para-regresión-programado-en-python"><i class="fa fa-check"></i><b>5.2</b> KNN para regresión programado en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i><b>6</b> Bibliografía</a></li>
<li class="divider"></li>
<li><a href="https://estadistica4all.com" target="blank">Estadistica4all.com</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Algoritmo K vecinos más cercanos</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="knn-como-algoritmo-de-regresión" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> KNN como algoritmo de regresión<a href="knn-como-algoritmo-de-regresión.html#knn-como-algoritmo-de-regresión" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Un modelo de regresión es un modelo estadístico que permite predecir una respuesta <strong>cuantitativa</strong> usando para ello información sobre una serie de predictores y de la propia respuesta.</p>
<ul>
<li><p>Se consideran <span class="math inline">\(\hspace{0.02cm} p\hspace{0.02cm}\)</span> predictores <span class="math inline">\(\hspace{0.02cm}\mathcal{X}_1,...,\mathcal{X}_p\hspace{0.02cm}\)</span> y una variable respuesta <strong>cuantitativa</strong> <span class="math inline">\(\hspace{0.02cm}\mathcal{Y}\)</span>.</p></li>
<li><p>Se tiene una muestra de observaciones <span class="math inline">\(\hspace{0.02cm}X_r = (x_{1r},...,x_{nr})^t\hspace{0.02cm}\)</span> de la variable <span class="math inline">\(\hspace{0.02cm}\mathcal{X}_r\hspace{0.1cm}\)</span> , para cada <span class="math inline">\(\hspace{0.1cm}r \in \lbrace 1,...,p \rbrace\)</span>.</p></li>
<li><p>Se tiene una muestra de observaciones <span class="math inline">\(\hspace{0.02cm}Y = (y_1,...,y_n)^t\hspace{0.02cm}\)</span> de la variable <span class="math inline">\(\hspace{0.02cm}\mathcal{Y}\)</span>.</p></li>
<li><p>En conclusión, se tiene una muestra de observaciones de los predictores y la respuesta.</p></li>
</ul>
<p><span class="math display">\[D=[\hspace{0.12cm}X_1,...,X_p,Y \hspace{0.12cm}]\hspace{0.12cm}=\begin{pmatrix}
    x_{11}&amp;x_{12}&amp;...&amp;x_{1p}&amp; y_1\\
    x_{21}&amp;x_{22}&amp;...&amp;x_{2p} &amp; y_2\\
    ... &amp;...&amp; ...&amp; .... &amp; ...\\
    x_{n1}&amp;x_{n2}&amp;...&amp;x_{np}&amp; y_n
    \end{pmatrix} = \begin{pmatrix}
    x_{1}&amp; y_1\\
    x_{2}&amp; y_2 \\
     ...&amp;... \\
     x_{n} &amp; y_n
    \end{pmatrix}\]</span></p>
<p><br></p>
<p>El algoritmo KNN para regresión tiene los siguientes pasos:</p>
<div class="warning" style="background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;">
<p><span></p>
<p style="margin-left:1em;">
<ul>
<li><p>Se define una medida de distancia entre pares de observaciones de variables <span class="math inline">\(\hspace{0.15cm} \Rightarrow \hspace{0.15cm}\)</span> <span class="math inline">\(\delta\)</span> .</p></li>
<li><p>Dada una nueva observación <span class="math inline">\(\hspace{0.02cm}x_{*}\hspace{0.05cm}=\hspace{0.05cm}(x_{*1} \hspace{0.05cm},\hspace{0.05cm} x_{*2} \hspace{0.05cm},\dots ,\hspace{0.05cm} x_{*p})\hspace{0.02cm}\)</span> de los predictores <span class="math inline">\(\hspace{0.02cm}(\mathcal{X}_1 ,...,\mathcal{X}_p)\hspace{0.02cm}\)</span> , es decir, una observación que no está en la muestra de train, se calculan las distancias entre cada par de observaciones <span class="math inline">\(\hspace{0.02cm}(x_{*} \hspace{0.1cm},\hspace{0.1cm} x_i)\hspace{0.02cm}\)</span> <span class="math inline">\(\hspace{0.15cm}\Rightarrow\hspace{0.15cm}\)</span> <span class="math inline">\(\delta(x_{*} , x_i) \hspace{0.2cm} , \hspace{0.2cm} i=1,...,n\)</span>.</p></li>
<li><p>Se seleccionan las <span class="math inline">\(\hspace{0.02cm}k\hspace{0.02cm}\)</span> observaciones <span class="math inline">\(\hspace{0.02cm}x_1,...,x_n\hspace{0.02cm}\)</span> que son más cercanas a la nueva observación <span class="math inline">\(\hspace{0.02cm}x_{*}\)</span> , según la distancia <span class="math inline">\(\hspace{0.02cm}\delta\)</span>.</p>
<p>El conjunto de estas <span class="math inline">\(\hspace{0.01cm}k\hspace{0.01cm}\)</span> observaciones son los <strong><span class="math inline">\(k\)</span> vecinos más cercanos de <span class="math inline">\(x_{*}\)</span></strong> y se denota por <span class="math inline">\(\hspace{0.01cm}K_{x_{*}}\)</span></p></li>
<li><p>Se predice la respuesta para la nueva observación <span class="math inline">\(\hspace{0.02cm}x_{*}\hspace{0.02cm}\)</span> de los predictores como la media de la respuesta en el conjunto <span class="math inline">\(\hspace{0.02cm}K_{x_{*}} \hspace{0.02cm}\)</span>.</p>
<p><span class="math display">\[\widehat{y}_{*}  \hspace{0.2cm} = \hspace{0.2cm} \dfrac{1}{k} \cdot \sum_{ \hspace{0.05cm} i \hspace{0.05cm} \in \hspace{0.05cm} \Delta } \hspace{0.07cm} y_i \]</span></p>
<p>donde:</p>
<p><span class="math display">\[\Delta \hspace{0.1cm} = \hspace{0.1cm} \Bigl\{ \hspace{0.05cm} i = 1,...,n \hspace{0.2cm} : \hspace{0.2cm}  x_i \in K_{x_{*}} \hspace{0.05cm} \Bigr\} \\\]</span></p>
<ul>
<li><p>Otra forma de expresar el punto anterior es la siguiente:</p>
<ul>
<li><p>Si <span class="math inline">\(\hspace{0.05cm}Y_{K_{x_{*}}}\hspace{0.03cm}\)</span> es la muestra de la respuesta para los <span class="math inline">\(\hspace{0.02cm}k\hspace{0.02cm}\)</span> individuos asociados al conjunto <span class="math inline">\(\hspace{0.02cm}K_{x_{*}}\)</span></p>
<p>Es decir:</p>
<p><span class="math display">\[Y_{K_{x_{*}}} = \Bigl(\hspace{0.06cm} y_i \hspace{0.2cm} : \hspace{0.2cm}  i= 1,...,n  \hspace{0.3cm}  \text{y} \hspace{0.3cm} x_i \in K_{x_{*}}\hspace{0.06cm} \Bigr)\]</span></p>
<p>entonces:</p>
<p><span class="math display">\[\widehat{y}_{*} \hspace{0.05cm} = \hspace{0.05cm} \overline{\hspace{0.1cm} Y \hspace{0.1cm}}_{K_{x_{*}}}\\\]</span></p></li>
</ul></li>
</ul></li>
</ul>
</p>
</p>
<p></span></p>
</div>
<p><br></p>
<p><br></p>
<div id="knn-para-regresión-con-sklearn" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> KNN para regresión con <code>sklearn</code><a href="knn-como-algoritmo-de-regresión.html#knn-para-regresión-con-sklearn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Volvemos a crear los data-frames con los que entrenaremos el modelo y los que usaremos para predecir la respuesta. En este caso la variable respuesta es el precio de las viviendas (<strong>price</strong>).</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="knn-como-algoritmo-de-regresión.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co">## TRAIN DATA (Datos que se usan para entrenar el modelo)</span></span>
<span id="cb56-2"><a href="knn-como-algoritmo-de-regresión.html#cb56-2" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> Data_Mixed_train.loc[: , Data_Mixed_train.columns <span class="op">!=</span> <span class="st">&#39;price&#39;</span>]</span>
<span id="cb56-3"><a href="knn-como-algoritmo-de-regresión.html#cb56-3" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> Data_Mixed_train.loc[: , <span class="st">&#39;price&#39;</span>]</span>
<span id="cb56-4"><a href="knn-como-algoritmo-de-regresión.html#cb56-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-5"><a href="knn-como-algoritmo-de-regresión.html#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="co">## NEW DATA (Nuevos datos de los predictores)</span></span>
<span id="cb56-6"><a href="knn-como-algoritmo-de-regresión.html#cb56-6" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> Data_Mixed_new.loc[: , Data_Mixed_test.columns <span class="op">!=</span> <span class="st">&#39;price&#39;</span>]</span>
<span id="cb56-7"><a href="knn-como-algoritmo-de-regresión.html#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="co"># En la práctica real no se tienen datos sobre la respuesta asociado a las nuevas observaciones de los predictores</span></span>
<span id="cb56-8"><a href="knn-como-algoritmo-de-regresión.html#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Eso es justo lo que se quiere predecir.</span></span>
<span id="cb56-9"><a href="knn-como-algoritmo-de-regresión.html#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Pero en este ejemplo al usar como &quot;nuevos&quot; datos una parte del data set original, si que tenemos esa información.</span></span>
<span id="cb56-10"><a href="knn-como-algoritmo-de-regresión.html#cb56-10" aria-hidden="true" tabindex="-1"></a>Y_new <span class="op">=</span> Data_Mixed_new.loc[: , <span class="st">&#39;price&#39;</span>] </span></code></pre></div>
<p><br></p>
<p>Fijamos algunos parámetros de la función <code>KNeighborsRegressor</code></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="knn-como-algoritmo-de-regresión.html#cb57-1" aria-hidden="true" tabindex="-1"></a>knn_regression <span class="op">=</span> sklearn.neighbors.KNeighborsRegressor(n_neighbors<span class="op">=</span><span class="dv">10</span> , p<span class="op">=</span><span class="dv">2</span>,  metric<span class="op">=</span><span class="st">&#39;minkowski&#39;</span>)</span></code></pre></div>
<p>Entrenamos el algoritmo con las observaciones de los predictores contenidas en <code>X_train</code> y con las observaciones de la respuesta contenidas en <code>Y_train</code>.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="knn-como-algoritmo-de-regresión.html#cb58-1" aria-hidden="true" tabindex="-1"></a>knn_regression.fit(X_train, Y_train)</span></code></pre></div>
<p>Predecimos la respuesta para las observaciones de los predictores contenidas en <code>X_new</code>.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="knn-como-algoritmo-de-regresión.html#cb59-1" aria-hidden="true" tabindex="-1"></a>knn_regression.predict( X_new ) </span></code></pre></div>
<pre><code>array([ 1963827.5,  3046500. ,  2523990. ,  2477788.8,  2513966.5,
        1732699.8,  2209200. ,  1629939.2,  2014867.5,  1567750.3,
         938557.7,  1272000. ,  1739199.8,  5767999. ,  2620300. ,
        2699000. ,  2184789.9,  1313399.9,  1606889.9,  1606988.7,
        2143900. ,   456800. ,  2475000. ,   923199.9,  1514399.6,
        2689500. ,   627404.9,  1807950. ,  2860688.7,   934390.3,
       11746999.9,  1370299.9,  1412638.6,   624110. ,  3395900. ,
         463982.8,  1092166.4,  1523899.9,   712312.9,  1008838.9,
        2053188.6,  1253360.8,   832938. ,  1569545.9,   730918.7,
        2728490. ,  2270500. ,  2048800. ,  3383900. ,  1847288.7,
        1162499.9,   994277.6,  3398877.8,  1392877.6,  1598200. ,
        1731699.8,  1695900. ,  1443087.8,  2424182.7,  3602390. ,
        2430588.8,  7251000. ,  1356689.2,   635069. ,  1806400. ,
        1465699.9,  2592000. ,  3567400. ,  2359377.6,  1399400. ,
        2120779.8,  1227612.6,  2239000. ,  1172078.8,  2170779.8,
        2262490. ,  5347499. ,  5502999. ,  1626888.8,  2748877.6,
         556904.8,  1380093.3,  3455000. ,   916588.6,   893834.4,
         785402.6,  1721288.8,  1900700. ,  4439900. , 15675244. ,
        1604676.6,  1416399.9,  1004399.9, 13611999.9,   620450. ,
        2070489.9,  2066000. ,  1664843.7,   653822.8,  1671599.9,
        5767999. ,  1687399.9,  2356188.8,  3228400. ,  1825599.9,
        1223644. ,   599630. ,  1781599.7,  2264280. ,  3247066.3,
        1139777.6,  2990688.7,  1489220.3,  1523966. ,  1009100. ,
         876888.8,  3582390. ,  1646445.8,   511216.4,  2684899.8,
       23276900. ,  1723199.8,  1085766.4,  3153566.4,  3153566.4,
        2870077.6,  1694366. ,  2375490. ,  1457078.6,  3383900. ,
        1509499.9,  3604800. ,   923788.8,  1739199.8,  1108618.8,
         759798.4,  1644599.9,   994055.3,  1323967.6,  1600478.4,
         873700. ,  1579500. ,  4622500. ,   888788.8,  3602499.4,
        1816988.7,   720980.2,   648950.1,  1992588.8,  1678499.9,
        1807840.2,  1212560.8,  2330084.6,  2684899.8,   997867.6,
         975277.6,  1616279.3,  1881200. ,  1170000. ,  1895077.5,
        1030537.9,  7541000. ,  1363877.6,  2118999.9,  1598899.9,
        1138660.8,  7949288.7,  1243228. ,   502651.3,  1036966.4,
        4669900. ,  3317499.3,  1272000. ,  1712995. ,  2412994.9,
         749812.9,  3067487.8,   927177.5,  4859999. ,   938557.7,
        2858566.5,  1908288.7,   615977.6,  1816150.4,  2458688.8,
        2893800. ,  2324295. ,  2182066.4,  1464800. ,   990813.2,
         662341. ,   936288.8,  2831880. ,  1053266.4,  2433999.9,
         595600.2,  1203499.9,  1305488.8,  2322600. ,  2069713.2,
        1455966. ,  1712995. ,   978589.9,   994277.6,   508982.8,
        4824899.9,  2050988.7,  1094100. ,  1046666.4,  4839999. ,
         417811.6,  1006100. ,  2114990. ,   923771.8,  1846988.7,
        1422690. ,  2717377.6,  5767999. ,  2589590. , 16459244. ,
        1135179.1,   717239.7,   717239.7,   572791.5,  1232336.8,
        1868516.1,  1242336.8,  2196889.9,   990537.9,  1036966.4,
        1748738.6,  3455000. ,  2545500. ,  1953493.8,   936288.8,
        1412638.6,  1305488.8,   945488.4,  2307788.8,   494885. ,
        1205939. ,   983588.7,  1984766.4,   494885. ,  1545567.5,
         545205.2,  2044588.7,   896000. ,  1509316.6,  1839238.7,
        1110888.8,  1751279.3,  2590044.1,  2059244.1,   962783.7,
       11746999.9,  1163723.8,  1012000. ,  1307138.6,  1497966.3,
         999400. ,  1434700. ,  1712995. ,   611224.1,  1658084.6,
         884047.7,  1721542.7,   610902.5,  1002500. ,  3242494.3,
        1751279.3,  1088657.4,  1868516.1,  1456367.4,  2077777.6,
         595244.8,   859625. ,   945537.9,  1877838.7,  1536750.3,
        1938940. ,  1409288.8,  2209428.6,   451400. ,  2532088.7,
        2255977.5,  1657695. ,  2341290. ,  2209200. ,  1036966.4,
        2209200. ,   995488.8,   518897. ,  2940173.4,  4451900. ,
        3604800. ,  1877838.7,  2468990. ,  4621994.9,   669129.9,
        2240490. ,  2343584.6,   998788.7,  1723066.6,  1305149.6,
        1277828. ,   991699.9,  2170779.8,   942600. ,  3567400. ,
        1569935. ,   942861. ,  1192490. ,  2719500. ,  6827988.8,
        1933590. ,  1305488.8,  1038588.8,  2524683.8,  1278449.6,
       19871255.2,   651977.6,  5767999. ,  1472677.5,  1296977.6,
        1088657.4,   893834.4,   624697.1,   635638.5,  1579500. ,
         608100. ,  2458688.8,  1330832.9,  3094900. ,  1845988.7,
        1044499.9,  1192055.2,  2020488.7,   783602.6,  2799990. ,
        1814999.9,  2581178.8,   896000. ,  7977988.8,  2968487.9,
         887678.9,  2513966.5,  2468990. ,  2604090. ,  3036389.8,
        1809838.7,  1625762.7,   514805.2,  1758590. ,  1833999.7,
        1500399.8,  1581399.8,  1792945.1,  7473288.8,  2217783.8,
        1956490. ,   595689. ,  1089428. ,   669198.4,  2748877.6,
         449866.4,  3317499.3,  1520567.5,  5132388.8,  2490490. ,
        2462875. ,  2589590. ,  1479284.2,  1835499.9,  1488677.6,
        2015688.7,  7541000. , 15675244. ,  1088657.4,  1165544. ,
        1130149.8])</code></pre>
<p><br></p>
<p>Como disponemos de los valores de la respuesta para las observaciones de los predictores con las que hemos predicho la respuesta podemos comparar los valores reales con los predichos a través de una métrica de error como el error cuadratico medio (ECM):</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="knn-como-algoritmo-de-regresión.html#cb61-1" aria-hidden="true" tabindex="-1"></a>ECM <span class="op">=</span> <span class="bu">sum</span>( (knn_regression.predict( X_new )  <span class="op">-</span> Y_new.reset_index().price )<span class="op">**</span><span class="dv">2</span> ) <span class="op">/</span> <span class="bu">len</span>(Y_new)</span>
<span id="cb61-2"><a href="knn-como-algoritmo-de-regresión.html#cb61-2" aria-hidden="true" tabindex="-1"></a>ECM</span></code></pre></div>
<pre><code>2405540681171.826</code></pre>
<p>Para reducir la dimensión de la cantidad anterior y además lograr que se mida en las mismas unidades que la respuesta tomamos la raiz cuadrada:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="knn-como-algoritmo-de-regresión.html#cb63-1" aria-hidden="true" tabindex="-1"></a>np.sqrt(ECM)</span></code></pre></div>
<pre><code>1550980.5547368499</code></pre>
<p><br></p>
<p><br></p>
</div>
<div id="knn-para-regresión-programado-en-python" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> KNN para regresión programado en <code>Python</code><a href="knn-como-algoritmo-de-regresión.html#knn-para-regresión-programado-en-python" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Programamos el algoritmo KNN para regresión:</strong></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="knn-como-algoritmo-de-regresión.html#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> KNNRegression :</span>
<span id="cb65-2"><a href="knn-como-algoritmo-de-regresión.html#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="co"># k es el hiper-parametro del algoritmo KNN.</span></span>
<span id="cb65-3"><a href="knn-como-algoritmo-de-regresión.html#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="co"># &#39;Distance_Matrix_New_Data&#39; debe contenere las distancias entre las nuevas observaciones de los predictores y las de train.</span></span>
<span id="cb65-4"><a href="knn-como-algoritmo-de-regresión.html#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Su fila i-esima debe contener las distancias entre la i-esima nueva observacion de los predictores y las observaciones de train de los predictores.</span></span>
<span id="cb65-5"><a href="knn-como-algoritmo-de-regresión.html#cb65-5" aria-hidden="true" tabindex="-1"></a><span class="co"># X_train tiene que ser un data-frame con las observaciones de los predictores con las que se va a entrenar el modelo.</span></span>
<span id="cb65-6"><a href="knn-como-algoritmo-de-regresión.html#cb65-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Y_train tiene que ser un numpy array que contenga las observaciones de la respuesta con las que se va a entrenar el modelo.</span></span>
<span id="cb65-7"><a href="knn-como-algoritmo-de-regresión.html#cb65-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-8"><a href="knn-como-algoritmo-de-regresión.html#cb65-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, k, X_train, Y_train, distance_matrix_new_data):</span>
<span id="cb65-9"><a href="knn-como-algoritmo-de-regresión.html#cb65-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> k</span>
<span id="cb65-10"><a href="knn-como-algoritmo-de-regresión.html#cb65-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.X_train <span class="op">=</span> X_train</span>
<span id="cb65-11"><a href="knn-como-algoritmo-de-regresión.html#cb65-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Y_train <span class="op">=</span> Y_train</span>
<span id="cb65-12"><a href="knn-como-algoritmo-de-regresión.html#cb65-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.distance_matrix_new_data <span class="op">=</span> distance_matrix_new_data</span>
<span id="cb65-13"><a href="knn-como-algoritmo-de-regresión.html#cb65-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.predictions <span class="op">=</span> <span class="va">None</span></span>
<span id="cb65-14"><a href="knn-como-algoritmo-de-regresión.html#cb65-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-15"><a href="knn-como-algoritmo-de-regresión.html#cb65-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>):</span>
<span id="cb65-16"><a href="knn-como-algoritmo-de-regresión.html#cb65-16" aria-hidden="true" tabindex="-1"></a>        Y_predict_x_new_i_LIST <span class="op">=</span> []</span>
<span id="cb65-17"><a href="knn-como-algoritmo-de-regresión.html#cb65-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(<span class="va">self</span>.distance_matrix_new_data)):</span>
<span id="cb65-18"><a href="knn-como-algoritmo-de-regresión.html#cb65-18" aria-hidden="true" tabindex="-1"></a>            distancias_x_new_i <span class="op">=</span> pd.DataFrame({<span class="st">&#39;id_x_train&#39;</span>: <span class="va">self</span>.X_train.index, <span class="st">&#39;distancias&#39;</span>: <span class="va">self</span>.distance_matrix_new_data[i,:]})</span>
<span id="cb65-19"><a href="knn-como-algoritmo-de-regresión.html#cb65-19" aria-hidden="true" tabindex="-1"></a>            distancias_x_new_i_sort <span class="op">=</span> distancias_x_new_i.sort_values(by<span class="op">=</span>[<span class="st">&quot;distancias&quot;</span>]).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb65-20"><a href="knn-como-algoritmo-de-regresión.html#cb65-20" aria-hidden="true" tabindex="-1"></a>            knn_x_new_i <span class="op">=</span> distancias_x_new_i_sort.iloc[<span class="dv">0</span>:<span class="va">self</span>.k, <span class="dv">0</span>]</span>
<span id="cb65-21"><a href="knn-como-algoritmo-de-regresión.html#cb65-21" aria-hidden="true" tabindex="-1"></a>            Y_values_knn_x_new_i <span class="op">=</span> []</span>
<span id="cb65-22"><a href="knn-como-algoritmo-de-regresión.html#cb65-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> knn_x_new_i :</span>
<span id="cb65-23"><a href="knn-como-algoritmo-de-regresión.html#cb65-23" aria-hidden="true" tabindex="-1"></a>                Y_values_knn_x_new_i.append(<span class="va">self</span>.Y_train[j])</span>
<span id="cb65-24"><a href="knn-como-algoritmo-de-regresión.html#cb65-24" aria-hidden="true" tabindex="-1"></a>            Y_predict_x_new_i <span class="op">=</span> <span class="bu">sum</span>(Y_values_knn_x_new_i)<span class="op">/</span><span class="va">self</span>.k    </span>
<span id="cb65-25"><a href="knn-como-algoritmo-de-regresión.html#cb65-25" aria-hidden="true" tabindex="-1"></a>            Y_predict_x_new_i_LIST.append(Y_predict_x_new_i)</span>
<span id="cb65-26"><a href="knn-como-algoritmo-de-regresión.html#cb65-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df_predictions <span class="op">=</span> pd.DataFrame({<span class="st">&#39;id_x_new&#39;</span>:<span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(<span class="va">self</span>.distance_matrix_new_data)) , <span class="st">&#39;Y_predict&#39;</span>: Y_predict_x_new_i_LIST})</span>
<span id="cb65-27"><a href="knn-como-algoritmo-de-regresión.html#cb65-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.predictions <span class="op">=</span> <span class="va">self</span>.df_predictions[<span class="st">&#39;Y_predict&#39;</span>]</span></code></pre></div>
<p><br></p>
<p>Cargamos de nuevo los datos:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="knn-como-algoritmo-de-regresión.html#cb66-1" aria-hidden="true" tabindex="-1"></a>Data <span class="op">=</span> pd.read_csv(<span class="st">&#39;House_Price_Regression.csv&#39;</span>)</span>
<span id="cb66-2"><a href="knn-como-algoritmo-de-regresión.html#cb66-2" aria-hidden="true" tabindex="-1"></a>Data_Mixed <span class="op">=</span> Data.loc[:, [<span class="st">&#39;latitude&#39;</span>, <span class="st">&#39;longitude&#39;</span>, <span class="st">&#39;price&#39;</span>, <span class="st">&#39;size_in_m_2&#39;</span>, <span class="st">&#39;balcony_recode&#39;</span>, <span class="st">&#39;private_garden_recode&#39;</span>, <span class="st">&#39;quality_recode&#39;</span>]]</span>
<span id="cb66-3"><a href="knn-como-algoritmo-de-regresión.html#cb66-3" aria-hidden="true" tabindex="-1"></a>Data_Mixed.head()</span></code></pre></div>
<div style="overflow-x: scroll;">
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
latitude
</th>
<th>
longitude
</th>
<th>
price
</th>
<th>
size_in_m_2
</th>
<th>
balcony_recode
</th>
<th>
private_garden_recode
</th>
<th>
quality_recode
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
25.113208
</td>
<td>
55.138932
</td>
<td>
2700000
</td>
<td>
100.242337
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
25.106809
</td>
<td>
55.151201
</td>
<td>
2850000
</td>
<td>
146.972546
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
25.063302
</td>
<td>
55.137728
</td>
<td>
1150000
</td>
<td>
181.253753
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
3
</th>
<td>
25.227295
</td>
<td>
55.341761
</td>
<td>
2850000
</td>
<td>
187.664060
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
4
</th>
<td>
25.114275
</td>
<td>
55.139764
</td>
<td>
1729200
</td>
<td>
47.101821
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<p>Volvemos a separar en train y new data, teniendo en cuenta que ahora la variable respuesta debe ser cuantitativa, en este caso <strong>price</strong>.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="knn-como-algoritmo-de-regresión.html#cb67-1" aria-hidden="true" tabindex="-1"></a>Data_Mixed_train <span class="op">=</span> Data_Mixed.sample(frac<span class="op">=</span><span class="fl">0.8</span>, replace<span class="op">=</span><span class="va">False</span>, weights<span class="op">=</span><span class="va">None</span>, random_state<span class="op">=</span><span class="dv">123</span>, axis<span class="op">=</span><span class="va">None</span>, ignore_index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb67-2"><a href="knn-como-algoritmo-de-regresión.html#cb67-2" aria-hidden="true" tabindex="-1"></a>Data_Mixed_new <span class="op">=</span>  Data_Mixed.drop( Data_Mixed_train.index , )</span></code></pre></div>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="knn-como-algoritmo-de-regresión.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co">## TRAIN DATA (Datos dispobles --&gt; se usan para entrenar el modelo)</span></span>
<span id="cb68-2"><a href="knn-como-algoritmo-de-regresión.html#cb68-2" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> Data_Mixed_train.loc[: , Data_Mixed_train.columns <span class="op">!=</span> <span class="st">&#39;price&#39;</span>]</span>
<span id="cb68-3"><a href="knn-como-algoritmo-de-regresión.html#cb68-3" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> Data_Mixed_train.loc[: , <span class="st">&#39;price&#39;</span>]</span>
<span id="cb68-4"><a href="knn-como-algoritmo-de-regresión.html#cb68-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-5"><a href="knn-como-algoritmo-de-regresión.html#cb68-5" aria-hidden="true" tabindex="-1"></a><span class="co">## NEW DATA (Nuevos datos de los predictores)</span></span>
<span id="cb68-6"><a href="knn-como-algoritmo-de-regresión.html#cb68-6" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> Data_Mixed_new.loc[: , Data_Mixed_new.columns <span class="op">!=</span> <span class="st">&#39;price&#39;</span>]</span>
<span id="cb68-7"><a href="knn-como-algoritmo-de-regresión.html#cb68-7" aria-hidden="true" tabindex="-1"></a><span class="co"># En la práctica real no se tienen datos sobre la respuesta asociado a las nuevas obsrvaciones de los predictores</span></span>
<span id="cb68-8"><a href="knn-como-algoritmo-de-regresión.html#cb68-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Eso es justo lo que se quiere predecir.</span></span>
<span id="cb68-9"><a href="knn-como-algoritmo-de-regresión.html#cb68-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Pero en este ejemplo al usar como &quot;nuevos&quot; datos una parte del data set original, si que tenemos esa información.</span></span>
<span id="cb68-10"><a href="knn-como-algoritmo-de-regresión.html#cb68-10" aria-hidden="true" tabindex="-1"></a>Y_new <span class="op">=</span> Data_Mixed_new.loc[: , <span class="st">&#39;price&#39;</span>] </span></code></pre></div>
<p>Hacemos una vista del data-frame <code>X</code> :</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="knn-como-algoritmo-de-regresión.html#cb69-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.concat([X_train , X_new])</span>
<span id="cb69-2"><a href="knn-como-algoritmo-de-regresión.html#cb69-2" aria-hidden="true" tabindex="-1"></a>X.head()</span></code></pre></div>
<div style="overflow-x: scroll;">
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
latitude
</th>
<th>
longitude
</th>
<th>
size_in_m_2
</th>
<th>
balcony_recode
</th>
<th>
private_garden_recode
</th>
<th>
quality_recode
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
382
</th>
<td>
25.196489
</td>
<td>
55.272126
</td>
<td>
488.019459
</td>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
732
</th>
<td>
25.107984
</td>
<td>
55.244923
</td>
<td>
138.704179
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
1888
</th>
<td>
25.071504
</td>
<td>
55.128579
</td>
<td>
171.220229
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
</tr>
<tr>
<th>
679
</th>
<td>
25.054336
</td>
<td>
55.203423
</td>
<td>
116.035847
</td>
<td>
1.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
1004
</th>
<td>
25.087251
</td>
<td>
55.145574
</td>
<td>
162.208638
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<p><br></p>
<ul>
<li>Probamos el algoritmo con la distancia de <strong>Gower</strong> y <strong><span class="math inline">\(\hspace{0.02cm}k=10\)</span></strong> :</li>
</ul>
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="knn-como-algoritmo-de-regresión.html#cb70-1" aria-hidden="true" tabindex="-1"></a>M_Gower <span class="op">=</span> Matrix_Gower_Distance(Data<span class="op">=</span>X.to_numpy(), p1<span class="op">=</span><span class="dv">3</span>, p2<span class="op">=</span><span class="dv">2</span>, p3<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb70-2"><a href="knn-como-algoritmo-de-regresión.html#cb70-2" aria-hidden="true" tabindex="-1"></a>M_Gower_new_data <span class="op">=</span> M_Gower[ <span class="bu">len</span>(X_train):<span class="bu">len</span>(X) , <span class="dv">0</span>:<span class="bu">len</span>(X_train) ]  </span></code></pre></div>
<div class="sourceCode" id="cb71"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="knn-como-algoritmo-de-regresión.html#cb71-1" aria-hidden="true" tabindex="-1"></a>KNNRegression_init <span class="op">=</span> KNNRegression(k<span class="op">=</span><span class="dv">10</span>, X_train<span class="op">=</span>X_train, Y_train<span class="op">=</span>Y_train, distance_matrix_new_data<span class="op">=</span>M_Gower_new_data)</span>
<span id="cb71-2"><a href="knn-como-algoritmo-de-regresión.html#cb71-2" aria-hidden="true" tabindex="-1"></a>KNNRegression_init.predict()</span></code></pre></div>
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="knn-como-algoritmo-de-regresión.html#cb72-1" aria-hidden="true" tabindex="-1"></a>KNNRegression_init.df_predictions</span></code></pre></div>
<pre><code>     id_x_new   Y_predict
0           0   2081499.9
1           1   1710999.9
2           2   2639277.7
3           3   2817500.0
4           4   2839583.6
..        ...         ...
376       376   4778999.9
377       377  12093477.6
378       378   1019720.0
379       379   1260699.9
380       380   1088923.8</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="knn-como-algoritmo-de-regresión.html#cb74-1" aria-hidden="true" tabindex="-1"></a>KNNRegression_init.predictions</span></code></pre></div>
<pre><code>0       2081499.9
1       1710999.9
2       2639277.7
3       2817500.0
4       2839583.6
          ...    
376     4778999.9
377    12093477.6
378     1019720.0
379     1260699.9
380     1088923.8
Name: Y_predict, Length: 381, dtype: float64</code></pre>
<p>Calculamos el error cuadrático medio:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="knn-como-algoritmo-de-regresión.html#cb76-1" aria-hidden="true" tabindex="-1"></a>ECM <span class="op">=</span> <span class="bu">sum</span>( (KNNRegression_init.predictions <span class="op">-</span> Y_new.reset_index().price)<span class="op">**</span><span class="dv">2</span> ) <span class="op">/</span> <span class="bu">len</span>(Y_new)</span>
<span id="cb76-2"><a href="knn-como-algoritmo-de-regresión.html#cb76-2" aria-hidden="true" tabindex="-1"></a>ECM</span></code></pre></div>
<pre><code>2694547525120.1924</code></pre>
<div class="sourceCode" id="cb78"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="knn-como-algoritmo-de-regresión.html#cb78-1" aria-hidden="true" tabindex="-1"></a>np.sqrt(ECM)</span></code></pre></div>
<pre><code>1641507.6987696989</code></pre>
<p><br></p>
<ul>
<li>Probamos ahora el algoritmo con la distancia <strong>Euclidea</strong> y <strong><span class="math inline">\(\hspace{0.02cm}k=10\)</span></strong> :</li>
</ul>
<div class="sourceCode" id="cb80"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="knn-como-algoritmo-de-regresión.html#cb80-1" aria-hidden="true" tabindex="-1"></a>M_Euclidea <span class="op">=</span> Matrix_Dist_Euclidea(Data<span class="op">=</span>X.to_numpy())</span>
<span id="cb80-2"><a href="knn-como-algoritmo-de-regresión.html#cb80-2" aria-hidden="true" tabindex="-1"></a>M_Euclidea  <span class="op">=</span> M_Euclidea <span class="op">+</span> M_Euclidea.T</span>
<span id="cb80-3"><a href="knn-como-algoritmo-de-regresión.html#cb80-3" aria-hidden="true" tabindex="-1"></a>M_Euclidea_new_data <span class="op">=</span> M_Euclidea[ <span class="bu">len</span>(X_train):<span class="bu">len</span>(X) , <span class="dv">0</span>:<span class="bu">len</span>(X_train) ]  </span></code></pre></div>
<div class="sourceCode" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="knn-como-algoritmo-de-regresión.html#cb81-1" aria-hidden="true" tabindex="-1"></a>KNNRegression_init <span class="op">=</span> KNNRegression(k<span class="op">=</span><span class="dv">10</span>, X_train<span class="op">=</span>X_train, Y_train<span class="op">=</span>Y_train, distance_matrix_new_data<span class="op">=</span>M_Euclidea_new_data)</span>
<span id="cb81-2"><a href="knn-como-algoritmo-de-regresión.html#cb81-2" aria-hidden="true" tabindex="-1"></a>KNNRegression_init.predict() </span></code></pre></div>
<p>Calculamos el error cuadrático medio:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb82-1"><a href="knn-como-algoritmo-de-regresión.html#cb82-1" aria-hidden="true" tabindex="-1"></a>ECM <span class="op">=</span> <span class="bu">sum</span>( (KNNRegression_init.predictions <span class="op">-</span> Y_new.reset_index().price)<span class="op">**</span><span class="dv">2</span> ) <span class="op">/</span> <span class="bu">len</span>(Y_new)</span>
<span id="cb82-2"><a href="knn-como-algoritmo-de-regresión.html#cb82-2" aria-hidden="true" tabindex="-1"></a>ECM</span></code></pre></div>
<pre><code>2406009496239.313</code></pre>
<p>Como vemos el ECM obtenido es similar al que obteniamos al implementar el algoritmo con <code>sklearn</code> , también con la distancia Euclidea y <span class="math inline">\(k=10\)</span>.</p>
<p><br></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="problema-de-regresión.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliografía.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://estadistica4all.com/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
