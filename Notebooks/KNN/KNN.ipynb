{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN for supervised classification   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\hspace{0.1cm} p \\hspace{0.1cm}$ variables $\\hspace{0.1cm} X=(X_1,...,X_p) \\hspace{0.1cm}$ measurements on a $n$ size sample.\n",
    "\n",
    "Also we have a categorical variable $\\hspace{0.1cm} Y \\hspace{0.1cm}$ with $\\hspace{0.1cm} g \\hspace{0.1cm}$  categories that indicates  the group to which each element of the sample belongs  $ ( \\hspace{0.05cm} Range(Y)=\\lbrace c_1 ,..., c_g \\rbrace \\hspace{0.05cm})$\n",
    "\n",
    "The groups generated by $\\hspace{0.1cm} Y \\hspace{0.1cm}$ are denoted as $\\hspace{0.1cm} \\Omega_1 ,..., \\Omega_g \\hspace{0.15cm}$   $\\hspace{0.15cm}( \\hspace{0.1cm} y_i = c_r \\hspace{0.15cm} \\Leftrightarrow \\hspace{0.15cm}$  $ i \\in \\Omega_r \\hspace{0.1cm})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supervised classification problem consists in, for a new observation $\\hspace{0.1cm} x_{new} = (x_{new,1},x_{new,2},...,x_{new,p}) \\hspace{0.1cm}$ of the variables $X_1,...,X_p  \\hspace{0.1cm}$, predict its $\\hspace{0.1cm} Y \\hspace{0.1cm}$ value $\\hspace{0.1cm} (y_{new})\\hspace{0.1cm}$  using the information of $\\hspace{0.1cm} X_1,...,X_p \\hspace{0.1cm}$ and $ \\hspace{0.1cm} Y$\n",
    "\n",
    "So , the problem is to classify a new element/individual in one of the $\\hspace{0.1cm} g \\hspace{0.1cm}$ groups generated by $\\hspace{0.1cm} Y \\hspace{0.1cm}$ using the information available of $\\hspace{0.1cm} X_1,...,X_p \\hspace{0.1cm}$ and $Y$, and also $\\hspace{0.1cm} x_{new} = (x_{new,1},x_{new,2},...,x_{new,p})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN (K-nearest neighbors) algorithm have the following steps:\n",
    "\n",
    " $1. \\hspace{0.15cm}$ Define a distance measure between the observation of the original sample respect to the variables $X_1,...,X_p$ $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$ $\\delta$\n",
    "\n",
    "\n",
    "\n",
    " $2. \\hspace{0.15cm}$ Compute the distances between $x_{new}$ and the initial observations $\\hspace{0.1cm} \\lbrace x_1,...,x_n \\rbrace$ $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$ $\\lbrace \\hspace{0.1 cm}  \\delta(x_{new}, x_i) \\hspace{0.1 cm} / \\hspace{0.1 cm}  i=1,...,n \\hspace{0.1 cm}  \\rbrace$\n",
    "\n",
    "  \n",
    " $3. \\hspace{0.15cm}$ Select the  $k$ nearest observation to $x_{new}$ based on $\\hspace{0.05cm} \\delta \\hspace{0.12cm}$ $(k$ nearest neighbors of $x_{new})$ $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$   The set of these observation will be denote by $KNN$ \n",
    "\n",
    " $4. \\hspace{0.15cm}$ Compute the proportion of these observation (neighbors) that belongs to each group $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$  \n",
    " \n",
    " $\\hspace{0.65cm} \\Rightarrow \\hspace{0.15cm}$ The proportion of $KNN$ that belongs to the group $\\hspace{0.15cm} \\Omega_r$ $\\hspace{0.1cm}(Y=c_r)\\hspace{0.1cm}$ will be denote by $\\hspace{0.1 cm} f^{knn}_{r}  $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   $$ \\hspace{0.1 cm} f^{knn}_{r} \\hspace{0.15cm}=\\hspace{0.15cm} \\dfrac{ \\# \\hspace{0.1cm}\\lbrace\\hspace{0.1cm} i \\in KNN \\hspace{0.1cm}/\\hspace{0.1cm} i \\in \\Omega_r \\hspace{0.1cm}\\rbrace  }{k} $$\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5. \\hspace{0.15cm}$ Classify $\\hspace{0.1cm} x_{new} \\hspace{0.1cm}$ in that group such has a bigger neighbors proportion $\\hspace{0.18cm} \\Rightarrow \\hspace{0.18cm}$ If $\\hspace{0.1cm} f^{knn}_{s} \\geqslant f^{knn}_{r} \\hspace{0.1cm}$   $\\hspace{0.1cm} \\forall r = 1,...,g \\hspace{0.1cm}  \\Rightarrow \\hspace{0.1cm} x_{new} \\hspace{0.1cm}$ is classify in $\\hspace{0.1cm} \\Omega_s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example:\n",
    "\n",
    "Sample: $n=3$\n",
    "\n",
    "Predictors:\n",
    "\n",
    "$X1 = (10 , 2 , 4)$\n",
    "$X2 = (20 , 25, 40)$\n",
    "\n",
    "Observations:\n",
    "\n",
    "$x_1 =(10,20)$\n",
    "$x_2=(2,25)$\n",
    "$x_3=(4,40)$\n",
    "\n",
    "Response: (2 categories (0,1), then 2 groups $\\Omega_0 , \\Omega_1$)\n",
    "\n",
    "$Y =( 1 , 1 , 0 )$\n",
    "\n",
    "Distance $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$  $ \\delta_{Euclidean}$\n",
    "\n",
    "New observation:\n",
    "\n",
    "$x_{new}=(6, 20)$\n",
    "\n",
    "Computing the distances:\n",
    "\n",
    "$\\delta(x_{new}, x_1)_{Euclidean} = (10-6)^2 + (20-20)^2 = 16$\n",
    "$\\delta(x_{new}, x_2)_{Euclidean} = (2-6)^2 + (25-20)^2 = 16 + 25 = 41$\n",
    "$\\delta(x_{new}, x_3)_{Euclidean} = (4-6)^2 + (40-20)^2 = 4 + 400 = 404$\n",
    "\n",
    "Selecting k=2 nearest neighbor to $x_{new}$: $x_1$ and $x_2$\n",
    "\n",
    "Computing the proportions $f^{knn}$ : $f^{knn}_0 =  0/2 = 0$ , $f^{knn}_1 =  2/2 = 1$\n",
    "\n",
    "So, the algorithm classify $x_{new}$ in the group $\\Omega_1$ , so the algorithm predict that $y_{new} = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why KNN is a supervised classification method and not an unsupervised ?\n",
    "\n",
    "Because in this problem we have a vector of observations of the response variable $Y$\n",
    "\n",
    "The fact that we haven't $\\hspace{0.1 cm} y_{new} \\hspace{0.1 cm}$ doesn't transform it in a unsupervised problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal k (ver Aurea)\n",
    "\n",
    "The typical approach to select the value of $k$ is to set $k=\\sqrt{n}$\n",
    "\n",
    "A more complex approach is the following:\n",
    "\n",
    "\n",
    "Tenemos x_i=(x_{i1},...,x_{ip}) y tambien y_i para i =1,...,n\n",
    "\n",
    "Suponer que x_i es una observacion no clasificada (auqnue realmente conocemos y_i) \n",
    "\n",
    "Fijar un valor de k\n",
    "\n",
    "Clasificar x_i usando KNN\n",
    "\n",
    "Determinar si la clasificacion ha sido correcta o incorrecta (lo podemos hacer gracias a que tenemos el dato y_i)\n",
    "\n",
    "Repetir el proceso para el resto de i=1,....,n \n",
    "\n",
    "Calcular la proporcion de clasificaciones correctas (proporcion de aciertos)  , esta sera la metrica de error del KNN para el hiperparametro k\n",
    "\n",
    "Repetir el proceso para el resto de k=2,3,...\n",
    "\n",
    "Seleccionar el valor de k que maximiza la proporcion de aciertos (minimiza la proporcion de errores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender_classification = pd.read_csv('gender_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_hair</th>\n",
       "      <th>forehead_width_cm</th>\n",
       "      <th>forehead_height_cm</th>\n",
       "      <th>nose_wide</th>\n",
       "      <th>nose_long</th>\n",
       "      <th>lips_thin</th>\n",
       "      <th>distance_nose_to_lip_long</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1</td>\n",
       "      <td>13.6</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1</td>\n",
       "      <td>11.9</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>1</td>\n",
       "      <td>12.9</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1</td>\n",
       "      <td>13.2</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>1</td>\n",
       "      <td>15.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5001 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      long_hair  forehead_width_cm  forehead_height_cm  nose_wide  nose_long  \\\n",
       "0             1               11.8                 6.1          1          0   \n",
       "1             0               14.0                 5.4          0          0   \n",
       "2             0               11.8                 6.3          1          1   \n",
       "3             0               14.4                 6.1          0          1   \n",
       "4             1               13.5                 5.9          0          0   \n",
       "...         ...                ...                 ...        ...        ...   \n",
       "4996          1               13.6                 5.1          0          0   \n",
       "4997          1               11.9                 5.4          0          0   \n",
       "4998          1               12.9                 5.7          0          0   \n",
       "4999          1               13.2                 6.2          0          0   \n",
       "5000          1               15.4                 5.4          1          1   \n",
       "\n",
       "      lips_thin  distance_nose_to_lip_long  gender  \n",
       "0             1                          1    Male  \n",
       "1             1                          0  Female  \n",
       "2             1                          1    Male  \n",
       "3             1                          1    Male  \n",
       "4             0                          0  Female  \n",
       "...         ...                        ...     ...  \n",
       "4996          0                          0  Female  \n",
       "4997          0                          0  Female  \n",
       "4998          0                          0  Female  \n",
       "4999          0                          0  Female  \n",
       "5000          1                          1    Male  \n",
       "\n",
       "[5001 rows x 8 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gender_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10 ,  weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_neighbors = Number of neighbors to use (k)\n",
    "weights:\n",
    "   ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "\n",
    "   ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "\n",
    "   [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n",
    "\n",
    "metric:\n",
    "\n",
    "  Metric to use for distance computation. Default is “minkowski”, which results in the standard Euclidean distance when p = 2. See the documentation of scipy.spatial.distance and the metrics listed in distance_metrics for valid metric values.\n",
    "\n",
    "  If metric is “precomputed”, X is assumed to be a distance matrix and must be square during fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Gender_classification.iloc[0:20 , 0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    " Y = Gender_classification.iloc[0:20 , 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = [1,\t10, 5, 1, 0, 1,\t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Female']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "print( knn.predict( [x_new] ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6 0.4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print( knn.predict_proba([x_new]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm for not depend on sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dist_Euclidea_Python(i, j, Quantitative_Data_set):\n",
    "\n",
    "    Dist_Euclidea = ( ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] )**2 ).sum()\n",
    "\n",
    "    Dist_Euclidea = np.sqrt(Dist_Euclidea)\n",
    "\n",
    "    return Dist_Euclidea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dist_Euclidea_Matrix_Python( Quantitative_Data_set ):\n",
    "\n",
    "    M = np.zeros((Quantitative_Data_set.shape[0] , Quantitative_Data_set.shape[0]))\n",
    "\n",
    "    for i in range(0 , Quantitative_Data_set.shape[0]):\n",
    "        for j in range(0 , Quantitative_Data_set.shape[0]):\n",
    "\n",
    "            M[i,j]=Dist_Euclidea_Python(i+1,j+1, Quantitative_Data_set)\n",
    "                 \n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_classification( X , Y , x_new, k, distance = \"Minkowski\" , q = 0, p1=0, p2=0, p3=0 ):\n",
    "\n",
    "####################################################################################################################################################################################################################################################\n",
    "\n",
    "    # Y tiene que ser una variable categorica con categorias estandar (0,1,2,...)\n",
    "\n",
    "    # Ejemplo de Y :  Y = Gender_classification.iloc[0:20 , 7]\n",
    "\n",
    "    # Ejemplo de como codificar Y en categorias estandar (si ya esta en el formato estandar indicado no hace falta):\n",
    "      \n",
    "      # for i in range(0, len(Y)):\n",
    "          # if Y[i]=='Male':\n",
    "          #   Y[i] = 0\n",
    "          # elif Y[i]=='Female':\n",
    "          #   Y[i]=1\n",
    "\n",
    "    # X tiene que ser un panda data frame con los predictotres (X1,...,Xp). Ejemplo X = Gender_classification.iloc[0:20 , 0:7]\n",
    "\n",
    "    # x_new tiene que ser una panda series. Ejemplo x_new = pd.Series({'long_hair': 1, 'forehead_width_cm': 4, 'forehead_height_cm': 6, 'nose_wide': 1 , 'nose_long': 1 , 'nose_long': 1 , 'lips_thin':1, 'distance_nose_to_lip_long': 1 })\n",
    "\n",
    "####################################################################################################################################################################################################################################################\n",
    "\n",
    "    X = pd.concat([X, x_new.to_frame().T], ignore_index=True)\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    groups_knn = []\n",
    "\n",
    "\n",
    "####################################################################################################################################################################################################################################################\n",
    "    \n",
    "    if distance == \"Euclidean\":\n",
    "\n",
    "        def Dist_Euclidea_Python(i, j, Quantitative_Data_set): \n",
    "\n",
    "            Dist_Euclidea = ( ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] )**2 ).sum()\n",
    "\n",
    "            Dist_Euclidea = np.sqrt(Dist_Euclidea)\n",
    "\n",
    "            return Dist_Euclidea\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Euclidea_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "    if distance == \"Minkowski\":\n",
    "\n",
    "        def Dist_Minkowski_Python(i,j, q , Quantitative_Data_set):\n",
    "\n",
    "            Dist_Minkowski = ( ( ( ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] ).abs() )**q ).sum() )**(1/q)\n",
    "\n",
    "            return Dist_Minkowski\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Minkowski_Python( len(X), i , q , X) )\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "    if distance == \"Canberra\":\n",
    "\n",
    "        def Dist_Canberra_Python(i,j, Quantitative_Data_set):\n",
    "\n",
    "            Dist_Canberra =  ( ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] ).abs()  / ( (Quantitative_Data_set.iloc[i-1, ]).abs() + (Quantitative_Data_set.iloc[j-1, ]).abs() ) ).sum()\n",
    "\n",
    "            return Dist_Canberra\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Canberra_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "   \n",
    "    if distance == \"Pearson\":\n",
    "\n",
    "        def Dist_Pearson_Python(i, j, Quantitative_Data_set):\n",
    "\n",
    "            Dist_Pearson = ( ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] )**2 / Quantitative_Data_set.var() ).sum()\n",
    "\n",
    "            Dist_Pearson = np.sqrt(Dist_Pearson)\n",
    "\n",
    "            return Dist_Pearson\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Pearson_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "    if distance == \"Mahalanobis\":\n",
    "\n",
    "        def Dist_Mahalanobis_Python(i, j, Quantitative_Data_set):\n",
    "\n",
    "            x = (Quantitative_Data_set.to_numpy()[i-1, ] - Quantitative_Data_set.to_numpy()[j-1, ])\n",
    "\n",
    "            x = np.array([x]) # necessary step to transpose a 1D array\n",
    "\n",
    "            S_inv = np.linalg.inv( Quantitative_Data_set.cov() ) # inverse of covariance matrix\n",
    "\n",
    "            Dist_Maha = np.sqrt( x @ S_inv @ x.T )  # x @ S_inv @ x.T = np.matmul( np.matmul(x , S_inv) , x.T )\n",
    "\n",
    "            Dist_Maha = float(Dist_Maha)\n",
    "\n",
    "            return Dist_Maha\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Mahalanobis_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "    if distance == \"Sokal\":\n",
    "\n",
    "        def Sokal_Similarity_Py(i, j, Binary_Data_Matrix):\n",
    "\n",
    "            X = Binary_Data_Py\n",
    "            a = X @ X.T\n",
    "            n = X.shape[0]\n",
    "            p = X.shape[1]\n",
    "            ones_matrix = np.ones((n, p))\n",
    "            b = (ones_matrix - X) @ X.T\n",
    "            c = b.T\n",
    "            d = (ones_matrix - X) @ (ones_matrix - X).T\n",
    "\n",
    "            Sokal_Similarity = (a.iloc[i-1,j-1] + d.iloc[i-1,j-1])/p\n",
    "\n",
    "            return Sokal_Similarity\n",
    "\n",
    "\n",
    "        def Dist_Sokal_Python(i, j, Binary_Data_set):\n",
    "\n",
    "            dist_Sokal = np.sqrt( Sokal_Similarity_Py(i,i, Binary_Data_set) + Sokal_Similarity_Py(i,i, Binary_Data_set) - 2*Sokal_Similarity_Py(i,j, Binary_Data_set) )\n",
    "\n",
    "            return dist_Sokal\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Sokal_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "   \n",
    "    if distance == \"Jaccard\":\n",
    "\n",
    "        def Jaccard_Similarity_Py(i, j, Binary_Data_Matrix):\n",
    "\n",
    "            X = Binary_Data_Py\n",
    "            a = X @ X.T\n",
    "            n = X.shape[0]\n",
    "            p = X.shape[1]\n",
    "            ones_matrix = np.ones((n, p)) \n",
    "            b = (ones_matrix - X) @ X.T\n",
    "            c = b.T\n",
    "            d = (ones_matrix - X) @ (ones_matrix - X).T\n",
    "\n",
    "            Jaccard_Similarity = a.iloc[i-1,j-1] / (a.iloc[i-1,j-1] + b.iloc[i-1,j-1] + c.iloc[i-1,j-1])\n",
    "            \n",
    "            return Jaccard_Similarity\n",
    "\n",
    "\n",
    "        def Dist_Jaccard_Python(i, j, Binary_Data_set):\n",
    "\n",
    "            dist_Jaccard = np.sqrt( Jaccard_Similarity_Py(i,i, Binary_Data_set) + Jaccard_Similarity_Py(i,i, Binary_Data_set) - 2*Jaccard_Similarity_Py(i,j, Binary_Data_set) )\n",
    "\n",
    "            return dist_Jaccard\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Jaccard_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "    if distance == \"Matches\":\n",
    "\n",
    "        def alpha_py(i,j, Multiple_Categorical_Data):\n",
    "\n",
    "            X = Multiple_Categorical_Data\n",
    "            alpha = np.repeat(0, X.shape[1])\n",
    "\n",
    "            for k in range(0, X.shape[1]) :\n",
    "\n",
    "                if X.iloc[i-1, k] == X.iloc[j-1, k] :\n",
    "\n",
    "                    alpha[k] = 1\n",
    "\n",
    "                else :\n",
    "\n",
    "                    alpha[k] = 0\n",
    "\n",
    "            alpha = alpha.sum()\n",
    "\n",
    "            return(alpha)\n",
    "\n",
    "\n",
    "        def matches_similarity_py(i, j, Multiple_Categorical_Data):\n",
    "\n",
    "            p = Multiple_Categorical_Data.shape[1]\n",
    "\n",
    "            matches_similarity = alpha_py(i,j, Multiple_Categorical_Data) / p\n",
    "\n",
    "            return(matches_similarity)\n",
    "\n",
    "\n",
    "        def Dist_Matches_Py(i,j, Multiple_Categorical_Data):\n",
    "\n",
    "            Dist_Matches = np.sqrt( matches_similarity_py(i, i, Multiple_Categorical_Data) +  matches_similarity_py(j, j, Multiple_Categorical_Data) - 2*matches_similarity_py(i, j, Multiple_Categorical_Data) )\n",
    "\n",
    "            return( Dist_Matches )\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Matches_Py( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "   \n",
    "    if distance == \"Gower\":\n",
    "\n",
    "        def Gower_Similarity_Python(i,j, Mixed_Data_Set, p1, p2, p3):\n",
    "\n",
    "            X = Mixed_Data_Set\n",
    "\n",
    "            # The variable must to be order in the following way: \n",
    "            # the p1 first are quantitative, the following p2 are binary categorical, and the following p3 are multiclass categorical.\n",
    "\n",
    "        #################################################################################\n",
    "      \n",
    "            def G(k, X):\n",
    "\n",
    "                range = X.iloc[:,k].max() - X.iloc[:,k].min()\n",
    "                \n",
    "                return(range)\n",
    "\n",
    "            G_vector = np.repeat(0, p1)\n",
    "\n",
    "            for r in range(0, p1):\n",
    "\n",
    "                G_vector[r] = G(r, X)\n",
    "            \n",
    "        ########################################################################################\n",
    "    \n",
    "            ones = np.repeat(1, p1)\n",
    "\n",
    "            Quantitative_Data = X.iloc[: , 0:p1]\n",
    "            Binary_Data = X.iloc[: , (p1):(p1+p2)]\n",
    "            Multiple_Categorical_Data = X.iloc[: , (p1+p2):(p1+p2+p3) ]\n",
    "\n",
    "            a = Binary_Data @ Binary_Data.T\n",
    "\n",
    "            ones_matrix = np.ones(( Binary_Data.shape[0] , Binary_Data.shape[1])) \n",
    "   \n",
    "            d = (ones_matrix - Binary_Data) @ (ones_matrix - Binary_Data).T\n",
    "\n",
    "        #################################################################################\n",
    "\n",
    "            numerator_part_1 = ( ones - ( (Quantitative_Data.iloc[i,:] - Quantitative_Data.iloc[j,:]).abs() / G_vector ) ).sum() \n",
    "\n",
    "            numerator_part_2 = a.iloc[i,j] + alpha_py(i,j, Multiple_Categorical_Data)\n",
    "\n",
    "            numerator = numerator_part_1 + numerator_part_2\n",
    "\n",
    "            denominator = p1 + (p2 - d.iloc[i,j]) + p3\n",
    "\n",
    "            Similarity_Gower = numerator / denominator  \n",
    "\n",
    "            return(Similarity_Gower)\n",
    "\n",
    "        #################################################################################\n",
    "        \n",
    "        def Dist_Gower_Py(i, j, Mixed_Data , p1, p2, p3):\n",
    "\n",
    "            Dist_Gower = np.sqrt( 1 - Gower_Similarity_Python(i, j, Mixed_Data , p1, p2, p3) )\n",
    "\n",
    "            return(Dist_Gower)    \n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Gower_Py( len(X), i , X, p1, p2, p3) )\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if distance == \"euclidean\":\n",
    "\n",
    "        def Dist_Euclidea_Python(i, j, Quantitative_Data_set): \n",
    "\n",
    "            Dist_Euclidea = ( ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] )**2 ).sum()\n",
    "\n",
    "            Dist_Euclidea = np.sqrt(Dist_Euclidea)\n",
    "\n",
    "            return Dist_Euclidea\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Euclidea_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "\n",
    "    distances = pd.DataFrame({'distances': distances})\n",
    "\n",
    "    distances = distances.sort_values(by=[\"distances\"]).reset_index(drop=False)\n",
    "        \n",
    "    knn = distances.iloc[0:k , :]\n",
    "\n",
    "    for i in knn.iloc[:,0]:\n",
    "\n",
    "        groups_knn.append(Y[i])\n",
    "\n",
    "    unique, counts = np.unique(groups_knn , return_counts=True)\n",
    "\n",
    "    unique_Y , counts_Y = np.unique(Y , return_counts=True)\n",
    "\n",
    "    proportions_groups_knn = pd.DataFrame({'proportions_groups': counts/k, 'groups': unique_Y })\n",
    "\n",
    "    prediction_group = proportions_groups_knn.sort_values(by=[\"proportions_groups\"], ascending=False).iloc[0,:]['groups']\n",
    "\n",
    "    message = print( \"x_new is classify in the group\", prediction_group , \". So KNN algorithm predict y_new =\",  prediction_group )                                      \n",
    "                                       \n",
    "\n",
    "    return proportions_groups_knn , message\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_10248\\712242455.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Y[i] = 0\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_10248\\712242455.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Y[i]=1\n"
     ]
    }
   ],
   "source": [
    "Gender_classification = pd.read_csv('gender_classification.csv')\n",
    "\n",
    "X = Gender_classification.iloc[: , 0:7]\n",
    "\n",
    "Y = Gender_classification.iloc[: , 7]\n",
    "\n",
    "x_new = pd.Series({'long_hair': 1, 'forehead_width_cm': 4, 'forehead_height_cm': 6, 'nose_wide': 1 , 'nose_long': 1 , 'nose_long': 1 , 'lips_thin':1, 'distance_nose_to_lip_long': 1 })\n",
    "\n",
    "for i in range(0, len(Y)):\n",
    "\n",
    "    if Y[i]=='Male':\n",
    "\n",
    "        Y[i] = 0\n",
    "\n",
    "    elif Y[i]=='Female':\n",
    "\n",
    "        Y[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_new is classify in the group 0 . So KNN algorithm predict y_new = 0\n"
     ]
    }
   ],
   "source": [
    "proportions_groups_knn , message = KNN_classification( X , Y , x_new, 10 , distance = \"euclidean\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proportions_groups</th>\n",
       "      <th>groups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   proportions_groups groups\n",
       "0                 0.9      0\n",
       "1                 0.1      1"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportions_groups_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_classification( X , Y , x_new, 10 , distance = \"Minkowski\" , q = 2, p1=0, p2=0, p3=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_classification( X , Y , x_new, 10 , distance = \"Minkowski\" , q = 2, p1=0, p2=0, p3=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN para regresion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c63d8c7d738c2960218a10995aedf0a7f67a49a231e71037adf0440953cdb45b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
