{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN in `Python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN for supervised classification   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\hspace{0.1cm} p \\hspace{0.1cm}$ variables $\\hspace{0.1cm} X=(X_1,...,X_p) \\hspace{0.1cm}$ measurements on a $n$ size sample.\n",
    "\n",
    "We also have a categorical response variable $\\hspace{0.1cm} Y \\hspace{0.1cm}$ with $\\hspace{0.1cm} g \\hspace{0.1cm}$  categories that indicates  the group to which each element of the sample belongs  $ ( \\hspace{0.05cm} Range(Y)=\\lbrace c_1 ,..., c_g \\rbrace \\hspace{0.05cm})$\n",
    "\n",
    "The groups generated by $\\hspace{0.1cm} Y \\hspace{0.1cm}$ are denoted as $\\hspace{0.1cm} \\Omega_1 ,..., \\Omega_g \\hspace{0.15cm}$   $\\hspace{0.15cm}( \\hspace{0.1cm} y_i = c_r \\hspace{0.15cm} \\Leftrightarrow \\hspace{0.15cm}$  $ i \\in \\Omega_r \\hspace{0.1cm})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supervised classification problem consists in, for a new observation $\\hspace{0.1cm} x_{new} = (x_{new,1},x_{new,2},...,x_{new,p}) \\hspace{0.1cm}$ of the variables $X_1,...,X_p  \\hspace{0.1cm}$, predict it's $\\hspace{0.1cm} Y \\hspace{0.1cm}$ value $\\hspace{0.1cm} (y_{new})\\hspace{0.1cm}$  using the information of $\\hspace{0.1cm} X_1,...,X_p \\hspace{0.1cm}$ and $ \\hspace{0.1cm} Y$\n",
    "\n",
    "So , the problem is to classify a new element/individual in one of the $\\hspace{0.1cm} g \\hspace{0.1cm}$ groups generated by $\\hspace{0.1cm} Y \\hspace{0.1cm}$ using the information available of $\\hspace{0.1cm} X_1,...,X_p \\hspace{0.1cm}$ and $Y$, and also $\\hspace{0.1cm} x_{new} = (x_{new,1},x_{new,2},...,x_{new,p})$\n",
    "\n",
    "Note that if we haven't information about $\\hspace{0.1cm} Y \\hspace{0.1cm}$ this would be an unsupervised classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN (K-nearest neighbors) algorithm for classification have the following steps:\n",
    "\n",
    " $1. \\hspace{0.15cm}$ Define a distance measure between the observation of the original sample respect to the variables $X_1,...,X_p$ $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$ $\\delta$\n",
    "\n",
    "\n",
    "\n",
    " $2. \\hspace{0.15cm}$ Compute the distances between $x_{new}$ and the initial observations $\\hspace{0.1cm} \\lbrace x_1,...,x_n \\rbrace$ $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$ $\\lbrace \\hspace{0.1 cm}  \\delta(x_{new}, x_i) \\hspace{0.1 cm} / \\hspace{0.1 cm}  i=1,...,n \\hspace{0.1 cm}  \\rbrace$\n",
    "\n",
    "  \n",
    " $3. \\hspace{0.15cm}$ Select the  $k$ nearest observation to $x_{new}$ based on $\\hspace{0.05cm} \\delta \\hspace{0.12cm}$ $(k$ nearest neighbors of $x_{new})$ $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$   The set of these observation will be denote by $KNN$ \n",
    "\n",
    " $4. \\hspace{0.15cm}$ Compute the proportion of these observation (neighbors) that belongs to each group $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$  \n",
    " \n",
    " $\\hspace{0.65cm} \\Rightarrow \\hspace{0.15cm}$ The proportion of $KNN$ that belongs to the group $\\hspace{0.15cm} \\Omega_r$ $\\hspace{0.1cm}(Y=c_r)\\hspace{0.1cm}$ will be denote by $\\hspace{0.1 cm} f^{knn}_{r}  $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   $$ \\hspace{0.1 cm} f^{knn}_{r} \\hspace{0.15cm}=\\hspace{0.15cm} \\dfrac{ \\# \\hspace{0.1cm}\\lbrace\\hspace{0.1cm} i \\in KNN \\hspace{0.1cm}/\\hspace{0.1cm} i \\in \\Omega_r \\hspace{0.1cm}\\rbrace  }{k} $$\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5. \\hspace{0.15cm}$ Classify $\\hspace{0.1cm} x_{new} \\hspace{0.1cm}$ in that group such has a bigger neighbors proportion $\\hspace{0.18cm} \\Rightarrow \\hspace{0.2cm}$ $\\text{If}   \\hspace{0.15cm} \\underbrace{ f^{knn}_{s} \\geqslant f^{knn}_{r} \\hspace{0.15cm},\\hspace{0.15cm} \\forall r = 1,...,g  }_{\\Omega_s \\hspace{0.1cm}\\text{is the most frequent group in}\\hspace{0.1cm} KNN } $    $\\hspace{0.1cm} \\hspace{0.15cm}  \\Rightarrow \\hspace{0.15cm} x_{new} \\hspace{0.1cm}$ is classify in $\\hspace{0.1cm} \\Omega_s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why KNN is a supervised classification method and not an unsupervised ?\n",
    "\n",
    "Because in this problem we have a vector of observations of the response variable $Y$\n",
    "\n",
    "The fact that we haven't $\\hspace{0.1 cm} y_{new} \\hspace{0.1 cm}$ doesn't transform it in a unsupervised problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example:\n",
    "\n",
    "Sample: $n=3$\n",
    "\n",
    "Predictors:\n",
    "\n",
    "$X1 = (10 , 2 , 4)$\n",
    "$X2 = (20 , 25, 40)$\n",
    "\n",
    "Observations:\n",
    "\n",
    "$x_1 =(10,20)$\n",
    "$x_2=(2,25)$\n",
    "$x_3=(4,40)$\n",
    "\n",
    "Response: (2 categories (0,1), then 2 groups $\\Omega_0 , \\Omega_1$)\n",
    "\n",
    "$Y =( 1 , 1 , 0 )$\n",
    "\n",
    "Distance $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$  $ \\delta_{Euclidean}$\n",
    "\n",
    "New observation:\n",
    "\n",
    "$x_{new}=(6, 20)$\n",
    "\n",
    "Computing the distances:\n",
    "\n",
    "$\\delta(x_{new}, x_1)_{Euclidean} = (10-6)^2 + (20-20)^2 = 16$\n",
    "$\\delta(x_{new}, x_2)_{Euclidean} = (2-6)^2 + (25-20)^2 = 16 + 25 = 41$\n",
    "$\\delta(x_{new}, x_3)_{Euclidean} = (4-6)^2 + (40-20)^2 = 4 + 400 = 404$\n",
    "\n",
    "Selecting k=2 nearest neighbor to $x_{new}$: $x_1$ and $x_2$\n",
    "\n",
    "Computing the proportions $f^{knn}$ : $f^{knn}_0 =  0/2 = 0$ , $f^{knn}_1 =  2/2 = 1$\n",
    "\n",
    "So, the algorithm classify $x_{new}$ in the group $\\Omega_1$ , so the algorithm predict that $\\hat{y}_{new} = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal k (ver Aurea)\n",
    "\n",
    "The typical approach to select the value of $k$ is to set $k=\\sqrt{n}$\n",
    "\n",
    "A more complex approach is the following:\n",
    "\n",
    "\n",
    "Tenemos x_i=(x_{i1},...,x_{ip}) y tambien y_i para i =1,...,n\n",
    "\n",
    "Suponer que x_i es una observacion no clasificada (auqnue realmente conocemos y_i) \n",
    "\n",
    "Fijar un valor de k\n",
    "\n",
    "Clasificar x_i usando KNN\n",
    "\n",
    "Determinar si la clasificacion ha sido correcta o incorrecta (lo podemos hacer gracias a que tenemos el dato y_i)\n",
    "\n",
    "Repetir el proceso para el resto de i=1,....,n \n",
    "\n",
    "Calcular la proporcion de clasificaciones correctas (proporcion de aciertos)  , esta sera la metrica de error del KNN para el hiperparametro k\n",
    "\n",
    "Repetir el proceso para el resto de k=2,3,...\n",
    "\n",
    "Seleccionar el valor de k que maximiza la proporcion de aciertos (minimiza la proporcion de errores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using K-folds Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN for classification in Python with `Sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender_classification = pd.read_csv('gender_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_hair</th>\n",
       "      <th>forehead_width_cm</th>\n",
       "      <th>forehead_height_cm</th>\n",
       "      <th>nose_wide</th>\n",
       "      <th>nose_long</th>\n",
       "      <th>lips_thin</th>\n",
       "      <th>distance_nose_to_lip_long</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   long_hair  forehead_width_cm  forehead_height_cm  nose_wide  nose_long  \\\n",
       "0          1               11.8                 6.1          1          0   \n",
       "1          0               14.0                 5.4          0          0   \n",
       "2          0               11.8                 6.3          1          1   \n",
       "3          0               14.4                 6.1          0          1   \n",
       "4          1               13.5                 5.9          0          0   \n",
       "\n",
       "   lips_thin  distance_nose_to_lip_long  gender  \n",
       "0          1                          1    Male  \n",
       "1          1                          0  Female  \n",
       "2          1                          1    Male  \n",
       "3          1                          1    Male  \n",
       "4          0                          0  Female  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gender_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Gender_classification.iloc[ : , 0:7]\n",
    "\n",
    "Y = Gender_classification.iloc[ : , 7]\n",
    "\n",
    "x_new = [1,\t10, 5, 1, 0, 1,\t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10 ,  weights='uniform', p=2, metric='minkowski')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_neighbors = Number of neighbors to use (k)\n",
    "\n",
    "weights:\n",
    "\n",
    "   ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "\n",
    "   ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "\n",
    "   [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n",
    "\n",
    "metric:\n",
    "\n",
    "  Metric to use for distance computation. Default is “minkowski”, which results in the standard Euclidean distance when p = 2. See the documentation of scipy.spatial.distance and the metrics listed in distance_metrics for valid metric values. Metrics available ‘cityblock’ , 'cosine', ‘euclidean’, 'haversine', 'l1', 'l2', 'manhattan', 'nan_euclidean'.\n",
    "\n",
    "\n",
    "\n",
    "  If metric is “precomputed”, X is assumed to be a distance matrix and must be square during fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Male']\n",
      "[[0.2 0.8]]\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X, Y)\n",
    "\n",
    "print( knn.predict( [x_new] ) )\n",
    " \n",
    "print( knn.predict_proba([x_new]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With other distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10 ,  weights='uniform',  metric='cityblock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Male']\n",
      "[[0.1 0.9]]\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X, Y)\n",
    "\n",
    "print( knn.predict( [x_new] ) )\n",
    " \n",
    "print( knn.predict_proba([x_new]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10 ,  weights='uniform', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Male']\n",
      "[[0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X, Y)\n",
    "\n",
    "print( knn.predict( [x_new] ) ) \n",
    "\n",
    "print( knn.predict_proba([x_new]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10 ,  weights='uniform', metric='nan_euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Male']\n",
      "[[0.2 0.8]]\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X, Y)\n",
    "\n",
    "print( knn.predict( [x_new] ) ) \n",
    "\n",
    "print( knn.predict_proba([x_new]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10 ,  weights='uniform', metric='manhattan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Male']\n",
      "[[0.1 0.9]]\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X, Y)\n",
    "\n",
    "print( knn.predict( [x_new] ) ) \n",
    "\n",
    "print( knn.predict_proba([x_new]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN for classification in Python with own algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm for not depend on sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_classification( X , Y , x_new, k, distance = \"Minkowski\" , q = 0, p1=0, p2=0, p3=0 ):\n",
    "\n",
    "####################################################################################################################################################################################################################################################\n",
    "\n",
    "    # Y tiene que ser una variable categorica con categorias estandar (0,1,2,...)\n",
    "\n",
    "    # Ejemplo de Y :  Y = Gender_classification.iloc[0:20 , 7]\n",
    "\n",
    "    # Ejemplo de como codificar Y en categorias estandar (si ya esta en el formato estandar indicado no hace falta):\n",
    "      \n",
    "      # for i in range(0, len(Y)):\n",
    "          # if Y[i]=='Male':\n",
    "          #   Y[i] = 0\n",
    "          # elif Y[i]=='Female':\n",
    "          #   Y[i]=1\n",
    "\n",
    "    # X tiene que ser un panda data frame con los predictotres (X1,...,Xp). Ejemplo X = Gender_classification.iloc[0:20 , 0:7]\n",
    "\n",
    "    # x_new tiene que ser una panda series. Ejemplo x_new = pd.Series({'long_hair': 1, 'forehead_width_cm': 4, 'forehead_height_cm': 6, 'nose_wide': 1 , 'nose_long': 1 , 'nose_long': 1 , 'lips_thin':1, 'distance_nose_to_lip_long': 1 })\n",
    "\n",
    "####################################################################################################################################################################################################################################################\n",
    "\n",
    "    X = pd.concat([X, x_new.to_frame().T], ignore_index=True)\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    groups_knn = []\n",
    "\n",
    "\n",
    "####################################################################################################################################################################################################################################################\n",
    "    \n",
    "    if distance == \"Euclidean\":\n",
    "\n",
    "        def Dist_Euclidea_Python(i, j, Quantitative_Data_set): \n",
    "\n",
    "            Dist_Euclidea = ( ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] )**2 ).sum()\n",
    "\n",
    "            Dist_Euclidea = np.sqrt(Dist_Euclidea)\n",
    "\n",
    "            return Dist_Euclidea\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Euclidea_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "    if distance == \"Minkowski\":\n",
    "\n",
    "        def Dist_Minkowski_Python(i,j, q , Quantitative_Data_set):\n",
    "\n",
    "            Dist_Minkowski = ( ( ( ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] ).abs() )**q ).sum() )**(1/q)\n",
    "\n",
    "            return Dist_Minkowski\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Minkowski_Python( len(X), i , q , X) )\n",
    "\n",
    "        \n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "    if distance == \"Canberra\":\n",
    "\n",
    "        def Dist_Canberra_Python(i,j, Quantitative_Data_set):\n",
    "\n",
    "            numerator =  ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] ).abs()  \n",
    "\n",
    "            denominator =  ( (Quantitative_Data_set.iloc[i-1, ]).abs() + (Quantitative_Data_set.iloc[j-1, ]).abs() )\n",
    "       \n",
    "            numerator=np.array([numerator], dtype=float)\n",
    "\n",
    "            denominator=np.array([denominator], dtype=float)\n",
    "\n",
    "            Dist_Canberra = ( np.divide( numerator , denominator , out=np.zeros_like(numerator), where=denominator!=0) ).sum()\n",
    "\n",
    "            return Dist_Canberra\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Canberra_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "   \n",
    "    if distance == \"Pearson\":\n",
    "\n",
    "        def Dist_Pearson_Python(i, j, Quantitative_Data_set):\n",
    "\n",
    "            Dist_Pearson = ( ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] )**2 / Quantitative_Data_set.var() ).sum()\n",
    "\n",
    "            Dist_Pearson = np.sqrt(Dist_Pearson)\n",
    "\n",
    "            return Dist_Pearson\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Pearson_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "    if distance == \"Mahalanobis\":\n",
    "\n",
    "        def Dist_Mahalanobis_Python(i, j, Quantitative_Data_set):\n",
    "\n",
    "            x = (Quantitative_Data_set.to_numpy()[i-1, ] - Quantitative_Data_set.to_numpy()[j-1, ])\n",
    "\n",
    "            x = np.array([x]) # necessary step to transpose a 1D array\n",
    "\n",
    "            S_inv = np.linalg.inv( Quantitative_Data_set.cov() ) # inverse of covariance matrix\n",
    "\n",
    "            Dist_Maha = np.sqrt( x @ S_inv @ x.T )  # x @ S_inv @ x.T = np.matmul( np.matmul(x , S_inv) , x.T )\n",
    "\n",
    "            Dist_Maha = float(Dist_Maha)\n",
    "\n",
    "            return Dist_Maha\n",
    "\n",
    "        \n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Mahalanobis_Python( len(X), i , X) )\n",
    "\n",
    "       \n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "    if distance == \"Sokal\":\n",
    "\n",
    "        a = X @ X.T\n",
    "        n = X.shape[0]\n",
    "        p = X.shape[1]\n",
    "        ones_matrix = np.ones((n, p))\n",
    "        b = (ones_matrix - X) @ X.T\n",
    "        c = b.T\n",
    "        d = (ones_matrix - X) @ (ones_matrix - X).T\n",
    "\n",
    "        def Sokal_Similarity_Py(i, j):\n",
    "\n",
    "            Sokal_Similarity = (a.iloc[i-1,j-1] + d.iloc[i-1,j-1])/p\n",
    "\n",
    "            return Sokal_Similarity\n",
    "\n",
    "\n",
    "        def Dist_Sokal_Python(i, j, Binary_Data_set):\n",
    "\n",
    "            dist_Sokal = np.sqrt( 2 - 2*Sokal_Similarity_Py(i,j, Binary_Data_set) )\n",
    "\n",
    "            return dist_Sokal\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Sokal_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "   \n",
    "    if distance == \"Jaccard\":\n",
    "\n",
    "        def Jaccard_Similarity_Py(i, j, Binary_Data_Matrix):\n",
    "\n",
    "            X = Binary_Data_Py\n",
    "            a = X @ X.T\n",
    "            n = X.shape[0]\n",
    "            p = X.shape[1]\n",
    "            ones_matrix = np.ones((n, p)) \n",
    "            b = (ones_matrix - X) @ X.T\n",
    "            c = b.T\n",
    "            d = (ones_matrix - X) @ (ones_matrix - X).T\n",
    "\n",
    "            Jaccard_Similarity = a.iloc[i-1,j-1] / (a.iloc[i-1,j-1] + b.iloc[i-1,j-1] + c.iloc[i-1,j-1])\n",
    "            \n",
    "            return Jaccard_Similarity\n",
    "\n",
    "\n",
    "        def Dist_Jaccard_Python(i, j, Binary_Data_set):\n",
    "\n",
    "            dist_Jaccard = np.sqrt( Jaccard_Similarity_Py(i,i, Binary_Data_set) + Jaccard_Similarity_Py(i,i, Binary_Data_set) - 2*Jaccard_Similarity_Py(i,j, Binary_Data_set) )\n",
    "\n",
    "            return dist_Jaccard\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Jaccard_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "    if distance == \"Matches\":\n",
    "\n",
    "        def alpha_py(i,j, Multiple_Categorical_Data):\n",
    "\n",
    "            X = Multiple_Categorical_Data\n",
    "            alpha = np.repeat(0, X.shape[1])\n",
    "\n",
    "            for k in range(0, X.shape[1]) :\n",
    "\n",
    "                if X.iloc[i-1, k] == X.iloc[j-1, k] :\n",
    "\n",
    "                    alpha[k] = 1\n",
    "\n",
    "                else :\n",
    "\n",
    "                    alpha[k] = 0\n",
    "\n",
    "            alpha = alpha.sum()\n",
    "\n",
    "            return(alpha)\n",
    "\n",
    "\n",
    "        def matches_similarity_py(i, j, Multiple_Categorical_Data):\n",
    "\n",
    "            p = Multiple_Categorical_Data.shape[1]\n",
    "\n",
    "            matches_similarity = alpha_py(i,j, Multiple_Categorical_Data) / p\n",
    "\n",
    "            return(matches_similarity)\n",
    "\n",
    "\n",
    "        def Dist_Matches_Py(i,j, Multiple_Categorical_Data):\n",
    "\n",
    "            Dist_Matches = np.sqrt( matches_similarity_py(i, i, Multiple_Categorical_Data) +  matches_similarity_py(j, j, Multiple_Categorical_Data) - 2*matches_similarity_py(i, j, Multiple_Categorical_Data) )\n",
    "\n",
    "            return( Dist_Matches )\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Matches_Py( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "   \n",
    "    if distance == \"Gower\":\n",
    "\n",
    "        def Gower_Similarity_Python(i,j, Mixed_Data_Set, p1, p2, p3):\n",
    "\n",
    "            X = Mixed_Data_Set\n",
    "\n",
    "            # The variable must to be order in the following way: \n",
    "            # the p1 first are quantitative, the following p2 are binary categorical, and the following p3 are multiclass categorical.\n",
    "\n",
    "        #################################################################################\n",
    "      \n",
    "            def G(k, X):\n",
    "\n",
    "                range = X.iloc[:,k].max() - X.iloc[:,k].min()\n",
    "                \n",
    "                return(range)\n",
    "\n",
    "            G_vector = np.repeat(0, p1)\n",
    "\n",
    "            for r in range(0, p1):\n",
    "\n",
    "                G_vector[r] = G(r, X)\n",
    "            \n",
    "        ########################################################################################\n",
    "    \n",
    "            ones = np.repeat(1, p1)\n",
    "\n",
    "            Quantitative_Data = X.iloc[: , 0:p1]\n",
    "            Binary_Data = X.iloc[: , (p1):(p1+p2)]\n",
    "            Multiple_Categorical_Data = X.iloc[: , (p1+p2):(p1+p2+p3) ]\n",
    "\n",
    "            a = Binary_Data @ Binary_Data.T\n",
    "\n",
    "            ones_matrix = np.ones(( Binary_Data.shape[0] , Binary_Data.shape[1])) \n",
    "   \n",
    "            d = (ones_matrix - Binary_Data) @ (ones_matrix - Binary_Data).T\n",
    "\n",
    "        #################################################################################\n",
    "\n",
    "            numerator_part_1 = ( ones - ( (Quantitative_Data.iloc[i,:] - Quantitative_Data.iloc[j,:]).abs() / G_vector ) ).sum() \n",
    "\n",
    "            numerator_part_2 = a.iloc[i,j] + alpha_py(i,j, Multiple_Categorical_Data)\n",
    "\n",
    "            numerator = numerator_part_1 + numerator_part_2\n",
    "\n",
    "            denominator = p1 + (p2 - d.iloc[i,j]) + p3\n",
    "\n",
    "            Similarity_Gower = numerator / denominator  \n",
    "\n",
    "            return(Similarity_Gower)\n",
    "\n",
    "        #################################################################################\n",
    "        \n",
    "        def Dist_Gower_Py(i, j, Mixed_Data , p1, p2, p3):\n",
    "\n",
    "            Dist_Gower = np.sqrt( 1 - Gower_Similarity_Python(i, j, Mixed_Data , p1, p2, p3) )\n",
    "\n",
    "            return(Dist_Gower)    \n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Gower_Py( len(X), i , X, p1, p2, p3) )\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "    distances = pd.DataFrame({'distances': distances})\n",
    "\n",
    "    distances = distances.sort_values(by=[\"distances\"]).reset_index(drop=False)\n",
    "        \n",
    "    knn = distances.iloc[0:k , :]\n",
    "\n",
    "    for i in knn.iloc[:,0]:\n",
    "\n",
    "        groups_knn.append(Y[i])\n",
    "\n",
    "    unique, counts = np.unique(groups_knn , return_counts=True)\n",
    "\n",
    "    unique_Y , counts_Y = np.unique(Y , return_counts=True)\n",
    "\n",
    "    if len(unique) == len(unique_Y) :\n",
    "\n",
    "        proportions_groups_knn = pd.DataFrame({'proportions_groups': counts/k, 'groups': unique_Y })\n",
    "    \n",
    "    elif len(unique) < len(unique_Y) :\n",
    "\n",
    "        proportions_groups_knn = pd.DataFrame({'proportions_groups': counts/k, 'groups': unique })\n",
    "\n",
    "\n",
    "\n",
    "    prediction_group = proportions_groups_knn.sort_values(by=[\"proportions_groups\"], ascending=False).iloc[0,:]['groups']\n",
    "\n",
    "    message = print( \"x_new is classify in the group\", prediction_group , \". So KNN algorithm predict y_new =\",  prediction_group )                                      \n",
    "                                       \n",
    "\n",
    "    return proportions_groups_knn , message  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender_classification = pd.read_csv('gender_classification.csv')\n",
    "\n",
    "X = Gender_classification.iloc[: , 0:7]\n",
    "\n",
    "Y = Gender_classification.iloc[: , 7]\n",
    "\n",
    "x_new = pd.Series({'long_hair': 1, 'forehead_width_cm': 4, 'forehead_height_cm': 6, 'nose_wide': 1 , 'nose_long': 1 , 'nose_long': 1 , 'lips_thin':1, 'distance_nose_to_lip_long': 1 })\n",
    "\n",
    "for i in range(0, len(Y)):\n",
    "\n",
    "    if Y[i]=='Male':\n",
    "\n",
    "        Y[i] = 0\n",
    "\n",
    "    elif Y[i]=='Female':\n",
    "\n",
    "        Y[i]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_new is classify in the group 0 . So KNN algorithm predict y_new = 0\n"
     ]
    }
   ],
   "source": [
    "proportions_groups_knn , message = KNN_classification( X , Y , x_new, 10 , distance = \"Euclidean\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proportions_groups</th>\n",
       "      <th>groups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   proportions_groups groups\n",
       "0                 0.9      0\n",
       "1                 0.1      1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportions_groups_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_new is classify in the group 0 . So KNN algorithm predict y_new = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   proportions_groups groups\n",
       " 0                 0.9      0\n",
       " 1                 0.1      1,\n",
       " None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_classification( X , Y , x_new, 10 , distance = \"Minkowski\" , q = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_new is classify in the group 0.0 . So KNN algorithm predict y_new = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   proportions_groups  groups\n",
       " 0                 1.0       0,\n",
       " None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_classification( X , Y , x_new, 10 , distance = \"Minkowski\" , q = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_new is classify in the group 1.0 . So KNN algorithm predict y_new = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   proportions_groups  groups\n",
       " 0                 1.0       1,\n",
       " None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_classification( X , Y , x_new, 10 , distance = \"Minkowski\" , q = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_new is classify in the group 0.0 . So KNN algorithm predict y_new = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   proportions_groups  groups\n",
       " 0                 1.0       0,\n",
       " None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_classification( X , Y , x_new, 10 , distance = \"Canberra\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_new is classify in the group 0.0 . So KNN algorithm predict y_new = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   proportions_groups  groups\n",
       " 0                 1.0       0,\n",
       " None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_classification( X , Y , x_new, 10 , distance = \"Mahalanobis\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_new is classify in the group 0.0 . So KNN algorithm predict y_new = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   proportions_groups  groups\n",
       " 0                 1.0       0,\n",
       " None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_classification( X , Y , x_new, 10 , distance = \"Pearson\"  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso no puede implementarse la distancia de Gower porque X no es una matriz de datos mixta (solo tiene cuanqitativas y binarias, faltan las multiclase).\n",
    "Tampoco pueden usarse las distancias de Sokal y Jaccard porque X no es una matriz de datos binarios, ni tampoco el coeficiente de matches (coincidencias) porque X no es una matriz de datos categoricos multiclase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\hspace{0.1cm} p \\hspace{0.1cm}$ variables $\\hspace{0.1cm} X=(X_1,...,X_p) \\hspace{0.1cm}$ measurements on a $n$ size sample.\n",
    "\n",
    "We also have a quantitative response variable $\\hspace{0.1cm} Y $ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression problem consists in, for a new observation $\\hspace{0.1cm} x_{new} = (x_{new,1},x_{new,2},...,x_{new,p}) \\hspace{0.1cm}$ of the variables $X_1,...,X_p  \\hspace{0.1cm}$, predict it's $\\hspace{0.1cm} Y \\hspace{0.05cm}$ value $\\hspace{0.1cm} (y_{new})\\hspace{0.1cm}$  using the information of $\\hspace{0.1cm} X_1,...,X_p \\hspace{0.1cm}$ and $ \\hspace{0.1cm} Y$\n",
    "\n",
    "So , the problem is to get $\\hspace{0.1cm} \\hat{y}_{new} \\hspace{0.1cm}$  using the information available of $\\hspace{0.1cm} X_1,...,X_p \\hspace{0.1cm}$ , $Y$ and  $\\hspace{0.1cm} x_{new} = (x_{new,1},x_{new,2},...,x_{new,p})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN (K-nearest neighbors) algorithm for regression have the following steps:\n",
    "\n",
    " $1. \\hspace{0.15cm}$ Define a distance measure between the observation of the original sample respect to the variables $X_1,...,X_p$ $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$ $\\delta$\n",
    "\n",
    "\n",
    "\n",
    " $2. \\hspace{0.15cm}$ Compute the distances between $x_{new}$ and the initial observations $\\hspace{0.1cm} \\lbrace x_1,...,x_n \\rbrace$ $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$ $\\lbrace \\hspace{0.1 cm}  \\delta(x_{new}, x_i) \\hspace{0.1 cm} / \\hspace{0.1 cm}  i=1,...,n \\hspace{0.1 cm}  \\rbrace$\n",
    "\n",
    "  \n",
    " $3. \\hspace{0.15cm}$ Select the  $k$ nearest observation to $x_{new}$ based on $\\hspace{0.05cm} \\delta \\hspace{0.12cm}$ $(k$ nearest neighbors of $x_{new})$ $\\hspace{0.15cm} \\Rightarrow \\hspace{0.15cm}$   The set of these observation will be denote by $KNN$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5. \\hspace{0.15cm}$ The method predict $\\hspace{0.1cm} y_{new} \\hspace{0.1cm}$  as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "   \n",
    " \\widehat{y}_{new} =  \\dfrac{1}{KNN }\\cdot \\sum_{i \\in KNN}  y_i\n",
    "  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN for regression in Python with `Sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN for regression in Python with own algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_regression( X , Y , x_new, k, distance = \"Minkowski\" , q = 0, p1=0, p2=0, p3=0 ):\n",
    "\n",
    "####################################################################################################################################################################################################################################################\n",
    "\n",
    "    # Y tiene que ser una variable cuantitativa\n",
    "\n",
    "    # Ejemplo de Y :  \n",
    "\n",
    " \n",
    "    # X tiene que ser un panda data frame con los predictotres (X1,...,Xp). Ejemplo de X :\n",
    "\n",
    "    # x_new tiene que ser una panda series. Ejemplo   \n",
    "\n",
    "####################################################################################################################################################################################################################################################\n",
    "\n",
    "    X = pd.concat([X, x_new.to_frame().T], ignore_index=True)\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    Y_values_knn = []\n",
    "\n",
    "\n",
    "####################################################################################################################################################################################################################################################\n",
    "    \n",
    "    if distance == \"Euclidean\":\n",
    "\n",
    "        def Dist_Euclidea_Python(i, j, Quantitative_Data_set): \n",
    "\n",
    "            Dist_Euclidea = ( ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] )**2 ).sum()\n",
    "\n",
    "            Dist_Euclidea = np.sqrt(Dist_Euclidea)\n",
    "\n",
    "            return Dist_Euclidea\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Euclidea_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "    if distance == \"Minkowski\":\n",
    "\n",
    "        def Dist_Minkowski_Python(i,j, q , Quantitative_Data_set):\n",
    "\n",
    "            Dist_Minkowski = ( ( ( ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] ).abs() )**q ).sum() )**(1/q)\n",
    "\n",
    "            return Dist_Minkowski\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Minkowski_Python( len(X), i , q , X) )\n",
    "\n",
    "        \n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "    if distance == \"Canberra\":\n",
    "\n",
    "        def Dist_Canberra_Python(i,j, Quantitative_Data_set):\n",
    "\n",
    "            numerator =  ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] ).abs()  \n",
    "\n",
    "            denominator =  ( (Quantitative_Data_set.iloc[i-1, ]).abs() + (Quantitative_Data_set.iloc[j-1, ]).abs() )\n",
    "       \n",
    "            numerator=np.array([numerator], dtype=float)\n",
    "\n",
    "            denominator=np.array([denominator], dtype=float)\n",
    "\n",
    "            Dist_Canberra = ( np.divide( numerator , denominator , out=np.zeros_like(numerator), where=denominator!=0) ).sum()\n",
    "\n",
    "            return Dist_Canberra\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Canberra_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "   \n",
    "    if distance == \"Pearson\":\n",
    "\n",
    "        def Dist_Pearson_Python(i, j, Quantitative_Data_set):\n",
    "\n",
    "            Dist_Pearson = ( ( Quantitative_Data_set.iloc[i-1, ] - Quantitative_Data_set.iloc[j-1, ] )**2 / Quantitative_Data_set.var() ).sum()\n",
    "\n",
    "            Dist_Pearson = np.sqrt(Dist_Pearson)\n",
    "\n",
    "            return Dist_Pearson\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Pearson_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "    if distance == \"Mahalanobis\":\n",
    "\n",
    "        def Dist_Mahalanobis_Python(i, j, Quantitative_Data_set):\n",
    "\n",
    "            x = (Quantitative_Data_set.to_numpy()[i-1, ] - Quantitative_Data_set.to_numpy()[j-1, ])\n",
    "\n",
    "            x = np.array([x]) # necessary step to transpose a 1D array\n",
    "\n",
    "            S_inv = np.linalg.inv( Quantitative_Data_set.cov() ) # inverse of covariance matrix\n",
    "\n",
    "            Dist_Maha = np.sqrt( x @ S_inv @ x.T )  # x @ S_inv @ x.T = np.matmul( np.matmul(x , S_inv) , x.T )\n",
    "\n",
    "            Dist_Maha = float(Dist_Maha)\n",
    "\n",
    "            return Dist_Maha\n",
    "\n",
    "        \n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Mahalanobis_Python( len(X), i , X) )\n",
    "\n",
    "       \n",
    "    ###################################################################\n",
    "    \n",
    "    if distance == \"Sokal\":\n",
    "\n",
    "        a = X @ X.T\n",
    "        n = X.shape[0]\n",
    "        p = X.shape[1]\n",
    "        ones_matrix = np.ones((n, p))\n",
    "        b = (ones_matrix - X) @ X.T\n",
    "        c = b.T\n",
    "        d = (ones_matrix - X) @ (ones_matrix - X).T\n",
    "\n",
    "        def Sokal_Similarity_Py(i, j):\n",
    "\n",
    "            Sokal_Similarity = (a.iloc[i-1,j-1] + d.iloc[i-1,j-1])/p\n",
    "\n",
    "            return Sokal_Similarity\n",
    "\n",
    "\n",
    "        def Dist_Sokal_Python(i, j, Binary_Data_set):\n",
    "\n",
    "            dist_Sokal = np.sqrt( 2 - 2*Sokal_Similarity_Py(i,j, Binary_Data_set) )\n",
    "\n",
    "            return dist_Sokal\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Sokal_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "   \n",
    "    if distance == \"Jaccard\":\n",
    "\n",
    "        def Jaccard_Similarity_Py(i, j, Binary_Data_Matrix):\n",
    "\n",
    "            X = Binary_Data_Py\n",
    "            a = X @ X.T\n",
    "            n = X.shape[0]\n",
    "            p = X.shape[1]\n",
    "            ones_matrix = np.ones((n, p)) \n",
    "            b = (ones_matrix - X) @ X.T\n",
    "            c = b.T\n",
    "            d = (ones_matrix - X) @ (ones_matrix - X).T\n",
    "\n",
    "            Jaccard_Similarity = a.iloc[i-1,j-1] / (a.iloc[i-1,j-1] + b.iloc[i-1,j-1] + c.iloc[i-1,j-1])\n",
    "            \n",
    "            return Jaccard_Similarity\n",
    "\n",
    "\n",
    "        def Dist_Jaccard_Python(i, j, Binary_Data_set):\n",
    "\n",
    "            dist_Jaccard = np.sqrt( Jaccard_Similarity_Py(i,i, Binary_Data_set) + Jaccard_Similarity_Py(i,i, Binary_Data_set) - 2*Jaccard_Similarity_Py(i,j, Binary_Data_set) )\n",
    "\n",
    "            return dist_Jaccard\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Jaccard_Python( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "    if distance == \"Matches\":\n",
    "\n",
    "        def alpha_py(i,j, Multiple_Categorical_Data):\n",
    "\n",
    "            X = Multiple_Categorical_Data\n",
    "            alpha = np.repeat(0, X.shape[1])\n",
    "\n",
    "            for k in range(0, X.shape[1]) :\n",
    "\n",
    "                if X.iloc[i-1, k] == X.iloc[j-1, k] :\n",
    "\n",
    "                    alpha[k] = 1\n",
    "\n",
    "                else :\n",
    "\n",
    "                    alpha[k] = 0\n",
    "\n",
    "            alpha = alpha.sum()\n",
    "\n",
    "            return(alpha)\n",
    "\n",
    "\n",
    "        def matches_similarity_py(i, j, Multiple_Categorical_Data):\n",
    "\n",
    "            p = Multiple_Categorical_Data.shape[1]\n",
    "\n",
    "            matches_similarity = alpha_py(i,j, Multiple_Categorical_Data) / p\n",
    "\n",
    "            return(matches_similarity)\n",
    "\n",
    "\n",
    "        def Dist_Matches_Py(i,j, Multiple_Categorical_Data):\n",
    "\n",
    "            Dist_Matches = np.sqrt( matches_similarity_py(i, i, Multiple_Categorical_Data) +  matches_similarity_py(j, j, Multiple_Categorical_Data) - 2*matches_similarity_py(i, j, Multiple_Categorical_Data) )\n",
    "\n",
    "            return( Dist_Matches )\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Matches_Py( len(X), i , X) )\n",
    "\n",
    "    ###################################################################\n",
    "   \n",
    "    if distance == \"Gower\":\n",
    "\n",
    "        def Gower_Similarity_Python(i,j, Mixed_Data_Set, p1, p2, p3):\n",
    "\n",
    "            X = Mixed_Data_Set\n",
    "\n",
    "            # The variable must to be order in the following way: \n",
    "            # the p1 first are quantitative, the following p2 are binary categorical, and the following p3 are multiclass categorical.\n",
    "\n",
    "        #################################################################################\n",
    "      \n",
    "            def G(k, X):\n",
    "\n",
    "                range = X.iloc[:,k].max() - X.iloc[:,k].min()\n",
    "                \n",
    "                return(range)\n",
    "\n",
    "            G_vector = np.repeat(0, p1)\n",
    "\n",
    "            for r in range(0, p1):\n",
    "\n",
    "                G_vector[r] = G(r, X)\n",
    "            \n",
    "        ########################################################################################\n",
    "    \n",
    "            ones = np.repeat(1, p1)\n",
    "\n",
    "            Quantitative_Data = X.iloc[: , 0:p1]\n",
    "            Binary_Data = X.iloc[: , (p1):(p1+p2)]\n",
    "            Multiple_Categorical_Data = X.iloc[: , (p1+p2):(p1+p2+p3) ]\n",
    "\n",
    "            a = Binary_Data @ Binary_Data.T\n",
    "\n",
    "            ones_matrix = np.ones(( Binary_Data.shape[0] , Binary_Data.shape[1])) \n",
    "   \n",
    "            d = (ones_matrix - Binary_Data) @ (ones_matrix - Binary_Data).T\n",
    "\n",
    "        #################################################################################\n",
    "\n",
    "            numerator_part_1 = ( ones - ( (Quantitative_Data.iloc[i,:] - Quantitative_Data.iloc[j,:]).abs() / G_vector ) ).sum() \n",
    "\n",
    "            numerator_part_2 = a.iloc[i,j] + alpha_py(i,j, Multiple_Categorical_Data)\n",
    "\n",
    "            numerator = numerator_part_1 + numerator_part_2\n",
    "\n",
    "            denominator = p1 + (p2 - d.iloc[i,j]) + p3\n",
    "\n",
    "            Similarity_Gower = numerator / denominator  \n",
    "\n",
    "            return(Similarity_Gower)\n",
    "\n",
    "        #################################################################################\n",
    "        \n",
    "        def Dist_Gower_Py(i, j, Mixed_Data , p1, p2, p3):\n",
    "\n",
    "            Dist_Gower = np.sqrt( 1 - Gower_Similarity_Python(i, j, Mixed_Data , p1, p2, p3) )\n",
    "\n",
    "            return(Dist_Gower)    \n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "        for i in range(1, len(X)):\n",
    "\n",
    "            distances.append( Dist_Gower_Py( len(X), i , X, p1, p2, p3) )\n",
    "\n",
    "    ######################################################################################################################################\n",
    "    \n",
    "    distances = pd.DataFrame({'distances': distances})\n",
    "\n",
    "    distances = distances.sort_values(by=[\"distances\"]).reset_index(drop=False)\n",
    "        \n",
    "    knn = distances.iloc[0:k , :]\n",
    "\n",
    "    for i in knn.iloc[:,0]:\n",
    "\n",
    "        Y_values_knn.append(Y.iloc[i , :])\n",
    "\n",
    "    \n",
    "    from itertools import chain\n",
    "\n",
    "    Y_values_knn = list(chain(*Y_values_knn)) # To unlist a list\n",
    "\n",
    "\n",
    "    prediction = sum(Y_values_knn)/k\n",
    "\n",
    "    message = print( \"KNN algorithm predict y_new =\",  prediction )                                      \n",
    "                                       \n",
    "\n",
    "    return prediction , message  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the KNN_regression function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender_classification = pd.read_csv('gender_classification.csv')\n",
    "\n",
    "for i in range(0, len(Gender_classification)):\n",
    "\n",
    "    if Gender_classification['gender'][i] == 'Male':\n",
    "\n",
    "        Gender_classification['gender'][i] = 0\n",
    "\n",
    "    elif Gender_classification['gender'][i] == 'Female':\n",
    "\n",
    "        Gender_classification['gender'][i] = 1\n",
    "\n",
    "X = Gender_classification.loc[ : , ['long_hair', 'forehead_height_cm', 'nose_wide', 'nose_long', 'lips_thin', 'distance_nose_to_lip_long', 'gender']]\n",
    "\n",
    "Y = Gender_classification.loc[: , ['forehead_width_cm']]  # forehead_width_cm will be our response in this case, because is quantitative (and  we are now in a regression problem)\n",
    "\n",
    "x_new = pd.Series({'long_hair': 1, 'forehead_height_cm': 6, 'nose_wide': 1 , 'nose_long': 1 , 'nose_long': 1 , 'lips_thin':1, 'distance_nose_to_lip_long': 1 , 'gender': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN algorithm predict y_new = 13.11\n"
     ]
    }
   ],
   "source": [
    "prediction , message = KNN_regression( X , Y , x_new, k=10, distance = \"Minkowski\" , q = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.11"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN algorithm predict y_new = 12.95\n"
     ]
    }
   ],
   "source": [
    "prediction , message = KNN_regression( X , Y , x_new, k=10, distance = \"Minkowski\" , q = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN algorithm predict y_new = 13.48\n"
     ]
    }
   ],
   "source": [
    "prediction , message = KNN_regression( X , Y , x_new, k=10, distance = \"Canberra\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['gender'] = X['gender'].astype(int) # it´s necessary to work with Mahalanobis distance function (all columns of X must be type = int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN algorithm predict y_new = 13.37\n"
     ]
    }
   ],
   "source": [
    "prediction , message = KNN_regression( X , Y , x_new, k=10, distance = \"Mahalanobis\"  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN algorithm predict y_new = 13.11\n"
     ]
    }
   ],
   "source": [
    "prediction , message = KNN_regression( X , Y , x_new, k=10, distance = \"Euclidean\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN algorithm predict y_new = 13.820000000000002\n"
     ]
    }
   ],
   "source": [
    "prediction , message = KNN_regression( X , Y , x_new, k=10, distance = \"Pearson\"  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c63d8c7d738c2960218a10995aedf0a7f67a49a231e71037adf0440953cdb45b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
