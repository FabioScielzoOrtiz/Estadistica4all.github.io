<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Distancia de Mahalanobis Robusta | Distancias Estadísticas</title>
  <meta name="description" content="Esta es una introducción a las distancias estadísticas." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Distancia de Mahalanobis Robusta | Distancias Estadísticas" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Esta es una introducción a las distancias estadísticas." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Distancia de Mahalanobis Robusta | Distancias Estadísticas" />
  
  <meta name="twitter:description" content="Esta es una introducción a las distancias estadísticas." />
  

<meta name="author" content="Fabio Scielzo Ortiz" />


<meta name="date" content="2023-03-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="distancia-de-gower-generalizada.html"/>
<link rel="next" href="bibliografía.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Distancias Estadísticas</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="data-sets.html"><a href="data-sets.html"><i class="fa fa-check"></i><b>2</b> Data-sets</a></li>
<li class="chapter" data-level="3" data-path="distancias.html"><a href="distancias.html"><i class="fa fa-check"></i><b>3</b> Distancias</a>
<ul>
<li class="chapter" data-level="3.1" data-path="distancias.html"><a href="distancias.html#casi-métrica"><i class="fa fa-check"></i><b>3.1</b> Casi-métrica</a></li>
<li class="chapter" data-level="3.2" data-path="distancias.html"><a href="distancias.html#semi-métrica"><i class="fa fa-check"></i><b>3.2</b> Semi-métrica</a></li>
<li class="chapter" data-level="3.3" data-path="distancias.html"><a href="distancias.html#métrica"><i class="fa fa-check"></i><b>3.3</b> Métrica</a></li>
<li class="chapter" data-level="3.4" data-path="distancias.html"><a href="distancias.html#distancia"><i class="fa fa-check"></i><b>3.4</b> Distancia</a></li>
<li class="chapter" data-level="3.5" data-path="distancias.html"><a href="distancias.html#matriz-de-distancias"><i class="fa fa-check"></i><b>3.5</b> Matriz de distancias</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="distancias-con-variables-estadísticas-cuantitativas.html"><a href="distancias-con-variables-estadísticas-cuantitativas.html"><i class="fa fa-check"></i><b>4</b> Distancias con variables estadísticas cuantitativas</a>
<ul>
<li class="chapter" data-level="4.1" data-path="distancias-con-variables-estadísticas-cuantitativas.html"><a href="distancias-con-variables-estadísticas-cuantitativas.html#distancia-euclidea"><i class="fa fa-check"></i><b>4.1</b> Distancia Euclidea</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="distancias-con-variables-estadísticas-cuantitativas.html"><a href="distancias-con-variables-estadísticas-cuantitativas.html#distancia-euclidea-en-python"><i class="fa fa-check"></i><b>4.1.1</b> Distancia Euclidea en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="distancias-con-variables-estadísticas-cuantitativas.html"><a href="distancias-con-variables-estadísticas-cuantitativas.html#distancia-de-minkowski"><i class="fa fa-check"></i><b>4.2</b> Distancia de Minkowski</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="distancias-con-variables-estadísticas-cuantitativas.html"><a href="distancias-con-variables-estadísticas-cuantitativas.html#casos-particulares-de-la-distancia-de-minkowski"><i class="fa fa-check"></i><b>4.2.1</b> Casos particulares de la distancia de Minkowski</a></li>
<li class="chapter" data-level="4.2.2" data-path="distancias-con-variables-estadísticas-cuantitativas.html"><a href="distancias-con-variables-estadísticas-cuantitativas.html#distancia-de-minkowski-en-python"><i class="fa fa-check"></i><b>4.2.2</b> Distancia de Minkowski en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="distancias-con-variables-estadísticas-cuantitativas.html"><a href="distancias-con-variables-estadísticas-cuantitativas.html#distancia-de-canberra"><i class="fa fa-check"></i><b>4.3</b> Distancia de Canberra</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="distancias-con-variables-estadísticas-cuantitativas.html"><a href="distancias-con-variables-estadísticas-cuantitativas.html#distancia-de-canberra-en-python"><i class="fa fa-check"></i><b>4.3.1</b> Distancia de Canberra en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="distancias-con-variables-estadísticas-cuantitativas.html"><a href="distancias-con-variables-estadísticas-cuantitativas.html#distancia-de-pearson"><i class="fa fa-check"></i><b>4.4</b> Distancia de Pearson</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="distancias-con-variables-estadísticas-cuantitativas.html"><a href="distancias-con-variables-estadísticas-cuantitativas.html#distancia-de-pearson-en-python"><i class="fa fa-check"></i><b>4.4.1</b> Distancia de Pearson en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="distancias-con-variables-estadísticas-cuantitativas.html"><a href="distancias-con-variables-estadísticas-cuantitativas.html#distancia-de-mahalanobis"><i class="fa fa-check"></i><b>4.5</b> Distancia de Mahalanobis</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="distancias-con-variables-estadísticas-cuantitativas.html"><a href="distancias-con-variables-estadísticas-cuantitativas.html#distancia-de-mahalanobis-en-python"><i class="fa fa-check"></i><b>4.5.1</b> Distancia de Mahalanobis en <code>Python</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="similaridades.html"><a href="similaridades.html"><i class="fa fa-check"></i><b>5</b> Similaridades</a>
<ul>
<li class="chapter" data-level="5.1" data-path="similaridades.html"><a href="similaridades.html#similaridad"><i class="fa fa-check"></i><b>5.1</b> Similaridad</a></li>
<li class="chapter" data-level="5.2" data-path="similaridades.html"><a href="similaridades.html#pasar-de-similaridad-a-distancia"><i class="fa fa-check"></i><b>5.2</b> Pasar de similaridad a distancia</a></li>
<li class="chapter" data-level="5.3" data-path="similaridades.html"><a href="similaridades.html#matriz-de-similaridades"><i class="fa fa-check"></i><b>5.3</b> Matriz de Similaridades</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="similaridades-con-variables-categoricas-binarias.html"><a href="similaridades-con-variables-categoricas-binarias.html"><i class="fa fa-check"></i><b>6</b> Similaridades con variables categoricas binarias</a>
<ul>
<li class="chapter" data-level="6.1" data-path="similaridades-con-variables-categoricas-binarias.html"><a href="similaridades-con-variables-categoricas-binarias.html#parámetros-a-b-c-y-d"><i class="fa fa-check"></i><b>6.1</b> Parámetros a, b , c y d</a></li>
<li class="chapter" data-level="6.2" data-path="similaridades-con-variables-categoricas-binarias.html"><a href="similaridades-con-variables-categoricas-binarias.html#matrices-con-los-parámetros-a-b-c-y-d"><i class="fa fa-check"></i><b>6.2</b> Matrices con los parámetros a, b, c y d</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="similaridades-con-variables-categoricas-binarias.html"><a href="similaridades-con-variables-categoricas-binarias.html#cálculo-de-las-matrices-a-b-c-y-d-en-python"><i class="fa fa-check"></i><b>6.2.1</b> Cálculo de las matrices a, b , c y d en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="similaridades-con-variables-categoricas-binarias.html"><a href="similaridades-con-variables-categoricas-binarias.html#similaridad-de-sokal"><i class="fa fa-check"></i><b>6.3</b> Similaridad de Sokal</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="similaridades-con-variables-categoricas-binarias.html"><a href="similaridades-con-variables-categoricas-binarias.html#distancia-de-sokal"><i class="fa fa-check"></i><b>6.3.1</b> Distancia de Sokal</a></li>
<li class="chapter" data-level="6.3.2" data-path="similaridades-con-variables-categoricas-binarias.html"><a href="similaridades-con-variables-categoricas-binarias.html#similaridad-de-sokal-en-python"><i class="fa fa-check"></i><b>6.3.2</b> Similaridad de Sokal en <code>Python</code></a></li>
<li class="chapter" data-level="6.3.3" data-path="similaridades-con-variables-categoricas-binarias.html"><a href="similaridades-con-variables-categoricas-binarias.html#distancia-de-sokal-en-python"><i class="fa fa-check"></i><b>6.3.3</b> Distancia de Sokal en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="similaridades-con-variables-categoricas-binarias.html"><a href="similaridades-con-variables-categoricas-binarias.html#similaridad-de-jaccard"><i class="fa fa-check"></i><b>6.4</b> Similaridad de Jaccard</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="similaridades-con-variables-categoricas-binarias.html"><a href="similaridades-con-variables-categoricas-binarias.html#distancia-de-jaccard"><i class="fa fa-check"></i><b>6.4.1</b> Distancia de Jaccard</a></li>
<li class="chapter" data-level="6.4.2" data-path="similaridades-con-variables-categoricas-binarias.html"><a href="similaridades-con-variables-categoricas-binarias.html#similaridad-de-jaccard-en-python"><i class="fa fa-check"></i><b>6.4.2</b> Similaridad de Jaccard en <code>Python</code></a></li>
<li class="chapter" data-level="6.4.3" data-path="similaridades-con-variables-categoricas-binarias.html"><a href="similaridades-con-variables-categoricas-binarias.html#distancia-de-jaccard-en-python"><i class="fa fa-check"></i><b>6.4.3</b> Distancia de Jaccard en <code>Python</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="similaridades-con-variables-categoricas-multiclase.html"><a href="similaridades-con-variables-categoricas-multiclase.html"><i class="fa fa-check"></i><b>7</b> Similaridades con variables categoricas multiclase</a>
<ul>
<li class="chapter" data-level="7.1" data-path="similaridades-con-variables-categoricas-multiclase.html"><a href="similaridades-con-variables-categoricas-multiclase.html#parámetro-alpha"><i class="fa fa-check"></i><b>7.1</b> Parámetro alpha</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="similaridades-con-variables-categoricas-multiclase.html"><a href="similaridades-con-variables-categoricas-multiclase.html#parámetro-alpha-en-python"><i class="fa fa-check"></i><b>7.1.1</b> Parámetro alpha en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="similaridades-con-variables-categoricas-multiclase.html"><a href="similaridades-con-variables-categoricas-multiclase.html#similaridad-simple-matching"><i class="fa fa-check"></i><b>7.2</b> Similaridad simple matching</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="similaridades-con-variables-categoricas-multiclase.html"><a href="similaridades-con-variables-categoricas-multiclase.html#distancia-simple-matching"><i class="fa fa-check"></i><b>7.2.1</b> Distancia simple matching</a></li>
<li class="chapter" data-level="7.2.2" data-path="similaridades-con-variables-categoricas-multiclase.html"><a href="similaridades-con-variables-categoricas-multiclase.html#similaridad-simple-matching-en-python"><i class="fa fa-check"></i><b>7.2.2</b> Similaridad simple matching en <code>Python</code></a></li>
<li class="chapter" data-level="7.2.3" data-path="similaridades-con-variables-categoricas-multiclase.html"><a href="similaridades-con-variables-categoricas-multiclase.html#distancia-simple-matching-en-python"><i class="fa fa-check"></i><b>7.2.3</b> Distancia simple matching en <code>Python</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="conjuntos-de-variables-estadisticas-de-tipo-mixto.html"><a href="conjuntos-de-variables-estadisticas-de-tipo-mixto.html"><i class="fa fa-check"></i><b>8</b> Conjuntos de variables estadisticas de tipo mixto</a></li>
<li class="chapter" data-level="9" data-path="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html"><a href="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html"><i class="fa fa-check"></i><b>9</b> Distancias con conjuntos de variables de tipo cuantitativo-binario-multiclase</a>
<ul>
<li class="chapter" data-level="9.1" data-path="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html"><a href="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html#similaridad-de-gower"><i class="fa fa-check"></i><b>9.1</b> Similaridad de Gower</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html"><a href="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html#distancia-de-gower"><i class="fa fa-check"></i><b>9.1.1</b> Distancia de Gower</a></li>
<li class="chapter" data-level="9.1.2" data-path="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html"><a href="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html#similaridad-de-gower-en-python"><i class="fa fa-check"></i><b>9.1.2</b> Similaridad de Gower en <code>Python</code></a></li>
<li class="chapter" data-level="9.1.3" data-path="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html"><a href="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html#distancia-de-gower-en-python"><i class="fa fa-check"></i><b>9.1.3</b> Distancia de Gower en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html"><a href="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html#similaridad-de-gower-mahalanobis"><i class="fa fa-check"></i><b>9.2</b> Similaridad de Gower-Mahalanobis</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html"><a href="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html#distancia-de-gower-mahalanobis"><i class="fa fa-check"></i><b>9.2.1</b> Distancia de Gower-Mahalanobis</a></li>
<li class="chapter" data-level="9.2.2" data-path="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html"><a href="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html#similaridad-de-gower-mahalanobis-en-python"><i class="fa fa-check"></i><b>9.2.2</b> Similaridad de Gower-Mahalanobis en <code>Python</code></a></li>
<li class="chapter" data-level="9.2.3" data-path="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html"><a href="distancias-con-conjuntos-de-variables-de-tipo-cuantitativo-binario-multiclase.html#distancia-de-gower-mahalanobis-en-python"><i class="fa fa-check"></i><b>9.2.3</b> Distancia de Gower-Mahalanobis en <code>Python</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="anexo.html"><a href="anexo.html"><i class="fa fa-check"></i><b>10</b> Anexo</a></li>
<li class="chapter" data-level="11" data-path="distancia-de-gower-generalizada.html"><a href="distancia-de-gower-generalizada.html"><i class="fa fa-check"></i><b>11</b> Distancia de Gower Generalizada</a>
<ul>
<li class="chapter" data-level="11.1" data-path="distancia-de-gower-generalizada.html"><a href="distancia-de-gower-generalizada.html#versión-simple"><i class="fa fa-check"></i><b>11.1</b> Versión simple</a></li>
<li class="chapter" data-level="11.2" data-path="distancia-de-gower-generalizada.html"><a href="distancia-de-gower-generalizada.html#versión-related-metric-scaling"><i class="fa fa-check"></i><b>11.2</b> Versión related metric scaling</a></li>
<li class="chapter" data-level="11.3" data-path="distancia-de-gower-generalizada.html"><a href="distancia-de-gower-generalizada.html#distancia-de-gower-generalizada-en-python"><i class="fa fa-check"></i><b>11.3</b> Distancia de Gower Generalizada en <code>Python</code></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html"><i class="fa fa-check"></i><b>12</b> Distancia de Mahalanobis Robusta</a>
<ul>
<li class="chapter" data-level="12.1" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html#estimación-robusta-de-la-matriz-de-covarianzas"><i class="fa fa-check"></i><b>12.1</b> Estimación robusta de la matriz de covarianzas</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html#estimación-robusta-de-la-varianza"><i class="fa fa-check"></i><b>12.1.1</b> Estimación robusta de la varianza</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html#estimación-robusta-de-la-correlación"><i class="fa fa-check"></i><b>12.2</b> Estimación robusta de la correlación</a></li>
<li class="chapter" data-level="12.3" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html#estimación-robusta-de-la-covarianza"><i class="fa fa-check"></i><b>12.3</b> Estimación robusta de la covarianza</a></li>
<li class="chapter" data-level="12.4" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html#estimación-robusta-de-la-matriz-de-correlaciones"><i class="fa fa-check"></i><b>12.4</b> Estimación robusta de la matriz de correlaciones</a></li>
<li class="chapter" data-level="12.5" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html#estimación-robusta-de-la-matriz-de-covarianzas-1"><i class="fa fa-check"></i><b>12.5</b> Estimación robusta de la matriz de covarianzas</a></li>
<li class="chapter" data-level="12.6" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html#inversa-de-la-estimación-robusta-de-la-matriz-de-covarianzas"><i class="fa fa-check"></i><b>12.6</b> Inversa de la estimación robusta de la matriz de covarianzas</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html#pseudo-inversa-de-moore-penrose"><i class="fa fa-check"></i><b>12.6.1</b> Pseudo inversa de Moore-Penrose</a></li>
<li class="chapter" data-level="12.6.2" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html#inversa-generalizada"><i class="fa fa-check"></i><b>12.6.2</b> Inversa generalizada</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html#transformación-de-delvin-para-obtener-s_r-definida-positiva"><i class="fa fa-check"></i><b>12.7</b> Transformación de Delvin para obtener <span class="math inline">\(S_R\)</span> definida positiva</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html#algoritmo-de-delvin-para-obtener-s_r-definida-positiva"><i class="fa fa-check"></i><b>12.7.1</b> Algoritmo de Delvin para obtener <span class="math inline">\(S_R\)</span> definida positiva</a></li>
<li class="chapter" data-level="12.7.2" data-path="distancia-de-mahalanobis-robusta.html"><a href="distancia-de-mahalanobis-robusta.html#distancia-de-mahalanobis-robusta-en-python"><i class="fa fa-check"></i><b>12.7.2</b> Distancia de Mahalanobis Robusta en <code>Python</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i><b>13</b> Bibliografía</a></li>
<li class="divider"></li>
<li><a href="https://estadistica4all.com" target="blank">Estadistica4all.com</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Distancias Estadísticas</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="distancia-de-mahalanobis-robusta" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">Chapter 12</span> Distancia de Mahalanobis Robusta<a href="distancia-de-mahalanobis-robusta.html#distancia-de-mahalanobis-robusta" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Dada una matriz de datos <span class="math inline">\(\hspace{0.03cm} X=(X_1,...,X_p)\hspace{0.03cm}\)</span> de las variables estadísticas <span class="math inline">\(\hspace{0.03cm}\mathcal{X}_1,...,\mathcal{X}_p\)</span></p>
<div class="warning" style="background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;">
<p><span></p>
<p style="margin-left:1em;">
<p><span class="math inline">\(\hspace{0.25cm}\)</span> La distancia de Mahalanobis <strong>robusta</strong> entre el par de observaciones <span class="math inline">\(\hspace{0.1cm}(x_i,x_r)\hspace{0.1cm}\)</span> de las variables estadísticas <span class="math inline">\(\hspace{0.1cm}\mathcal{X}_1,...,\mathcal{X}_p\)</span> se define como: <span class="math inline">\(\\\)</span></p>
<p><span class="math display">\[
\delta(x_i,x_r)_{Maha} \hspace{0.15cm}= \hspace{0.15cm} \sqrt{\hspace{0.1cm}(x_i - x_r)\hspace{0.03cm}^t \cdot S^{-1}_R \cdot (x_i - x_r ) \hspace{0.1cm}}   \\
\]</span></p>
</p>
</p>
<p></span></p>
</div>
<p><br></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(S_R \hspace{0.03cm}\)</span> es la estimación <strong>robusta</strong> de la matriz de covarianzas de las variables <span class="math inline">\(\hspace{0.03cm}(\mathcal{X}_1,\dots , \mathcal{X}_p)\hspace{0.03cm}\)</span> basada en la matriz de datos <span class="math inline">\(\hspace{0.03cm}X=(X_1,...,X_p)\hspace{0.03cm}.\)</span></li>
</ul>
<p><br></p>
<div id="estimación-robusta-de-la-matriz-de-covarianzas" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> Estimación robusta de la matriz de covarianzas<a href="distancia-de-mahalanobis-robusta.html#estimación-robusta-de-la-matriz-de-covarianzas" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="estimación-robusta-de-la-varianza" class="section level3 hasAnchor" number="12.1.1">
<h3><span class="header-section-number">12.1.1</span> Estimación robusta de la varianza<a href="distancia-de-mahalanobis-robusta.html#estimación-robusta-de-la-varianza" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="propuesta-1-desviación-absoluta-mediana" class="section level4 hasAnchor" number="12.1.1.1">
<h4><span class="header-section-number">12.1.1.1</span> Propuesta 1: Desviación absoluta mediana<a href="distancia-de-mahalanobis-robusta.html#propuesta-1-desviación-absoluta-mediana" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Siguiendo esta propuesta;</p>
<p>Dada la muestra <span class="math inline">\(\hspace{0.03cm} X_j\hspace{0.03cm}\)</span> de la variable estadística <span class="math inline">\(\mathcal{X}_j\hspace{0.03cm}\)</span>,</p>
<div class="warning" style="background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;">
<p><span></p>
<p style="margin-left:1em;">
<ul>
<li>La estimación robusta de la varianza de <span class="math inline">\(\mathcal{X}_j\)</span>
se define como:</li>
</ul>
<span class="math display">\[\sigma^2_R(X_j) \hspace{0.05cm} = \hspace{0.05cm} MAD(X_j)^2 \hspace{0.05cm} = \hspace{0.05cm} Me\hspace{0.05cm} \Bigl[ \hspace{0.05cm} \bigl( \hspace{0.1cm} | \hspace{0.1cm}  x_{ij} - Me(X_j) \hspace{0.1cm} |  \hspace{0.1cm} : \hspace{0.1cm} i = 1,...,n \hspace{0.1cm} \bigr)  \hspace{0.05cm} \Bigr]\]</span>
</p>
</p>
<p></span></p>
</div>
<p><br></p>
</div>
<div id="propuesta-2-varianza-recortada" class="section level4 hasAnchor" number="12.1.1.2">
<h4><span class="header-section-number">12.1.1.2</span> Propuesta 2: Varianza recortada<a href="distancia-de-mahalanobis-robusta.html#propuesta-2-varianza-recortada" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Siguiendo esta propuesta;</p>
<p>Dada la muestra <span class="math inline">\(\hspace{0.03cm}X_j\hspace{0.03cm}\)</span> de la variable estadística <span class="math inline">\(\hspace{0.03cm}\mathcal{X}_j\hspace{0.03cm}\)</span>,</p>
<ul>
<li>La muestra <span class="math inline">\(\alpha\)</span>-recortada de <span class="math inline">\(X_j\)</span> se define como:</li>
</ul>
<span class="math display">\[X_j^\alpha \hspace{0.1cm}=\hspace{0.1cm} \Bigl(\hspace{0.1cm} x_{ij} \hspace{0.3cm} \mathbf{:} \hspace{0.3cm} i\in \lbrace 1,...,n \rbrace \hspace{0.12cm} , \hspace{0.15cm}  x_{ij} \hspace{0.05cm}\geq\hspace{0.05cm} Q(\alpha/2\hspace{0.05cm} ,\hspace{0.05cm} X_j) \hspace{0.12cm} , \hspace{0.15cm} x_{ij} \hspace{0.05cm}\leq\hspace{0.05cm} Q(1-\alpha/2 \hspace{0.05cm},\hspace{0.05cm} X_j)   \hspace{0.1cm} \Bigr) \\\]</span>
<div class="warning" style="background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;">
<p><span></p>
<p style="margin-left:1em;">
<ul>
<li>La estimación robusta de la varianza de <span class="math inline">\(\mathcal{X}_j\)</span>
se define como:</li>
</ul>
<p><span class="math display">\[\sigma^2_R(X_j) \hspace{0.07cm} = \hspace{0.07cm} \sigma^2(X_j^\alpha) \hspace{0.07cm} = \hspace{0.07cm} \dfrac{1}{n} \cdot \sum_{i=1}^n \hspace{0.05cm} \Bigl( \hspace{0.05cm} x_{ij} - \overline{X_j^\alpha} \hspace{0.05cm} \Bigr)^2\]</span></p>
</p>
</p>
<p></span></p>
</div>
<p><br></p>
</div>
<div id="propuesta-3-varianza-winsorizada" class="section level4 hasAnchor" number="12.1.1.3">
<h4><span class="header-section-number">12.1.1.3</span> Propuesta 3: Varianza Winsorizada<a href="distancia-de-mahalanobis-robusta.html#propuesta-3-varianza-winsorizada" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Siguiendo esta propuesta;</p>
<p>Dada la muestra <span class="math inline">\(X_j\)</span> de la variable estadística <span class="math inline">\(\mathcal{X}_j\)</span>,</p>
<ul>
<li><p>La muestra <span class="math inline">\(\alpha\)</span>-winsorizada de <span class="math inline">\(X_j\)</span> se define como:</p>
<p><span class="math display">\[X_j^{\alpha} \hspace{0.08cm}=\hspace{0.08cm}   \Bigl(\hspace{0.1cm} h(x) \hspace{0.25cm} \mathbf{:} \hspace{0.25cm}  x \in X_j  \hspace{0.1cm} \Bigr)\]</span>
Donde:</p>
<ul>
<li><span class="math inline">\(h\)</span> es una función definida del siguiente modo:</li>
</ul>
<p><span class="math display">\[h(x) \hspace{0.05cm} = \hspace{0.05cm}  \begin{cases}
  a  \hspace{0.2cm},\hspace{0.2cm} \text{si }\hspace{0.2cm} x \in A \\
  b \hspace{0.2cm},\hspace{0.2cm} \text{si }\hspace{0.2cm}   x \in B \\
  x \hspace{0.2cm},\hspace{0.2cm} \text{si }\hspace{0.2cm}   x \not\in A,B \hspace{0.15cm} \text{y} \hspace{0.15cm} x\in X_j  \end{cases}\]</span></p>
<ul>
<li>Los conjunto <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> son definidos como sigue:</li>
</ul>
<p><span class="math display">\[A \hspace{0.05cm}=\hspace{0.05cm} \lbrace \hspace{0.1cm} x_{ij} \hspace{0.15cm}:\hspace{0.15cm} x_{ij} \hspace{0.05cm}\leq\hspace{0.05cm} Q(\alpha/2 \hspace{0.05cm},\hspace{0.05cm} X_j) \hspace{0.1cm}\rbrace\]</span>
<span class="math display">\[B \hspace{0.05cm}=\hspace{0.05cm} \lbrace \hspace{0.1cm} x_{ij} \hspace{0.15cm}:\hspace{0.15cm} x_{ij} \hspace{0.05cm}\leq\hspace{0.05cm} Q(\alpha/2 \hspace{0.05cm},\hspace{0.05cm} X_j) \hspace{0.1cm}\rbrace\]</span></p>
<ul>
<li><p>Los valores <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> se definen de la siguiente manera:</p>
<ul>
<li><p><span class="math inline">\(a\)</span> es el valor de <span class="math inline">\(X_j\)</span> que es inmediatamente superior a <span class="math inline">\(\hspace{0.05cm}Q(\alpha/2 \hspace{0.05cm},\hspace{0.05cm} X_j)\)</span></p></li>
<li><p><span class="math inline">\(b\)</span> es el valor de <span class="math inline">\(X_j\)</span> que es inmediatamente inferior a <span class="math inline">\(\hspace{0.05cm}Q(1-\alpha/2 \hspace{0.05cm},\hspace{0.05cm} X_j)\)</span></p></li>
</ul></li>
</ul></li>
</ul>
<p><br></p>
<div class="warning" style="background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;">
<p><span></p>
<p style="margin-left:1em;">
<ul>
<li>La estimación robusta de la varianza de <span class="math inline">\(\mathcal{X}_j\)</span>
se define como:</li>
</ul>
<span class="math display">\[\sigma^2_R(X_j) \hspace{0.1cm} = \hspace{0.1cm} \sigma^2(X_j^{\alpha}) \hspace{0.1cm} = \hspace{0.1cm} \dfrac{1}{n} \cdot \sum_{i=1}^n \hspace{0.05cm} \Bigl( \hspace{0.05cm} x_{ij} - \overline{X_j^{\alpha}} \hspace{0.05cm} \Bigr)^2\]</span>
</p>
</p>
<p></span></p>
</div>
<p><br></p>
</div>
</div>
</div>
<div id="estimación-robusta-de-la-correlación" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> Estimación robusta de la correlación<a href="distancia-de-mahalanobis-robusta.html#estimación-robusta-de-la-correlación" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sean <span class="math inline">\(X_j\)</span> y <span class="math inline">\(X_k\)</span> muestras de las variable estadística <span class="math inline">\(\mathcal{X_j}\)</span> y <span class="math inline">\(\mathcal{X_k}\)</span>, respectivamente.</p>
<p>Se define la muestra auxiliar <span class="math inline">\(Z_j\)</span> como <span class="math inline">\(Z_j = \dfrac{X_j}{ \sqrt{\sigma^2_R(X_j)} }\hspace{0.1cm}\)</span>. <span class="math inline">\(\\\)</span></p>
<div class="warning" style="background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;">
<p><span></p>
<p style="margin-left:1em;">
<ul>
<li>La estimación robusta del coeficiente de correlación lineal entre <span class="math inline">\(\mathcal{X}_j\hspace{0.05cm}\)</span> y <span class="math inline">\(\hspace{0.05cm}\mathcal{X}_k\)</span> se define como :</li>
</ul>
<p><span class="math display">\[r_{jk}^R \hspace{0.1cm}=\hspace{0.1cm} R\hspace{0.05cm}(X_j,X_k)_R \hspace{0.1cm}=\hspace{0.1cm} \dfrac{\hspace{0.3cm}\sigma^2_R(Z_j+Z_k)\hspace{0.1cm}-\hspace{0.1cm}\sigma^2_R(Z_j-Z_k)\hspace{0.3cm}}{\sigma^2_R(Z_j+Z_k)\hspace{0.1cm}+\hspace{0.1cm}\sigma^2_R(Z_j-Z_k)}\]</span></p>
</p>
</p>
<p></span></p>
</div>
<p><br></p>
</div>
<div id="estimación-robusta-de-la-covarianza" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> Estimación robusta de la covarianza<a href="distancia-de-mahalanobis-robusta.html#estimación-robusta-de-la-covarianza" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sean <span class="math inline">\(X_j\hspace{0.02cm}\)</span> y <span class="math inline">\(\hspace{0.02cm}X_k\)</span> muestras de las variable estadística <span class="math inline">\(\mathcal{X_j}\hspace{0.02cm}\)</span> y <span class="math inline">\(\hspace{0.02cm}\mathcal{X_k}\)</span>, respectivamente.</p>
<p>Usando la anterior definición de estimación robusta del coeficiente de correlación lineal y la relación existente entre la correlación y la covarianza, se tiene lo siguiente.</p>
<div class="warning" style="background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;">
<p><span></p>
<p style="margin-left:1em;">
<ul>
<li>La estimación robusta de la covarianza entre <span class="math inline">\(\mathcal{X}_j\hspace{0.02cm}\)</span> y <span class="math inline">\(\hspace{0.02cm}\mathcal{X}_k\)</span> es:</li>
</ul>
<span class="math display">\[S\hspace{0.05cm}(X_j,X_k)_R \hspace{0.12cm}=\hspace{0.12cm} R\hspace{0.05cm}(X_j,X_k)_R \cdot \sqrt{ \hspace{0.1cm}\sigma_R^2(X_j) \cdot \sigma_R^2(X_k) \hspace{0.2cm}  }\]</span>
</p>
</p>
<p></span></p>
</div>
<p><br></p>
</div>
<div id="estimación-robusta-de-la-matriz-de-correlaciones" class="section level2 hasAnchor" number="12.4">
<h2><span class="header-section-number">12.4</span> Estimación robusta de la matriz de correlaciones<a href="distancia-de-mahalanobis-robusta.html#estimación-robusta-de-la-matriz-de-correlaciones" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Usando la aproximación anterior para estimar de forma robusta el coeficiente de correlación lineal, la estimación robusta de la matriz de correlaciones entre <span class="math inline">\(p\)</span> variables estadísticas <span class="math inline">\(\mathcal{X}_1,...,\mathcal{X}_p\)</span> sería la matriz formada por tales estimaciones robustas del coeficiente de correlación entre cada par de esas variables, es decir, la siguiente matriz: <span class="math inline">\(\\\)</span></p>
<p><span class="math display">\[R_R \hspace{0.1cm}=\hspace{0.1cm} \Bigr( \hspace{0.1cm} r_{jk}^R \hspace{0.2cm}:\hspace{0.2cm} j,k = 1,...,p  \hspace{0.1cm}\Bigl)\]</span></p>
<p>Donde: <span class="math inline">\(\hspace{0.1cm} r_{jk}^R \hspace{0.08cm}=\hspace{0.08cm} R(X_j, X_k)_R\)</span></p>
<p><br></p>
</div>
<div id="estimación-robusta-de-la-matriz-de-covarianzas-1" class="section level2 hasAnchor" number="12.5">
<h2><span class="header-section-number">12.5</span> Estimación robusta de la matriz de covarianzas<a href="distancia-de-mahalanobis-robusta.html#estimación-robusta-de-la-matriz-de-covarianzas-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Usando la aproximación anterior para estimar de forma robusta la covarianza, la estimación robusta de la matriz de covarianzas entre <span class="math inline">\(p\)</span> variables estadísticas <span class="math inline">\(\mathcal{X}_1,...,\mathcal{X}_p\)</span> sería la matriz formada por tales estimaciones robustas de la covarianza entre cada par de esas variables, es decir, la siguiente matriz: <span class="math inline">\(\\\)</span></p>
<p><span class="math display">\[S_R \hspace{0.1cm}=\hspace{0.1cm} \Bigr( \hspace{0.1cm}  s_{jk}^R  \hspace{0.2cm}:\hspace{0.2cm} k,j = 1,...,p  \hspace{0.1cm}\Bigl)\]</span></p>
<p>Donde: <span class="math inline">\(\hspace{0.1cm} s_{jk}^R \hspace{0.08cm}=\hspace{0.08cm} S(X_j, X_k)_R\)</span></p>
<p><br></p>
</div>
<div id="inversa-de-la-estimación-robusta-de-la-matriz-de-covarianzas" class="section level2 hasAnchor" number="12.6">
<h2><span class="header-section-number">12.6</span> Inversa de la estimación robusta de la matriz de covarianzas<a href="distancia-de-mahalanobis-robusta.html#inversa-de-la-estimación-robusta-de-la-matriz-de-covarianzas" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Para calcular la distancia de Mahalanobis robusta es necesario calcular la inversa de <span class="math inline">\(S_R\)</span>, es decir, es necessario calcular <span class="math inline">\(S_R^{-1}\)</span>.</p>
<div class="warning" style="background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;">
<p><span></p>
<p style="margin-left:1em;">
<ul>
<li><p>Si <span class="math inline">\(\hspace{0.02cm}S_R\hspace{0.02cm}\)</span> es <strong>definida positiva</strong> (autovalores <span class="math inline">\(&gt; 0\)</span>) <span class="math inline">\(\hspace{0.2cm}\Rightarrow\hspace{0.2cm}\)</span> <strong>tiene inversa</strong> <span class="math inline">\(\bigl( \hspace{0.05cm} \exists \hspace{0.15cm} S_R^{-1} \hspace{0.05cm} \bigr)\)</span></p></li>
<li><p>Si es <span class="math inline">\(S_R\)</span> es <strong>semidefinida positiva</strong> (autovalores <span class="math inline">\(&gt; 0\)</span> y alguno <span class="math inline">\(= 0\)</span>) <span class="math inline">\(\hspace{0.2cm}\Rightarrow\hspace{0.2cm}\)</span> <strong>no tiene inversa</strong> <span class="math inline">\(\bigl( \hspace{0.05cm} \neg\hspace{0.1cm} \exists \hspace{0.15cm} S_R^{-1}\hspace{0.05cm} \bigr)\)</span> , pero se podría calcular la <strong>pseudo-inversa de Moore-Penrose</strong> o la <strong>inversa generalizada</strong> .</p></li>
<li><p>Si <span class="math inline">\(S_R\)</span> es <strong>no definida positiva</strong> (algún autovalor <span class="math inline">\(&lt; 0\)</span>) <span class="math inline">\(\hspace{0.2cm}\Rightarrow\hspace{0.2cm}\)</span> puede ser invertible (si no tiene autovalores <span class="math inline">\(=0\)</span>) o no invertible ( si tiene autovalores <span class="math inline">\(=0\)</span>). Pero aún en el primer caso, pese a que tuviera inversa, al usarla con la distancia de Mahalanobis podrían obtenerse distancias negativas o incluso imaginarias, lo que no tendría sentido. Por tanto, <strong>este es el caso más problemático</strong>.</p></li>
</ul>
</p>
</p>
<p></span></p>
</div>
<p><br></p>
<p><strong>Observaciones:</strong></p>
<ul>
<li>La condición para que una matriz sea invertible es que su determinante sea diferente de cero, y como el determinante de una matriz se puede descomponer en el producto de sus autovalores, entonces, una matriz es invertible si y solo si todos sus autovalores son distintos de cero.</li>
</ul>
<p><span class="math display">\[\exists \hspace{0.15cm} S_R^{-1} \hspace{0.2cm}\Leftrightarrow\hspace{0.2cm} det(S_R) \hspace{0.086cm}=\hspace{0.08cm} \lambda_1 \cdot ... \cdot \lambda_n \hspace{0.08cm}\neq\hspace{0.08cm} 0   \hspace{0.2cm}\Leftrightarrow\hspace{0.2cm} \\[0.45cm]
\hspace{0.2cm}\Leftrightarrow\hspace{0.2cm} \neg \hspace{0.1cm} \exists \hspace{0.15cm} i =1,...,n \hspace{0.2cm},\hspace{0.2cm} \lambda_i = 0 \hspace{0.2cm}\Leftrightarrow \\[0.45cm]
\hspace{0.15cm}\Leftrightarrow\hspace{0.2cm}\hspace{0.15cm} S_R \hspace{0.2cm} \text{no tiene autovalores iguales a cero.} \\\]</span></p>
<ul>
<li>Que sea <span class="math inline">\(R_R\)</span> definida positiva equivale a que <span class="math inline">\(S_R\)</span> también lo es.</li>
</ul>
<p><br></p>
<div id="pseudo-inversa-de-moore-penrose" class="section level3 hasAnchor" number="12.6.1">
<h3><span class="header-section-number">12.6.1</span> Pseudo inversa de Moore-Penrose<a href="distancia-de-mahalanobis-robusta.html#pseudo-inversa-de-moore-penrose" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>DEfinición tórica:</strong></p>
<p>A nivel teórico, la pseudo inversa de Moore-Penrose, denotada como <span class="math inline">\(A^+\)</span>, se define para cualquier matriz <span class="math inline">\(A\)</span> de dimensiones <span class="math inline">\(m\times n\)</span> y satisface las siguientes cuatro propiedades:</p>
<p><span class="math inline">\(AA^+A = A\)</span>: La matriz producto <span class="math inline">\(AA^+A\)</span> es igual a la matriz original <span class="math inline">\(A\)</span>.
<span class="math inline">\(A^+AA^+ = A^+\)</span>: La matriz producto <span class="math inline">\(A^+AA^+\)</span> es igual a la pseudo inversa <span class="math inline">\(A^+\)</span>.
<span class="math inline">\((AA^+)^T = AA^+\)</span>: La matriz producto <span class="math inline">\(AA^+\)</span> es hermítica, es decir, su traspuesta conjugada es igual a la matriz misma. En el caso de matrices reales, esto significa que <span class="math inline">\((AA^+)^T = AA^+\)</span>.
<span class="math inline">\((A^+A)^T = A^+A\)</span>: La matriz producto <span class="math inline">\(A^+A\)</span> es hermítica, es decir, su traspuesta conjugada es igual a la matriz misma. En el caso de matrices reales, esto significa que <span class="math inline">\((A^+A)^T = A^+A\)</span>.</p>
<p>La pseudo inversa de Moore-Penrose es una generalización de la matriz inversa para matrices que pueden no ser cuadradas o de rango completo. Proporciona una solución óptima en el sentido de mínimos cuadrados a problemas de sistemas lineales sobredeterminados, subdeterminados o de rango deficiente.</p>
<p><br></p>
<p><strong>Cálculo de la pseudo-inversa a través de SVD:</strong></p>
<p>La pseudo inversa de Moore-Penrose de <span class="math inline">\(S_R\)</span> , se puede calcular utilizando la descomposición en valores singulares (SVD) de la matriz <span class="math inline">\(S_R\)</span>. La expresión general de la pseudo inversa de Moore-Penrose es:</p>
<p><span class="math display">\[S_R^+ \hspace{0.1cm} = \hspace{0.1cm} V \cdot \Sigma^+ \cdot U^t\]</span></p>
<p>Donde <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span> son las matrices de la descomposición en valores singulares de <span class="math inline">\(S_R\)</span>.</p>
<p>La matriz <span class="math inline">\(\Sigma^+\)</span> se define como sigue:</p>
<p>Sea <span class="math inline">\(\lambda_i\)</span> el <span class="math inline">\(i\)</span>-esimo autovalor de <span class="math inline">\(S_R^t\cdot S_R\)</span>. Entonces, <span class="math inline">\(\sqrt{\lambda_i}\)</span> es <span class="math inline">\(i\)</span>-esimo valor singular de <span class="math inline">\(S_R\)</span>.</p>
<p>Si los autovalores mayores que cero son <span class="math inline">\(\lambda_1,..., \lambda_r\)</span>, entonces:</p>
<p><span class="math display">\[\Sigma^+ \hspace{0.1cm}=\hspace{0.1cm} diag\left(\hspace{0.02cm}\frac{1}{\sqrt{\hspace{0.05cm}\lambda_1\hspace{0.1cm}}}\hspace{0.1cm},...,\hspace{0.1cm} \frac{1}{\sqrt{\hspace{0.05cm}\lambda_r\hspace{0.1cm}}}\hspace{0.1cm},\hspace{0.1cm}0\hspace{0.1cm},...,\hspace{0.1cm}0 \hspace{0.02cm}\right)\]</span></p>
<p>Si alguno de los autovalores pese a ser positivo fuese muy pequeño, esto podría dar lugar a problemas numéricos. Esto puede evitarse usando el siguiente procedimiento de truncamiento, que es el empleado en la librería <code>numpy</code> de <code>Python</code> :</p>
<p>Dado un nivel de tolerancia <span class="math inline">\(t&gt;0\)</span>, si los autovalores mayores que <span class="math inline">\(t\)</span> son <span class="math inline">\(\lambda_1,..., \lambda_q\)</span>, entonces:</p>
<p><span class="math display">\[\Sigma^+ \hspace{0.1cm}=\hspace{0.1cm} diag\left(\hspace{0.02cm}\frac{1}{\sqrt{\hspace{0.05cm}\lambda_1\hspace{0.1cm}}}\hspace{0.1cm},...,\hspace{0.1cm} \frac{1}{\sqrt{\hspace{0.05cm}\lambda_q\hspace{0.1cm}}}\hspace{0.1cm},\hspace{0.1cm}0\hspace{0.1cm},...,\hspace{0.1cm}0 \hspace{0.02cm}\right)\]</span>
Notese que <span class="math inline">\(q \leq r\)</span>.</p>
<p><br></p>
</div>
<div id="inversa-generalizada" class="section level3 hasAnchor" number="12.6.2">
<h3><span class="header-section-number">12.6.2</span> Inversa generalizada<a href="distancia-de-mahalanobis-robusta.html#inversa-generalizada" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
</div>
</div>
<div id="transformación-de-delvin-para-obtener-s_r-definida-positiva" class="section level2 hasAnchor" number="12.7">
<h2><span class="header-section-number">12.7</span> Transformación de Delvin para obtener <span class="math inline">\(S_R\)</span> definida positiva<a href="distancia-de-mahalanobis-robusta.html#transformación-de-delvin-para-obtener-s_r-definida-positiva" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Como hemos visto en la sección anterior es importante que <span class="math inline">\(S_R\)</span> sea definida positiva para poder calcular su inversa y que no encontrar problemas en el cálculo de la distancia de Mahalanobis.</p>
<p>Delvin et al. (1975) proponen una transformación para cuando <span class="math inline">\(S_R\)</span> no es definida positiva, obtener una transformación que sea definida positiva.</p>
<div class="warning" style="background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;">
<p><span></p>
<p style="margin-left:1em;">
<ul>
<li><p>La transformación <span class="math inline">\(G\)</span> de <span class="math inline">\(R_R\)</span> propuesta es el siguiente:</p>
<p><span class="math display">\[G(R_R) = \Bigl( \hspace{0.15cm}  g \hspace{0.05cm} \bigl( \hspace{0.1 cm} r(X_j,X_k)_R \hspace{0.1 cm} \bigr)  \hspace{0.15cm} : \hspace{0.15cm} k,j=1,...,p  \hspace{0.15cm} \Bigr)\]</span>
Donde:</p>
<p><span class="math display">\[r_{jk}^R \hspace{0.1 cm} =\hspace{0.1 cm}  r(X_j,X_k)_R\]</span></p>
<p><span class="math display">\[g \hspace{0.05cm} \bigl( \hspace{0.1 cm} r_{j,k}^R \hspace{0.1 cm} \bigr)  \hspace{0.15cm} \hspace{0.05cm} = \hspace{0.05cm}  \begin{cases}
  0  \hspace{0.2cm},\hspace{0.2cm} \text{si }\hspace{0.2cm} | \hspace{0.1cm} r_{j,k}^R \hspace{0.1cm} | \hspace{0.1cm}\leq\hspace{0.1cm} z(\Delta)   \\
  z^{-1}(\hspace{0.1cm} z(\hspace{0.05cm}r_{j,k}^R\hspace{0.05cm})  \hspace{0.1cm}+\hspace{0.1cm} \Delta\hspace{0.1cm}) \hspace{0.2cm},\hspace{0.2cm} \text{si }\hspace{0.2cm}   r_{j,k}^R \hspace{0.1cm}&lt;\hspace{0.1cm} -z(\Delta) \\
   z^{-1}(\hspace{0.1cm} z(\hspace{0.05cm}r_{j,k}^R\hspace{0.05cm})  \hspace{0.1cm}-\hspace{0.1cm} \Delta\hspace{0.1cm}) \hspace{0.2cm},\hspace{0.2cm} \text{si }\hspace{0.2cm}   r_{j,k}^R \hspace{0.1cm} &gt; \hspace{0.1cm} z(\Delta)   \end{cases}\]</span></p>
<p><span class="math inline">\(\Delta\)</span> es un número positivo pequeño, por ejemplo, <span class="math inline">\(\Delta=0\)</span></p>
<p><span class="math display">\[z \hspace{0.05cm} (x) \hspace{0.1cm}=\hspace{0.1cm} tan \hspace{0.1cm} h\hspace{0.05cm} ^{-1}\hspace{0.05cm} (x) \hspace{0.1cm}=\hspace{0.1cm} arc \hspace{0.1cm} tan\hspace{0.05cm} h\hspace{0.05cm}(x) \hspace{0.1cm}= \hspace{0.1cm} \dfrac{1}{2} \cdot log \left(\dfrac{1+x}{1-x}\right)\]</span>
<span class="math display">\[z^{-1} \hspace{0.05cm} (x) \hspace{0.1cm}=\hspace{0.1cm} tan \hspace{0.1cm} h\hspace{0.05cm} \hspace{0.01cm} (x)\]</span></p>
<p><span class="math display">\[z(\Delta) = z(0.05) \approx 0.05\]</span></p></li>
</ul>
</p>
</p>
<p></span></p>
</div>
<p><br></p>
<ul>
<li><p>¿Cómo funciona la transformación <span class="math inline">\(g\)</span>?</p>
<ul>
<li><p>Si <span class="math inline">\(r_{jk}&gt;0.05\)</span>, entonces, <span class="math inline">\(g(r_{jk}) = r_{jk} - \varepsilon\)</span>, con <span class="math inline">\(\varepsilon &gt;0\)</span> pequeño. Es decir, <span class="math inline">\(g\)</span> transforma <span class="math inline">\(r_{jk}\)</span> en una correlación un poco menor.</p></li>
<li><p>Si <span class="math inline">\(r_{jk}&lt;-0.05\)</span>, entonces, <span class="math inline">\(g(r_{jk}) = r_{jk} + \varepsilon\)</span>, con <span class="math inline">\(\varepsilon &gt;0\)</span> pequeño. Es decir, <span class="math inline">\(g\)</span> transforma <span class="math inline">\(r_{jk}\)</span> en una correlación un poco mayor.</p></li>
<li><p>Si <span class="math inline">\(r_{jk} \in [-0.05,0.05]\)</span>, entonces, <span class="math inline">\(g(r_{jk}) =0\)</span>. Es decir, <span class="math inline">\(g\)</span> transforma <span class="math inline">\(r_{jk}\)</span> en cero.</p></li>
</ul>
<p>Este comportamiento podría ser ilustrado con algunos ejemplos.</p></li>
</ul>
<p><br></p>
<div id="algoritmo-de-delvin-para-obtener-s_r-definida-positiva" class="section level3 hasAnchor" number="12.7.1">
<h3><span class="header-section-number">12.7.1</span> Algoritmo de Delvin para obtener <span class="math inline">\(S_R\)</span> definida positiva<a href="distancia-de-mahalanobis-robusta.html#algoritmo-de-delvin-para-obtener-s_r-definida-positiva" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El siguiente algoritmo utiliza la transformación de Delvin para, dada la matriz <span class="math inline">\(S_R\)</span>, obtener una transformación que se definida positva, y por tanto invertible, y que por ello pueda usarse sin problemas para el cálculo de la distancia de Mahalanobis robusta.</p>
<ul>
<li><p>Se calcula <span class="math inline">\(G(R_R)\)</span> .</p></li>
<li><p>Se comprueba si <span class="math inline">\(G(R_R)\)</span> es definida positiva.</p>
<ul>
<li><p>Si <span class="math inline">\(G(R_R)\)</span> es definida positiva, entonces:</p>
<ul>
<li><p>Se obtiene la transformacion definida positiva de <span class="math inline">\(\hspace{0.06cm}S_R\hspace{0.06cm}\)</span> como <span class="math inline">\(\hspace{0.06cm}T\cdot G(R_R) \cdot T\)</span> , donde <span class="math inline">\(\hspace{0.06cm}T=diag\bigl( \hspace{0.06cm}\sigma_R(X_1),...,\sigma_R(X_p)\hspace{0.061cm}\bigr)\hspace{0.06cm}\)</span> y <span class="math inline">\(\hspace{0.06cm}\sigma_R(X_j) = \sqrt{\sigma_R^2(X_j)}\)</span>.</p></li>
<li><p>Se detiene el algoritmo.</p></li>
</ul></li>
<li><p>Si <span class="math inline">\(G(R_R)\)</span> no es definida positiva, entonces:</p>
<ul>
<li><p>Se actualiza: <span class="math inline">\(\hspace{0.1cm}R_R \leftarrow G(R_R)\)</span></p></li>
<li><p>Se vuelve al paso inicial del algoritmo.</p></li>
</ul></li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="distancia-de-mahalanobis-robusta-en-python" class="section level3 hasAnchor" number="12.7.2">
<h3><span class="header-section-number">12.7.2</span> Distancia de Mahalanobis Robusta en <code>Python</code><a href="distancia-de-mahalanobis-robusta.html#distancia-de-mahalanobis-robusta-en-python" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p><br></p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="distancia-de-gower-generalizada.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliografía.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://estadistica4all.com/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
