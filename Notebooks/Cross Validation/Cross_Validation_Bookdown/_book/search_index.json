[["index.html", "Algoritmos de validación de modelos de aprendizaje supervisado Chapter 1 Introducción", " Algoritmos de validación de modelos de aprendizaje supervisado Fabio Scielzo Ortiz 2023-03-07 Chapter 1 Introducción Más artículos: \\(\\hspace{0.1cm}\\) Estadistica4all Autor: \\(\\hspace{0.1cm}\\) Fabio Scielzo Ortiz Si utilizas este artículo, por favor, cítalo: \\(\\hspace{0.5cm}\\) Scielzo Ortiz, F. (2022). Algoritmos de validación cruzada. Estadistica4all. http://estadistica4all.com/Articulos/Algoritmos-de-validacion-cruzada.html Se recomienda abrir el artículo en un ordenador, en una tablet o en un móvil en versión de escritorio. Los algoritmos de validación son una familia muy importante de algoritmos dentro del aprendizaje estadístico o machine learning que permiten evaluar la capacidad predictiva de un modelo predictivo. Distinguiremos dos tipos de algoritmos de validación de modelos: Métodos de validación de modelos de aprendizaje supervisado \\(\\hspace{0.3cm} \\Rightarrow\\hspace{0.3cm}\\) son algoritmos que permiten evaluar modelos de aprendizaje supervisado usando muestras de test y train de los predictores y la respuesta, y una métrica de evaluación. Métodos de evaluación de modelos de aprendizaje no supervisado \\(\\hspace{0.3cm}\\Rightarrow\\hspace{0.3cm}\\) son algoritmos que permiten evaluar modelos de aprendizaje no supervisado usando una muestra de train de los predictores y una métrica de evaluación. Observación: Los métodos de validación de modelos de aprendizaje supervisado también son llamados métodos de validación cruzada. Aunque realmente son mucho más conocidos por su nombre en inglés: cross validation ¿Por qué no se aplican los mismos métodos de validación a los modelos de aprendizaje supervisado y no supervisado? Debido a que tienen unas características diferentes, en particular, en el aprendizaje supervisado se tienen datos de la variable respuesta, mientras que en el no supervisado no se dispone de información alguna. Si se quiere ver un planteamiento algo más detallado de los problemas de aprendizaje supervisado y no supervisado se recomienda leer el artículo sobre ese tema que tenemos en Estadistica4all Los métodos de evaluación son usados para dos propósitos, principalmente: Para seleccionar modelos. Para optimizar hiper-parametros de modelos. En otros artículos estudiaremos métodos para selección de modelos y ajuste de hiper-parámetros. En este articulo vamos a estudiar los métodos de validación de modelos de aprendizaje supervisado. Los métodos de validación de modelos de aprendizaje no supervisado serán estudiados en otro artículo. Por lo que consideraremos que \\(\\hspace{0.1cm} M \\hspace{0.1cm}\\) representa un modelo o algoritmo de aprendizaje supervisado, como por ejemplo el modelo de regresión lineal, regresión logistica, regresión no lineal, regresión lineal penalizada, arboles de regresión y clasificación, KNN , SVM, redes neuronales … Notese que en este articulo no se consideran modelos estadísticos predictivos no supervisados como son los modelos de clustering como K-medias, K-medoids, modelos jerarquicos, modelos basados en densidades … Nos interesa tener un método a través del cual pueda evaluarse la capacidad o poder predictivo del modelo de aprendizaje supervisado \\(\\hspace{0.1cm}M\\hspace{0.1cm}\\). Una aproximación naive es evaluar el poder predictivo del modelo usando dos elementos. Por un lado los datos disponibles de la variable respuesta, y por otro las predicciones que el modelo hace para los datos de los predcitores con los que el modelo ha sido entrenado. La idea sería comparar los valores reales de la respuesta con los predichos por el modelo, usando alguna métrica. Esta aproximación tiende a infra-estimar el error de predicción real del modelo, ya que se está prediciendo la respuesta para observaciones de los predictores que ya han sido “vistas” por el modelo, por ello el rendimiento del modelo al predecir la respuesta para estas observaciones tiende a ser mejor que si fueran observaciones con las que el modelo no ha sido entrenado (observaciones que no ha “visto” aún). Los métodos de validación de modelos predictivos supervisados tiene tres elementos: Muestras de train y test de los predictores y la respuesta. Una métrica de evaluación. Un algoritmo para evaluar el modelo que usa los anteriores dos elementos de algún modo. Este artículo es básicamente un tour teórico-práctico sobre estos elementos. "],["muestras-de-train-y-test.html", "Chapter 2 Muestras de train y test 2.1 Train-set 2.2 Predicciones de train 2.3 Test-set 2.4 Predicciones de test", " Chapter 2 Muestras de train y test Tenemos una muestra de \\(N\\) observaciones de \\(p\\) predictores \\(\\mathcal{X}_1,...,\\mathcal{X}_p\\) y de una variable respuesta \\(\\mathcal{Y}\\) : \\[D=[\\hspace{0.1cm} X_1,...,X_p,Y \\hspace{0.1cm}]=\\begin{pmatrix} x_{11}&amp;x_{12}&amp;...&amp;x_{1p}&amp; y_1\\\\ x_{21}&amp;x_{22}&amp;...&amp;x_{2p} &amp; y_2\\\\ ...&amp;...&amp;...&amp;...\\\\ x_{N1}&amp;x_{N2}&amp;...&amp;x_{Np}&amp; y_N \\end{pmatrix} = \\begin{pmatrix} x_{1}&amp; y_1\\\\ x_{2}&amp; y_2\\\\ ...&amp;...\\\\ x_{N}&amp; y_N \\end{pmatrix} \\\\\\] Sin entrar aquí en particularidades, para evaluar un modelo de aprendizaje supervisado \\(M\\) este tiene que ser entrenado con un subconjunto de \\(n\\) filas de \\(D\\) , llamado muestra de entrenamiento o de train \\(D_{train}\\), y testado con el subconjunto de las \\(h\\) filas restantes de \\(D\\), llamado muestra de test \\(D_{test}\\) , de modo que \\(n+h=N\\) Los métodos de validación típicos usan de algún modo \\(D_{train}\\) y \\(D_{test}\\) , junto con una métrica de evaluación, es por ello que vamos a definir estos elementos con mas precisión a continuación. 2.1 Train-set Los elementos antes mencionados se definen formalmente como sigue: Muestra train de \\(n\\) observaciones del predictor \\(\\mathcal{X}_j\\) : \\[X_j^{train} \\hspace{0.1cm}=\\hspace{0.1cm} (\\hspace{0.1cm} x_{1j}^{train},...,x_{nj}^{train}\\hspace{0.1cm} )^t \\hspace{0.3cm} , \\hspace{0.3cm} j\\in \\lbrace 1,...,p \\rbrace \\\\[0.4cm]\\] Muestra train de \\(n\\) observaciones de la respuesta \\(\\mathcal{Y}\\) : \\[Y^{train} \\hspace{0.1cm}=\\hspace{0.1cm} \\left(\\hspace{0.1cm}y_{1}^{train},...,y_n^{train}\\hspace{0.1cm}\\right)^t \\\\[0.4cm]\\] Train-set : \\[D_{train}\\hspace{0.1cm}=\\hspace{0.1cm}[\\hspace{0.1cm}X_1^{train}\\hspace{0.03cm},...,\\hspace{0.03cm}X_p^{train} \\hspace{0.03cm},\\hspace{0.03cm} Y^{train}\\hspace{0.1cm}]\\hspace{0.1cm} = \\hspace{0.1cm} \\begin{pmatrix} x_{11}^{train} &amp;...&amp;x_{1p}^{train}&amp; y_1^{train}\\\\ x_{21}^{train}&amp;...&amp;x_{2p}^{train} &amp; y_2^{train}\\\\ &amp;...&amp;\\\\ x_{n1}^{train}&amp;...&amp;x_{np}^{train}&amp; y_n^{train} \\end{pmatrix}= \\begin{pmatrix} x_{1}^{train}&amp; y_1^{train}\\\\ x_{2}^{train}&amp; y_2^{train}\\\\ ...&amp;...\\\\ x_{n}^{train}&amp; y_n^{train} \\end{pmatrix} \\\\\\] Observaciones: La fila \\(i\\) de \\(D_{train}\\) , es decir, \\(\\left(x_i^{train},y_i^{train}\\right) = \\left(x_{i1}^{train}, x_{i2}^{train},...,x_{ip}^{train}, y_i^{train}\\right)\\hspace{0.1cm}\\), es la \\(\\hspace{0.1cm}i\\)-esima observación de train de los predictores y la respuesta \\(\\hspace{0.1cm}\\mathcal{X}_1,...,\\mathcal{X}_p \\hspace{0.1cm},\\hspace{0.1cm} \\mathcal{Y}\\). \\(D_{train}\\) también es llamada simplemente observaciones de train de \\(\\mathcal{X}_1,...,\\mathcal{X}_p\\hspace{0.1cm},\\hspace{0.1cm} \\mathcal{Y}\\). 2.2 Predicciones de train Como \\(M\\) es un modelo de aprendizaje supervisado es capaz de, una vez es entrenado con las observaciones de \\(D_{train}=[X_1^{train},...,X_p^{train}, Y^{train}]\\) , generar predicciones de la variable respuesta \\(\\mathcal{Y}\\), tanto para las observaciones de train de los predictores como para nuevas observaciones. Notese que \\(M\\) nos interesa realmente para predecir la variable respuesta para nuevas observaciones de los predictores, es decir, para predecir la respuesta para individuos/elementos de los que solo tenemos información de los predictores. Las predicciones de train de la respuesta \\(\\hspace{0.1cm}\\mathcal{Y}\\hspace{0.1cm}\\) , son obtenidas aplicando el modelo de aprendizaje supervisado ya entrenado a las observaciones de train de los predictores: \\[\\widehat{Y}^{\\hspace{0.1cm} train} \\hspace{0.1cm} = \\hspace{0.1cm} M\\left(\\hspace{0.1cm} X_1^{train},...,X_p^{train} \\hspace{0.2cm}|\\hspace{0.2cm} X_1^{train},...,X_p^{train}, Y^{train} \\hspace{0.1cm}\\right)\\] Observaciones: Aqui \\(M( z \\hspace{0.1cm}|\\hspace{0.1cm} w)\\) representa una función de dos argumentos, el argumento posterior a \\(\\hspace{0.1cm}|\\hspace{0.1cm}\\) , es decir \\(w\\), son los datos de predictores y respuestas con los que se entrena el modelo de aprendizaje supervisado \\(M\\) , y el argumento previo a \\(\\hspace{0.1cm}|\\hspace{0.1cm}\\) , es decir \\(z\\), son los datos de los predictores que el modelo \\(M\\) usa para generar predicciones de la variable respuesta. Y devuelve un vector con esas predicciones de la respuesta. \\(\\widehat{\\hspace{0.01cm} Y}^{\\hspace{0.08cm} train}\\hspace{0.1cm}=\\hspace{0.1cm}(\\hat{y}_1^{train},..., \\hat{y}_n^{train})^t\\hspace{0.15cm}\\) es un vector con las predicciones de la respuesta hechas por el modelo entrenado \\(\\hspace{0.08cm}M\\hspace{0.1cm}\\) para la muestra train de observaciones de los predictores \\(\\hspace{0.1cm}(X_1^{train},...,X_p^{train}) \\\\\\) \\(\\widehat{y}_i^{train} = M(\\hspace{0.1cm} x_i^{train} \\hspace{0.15cm}|\\hspace{0.15cm} X_1^{train},...,X_p^{train}, Y^{train} \\hspace{0.1cm}) = M(\\hspace{0.1cm}x_{i1}^{train},...,x_{ip}^{train} \\hspace{0.15cm}|\\hspace{0.15cm} X_1^{train},...,X_p^{train}, Y^{train}\\hspace{0.1cm})\\hspace{0.1cm}\\) es la predicción de la variable respuesta generada por el modelo entrenado \\(M\\) para la observación de train de los predictores \\(x_i^{train}=(x_{i1}^{train},...,x_{ip}^{train})\\). 2.3 Test-set Muestra test de \\(\\hspace{0.08cm}h\\hspace{0.08cm}\\) observaciones del predictor \\(\\mathcal{X}_j\\): \\[X_j^{test}=\\left(x^{test}_{1j},...,x^{test}_{hj} \\right)^t \\hspace{0.3cm} , \\hspace{0.3cm} \\forall j=1,...,p\\\\[0.4cm]\\] Muestra test de \\(\\hspace{0.1cm}h\\hspace{0.1cm}\\) observaciones de la respuesta \\(\\mathcal{Y}\\): \\[Y^{test}=\\left(y^{test}_{1},...,y^{test}_{h} \\right)^t \\hspace{0.3cm} , \\hspace{0.3cm} \\forall j=1,...,p \\\\[0.4cm]\\] Test-set : \\[D_{test} \\hspace{0.1cm}=\\hspace{0.1cm} [\\hspace{0.1cm}X_1^{test},...,X_p^{test}, Y^{test}\\hspace{0.1cm}] \\hspace{0.1cm}=\\hspace{0.1cm} \\begin{pmatrix} x^{test}_{11}&amp;x^{test}_{12}&amp;...&amp;x^{test}_{1p} &amp; y^{test}_1\\\\ x^{test}_{21}&amp;x^{test}_{22}&amp;...&amp;x^{test}_{2p} &amp; y^{test}_2\\\\ &amp;...&amp;\\\\ x^{test}_{h1}&amp;x^{test}_{h2}&amp;...&amp;x^{test}_{hp} &amp; y^{test}_h \\end{pmatrix}= \\begin{pmatrix} x_{1}^{test}&amp; y_1^{test}\\\\ x_{2}^{test}&amp; y_2^{test}\\\\ ...&amp;...\\\\ x_{h}^{test}&amp; y_h^{test} \\end{pmatrix}\\] Observaciones: La fila \\(i\\) de \\(D_{test}\\) , es decir, \\((x_i^{test},y_i^{test})= (x^{test}_{i1}, x^{test}_{i2},...,x^{test}_{ip}, y_i^{test})\\), es la \\(i\\)-esima observación de test de los predictores y la respuesta \\(\\mathcal{X}_1,...,\\mathcal{X}_p\\hspace{0.08cm},\\hspace{0.08cm} \\mathcal{Y}\\). \\(D_{test}\\) también es llamada simplemente observaciones de test de \\(\\mathcal{X}_1,...,\\mathcal{X}_p, \\mathcal{Y}\\). 2.4 Predicciones de test Puesto que \\(M\\) es un modelo predictivo supervisado es capaz de, una vez es entrenado con las observaciones de \\(D_{train}=[X_1^{train},...,X_p^{train}, Y^{train}]\\) , generar predicciones de la respuesta \\(\\mathcal{Y}\\) , tanto para las observaciones de train como para nuevas observaciones de los predictores \\(\\mathcal{X}_1,...,\\mathcal{X}_p\\) . Notese, de nuevo, que \\(M\\) nos interesa realmente para predecir la respuesta para nuevas observaciones de los predictores, es decir, para predecir la respuesta para individuos/elementos de los que tenemos información sobre los predictores pero no sobre la respuesta. Las predicciones de test de la respuesta \\(\\mathcal{Y}\\) , son obtenidas aplicando el modelo predictivo entrenado a las observaciones de test: \\[\\widehat{Y}\\hspace{0.08cm}^{test} \\hspace{0.1cm} = \\hspace{0.1cm} M\\left(\\hspace{0.1cm} X_1^{test},...,X_p^{test} \\hspace{0.2cm}|\\hspace{0.2cm} X_1^{train},...,X_p^{train}, Y^{train} \\hspace{0.1cm}\\right)\\] Observaciones: Aqui \\(M( z \\hspace{0.1cm}|\\hspace{0.1cm} w)\\) representa una función de dos argumentos, el argumento posterior a \\(\\hspace{0.1cm}|\\hspace{0.1cm}\\) , es decir \\(w\\), son los datos de predictores y respuestas con los que se entrena el modelo \\(M\\) , y el argumento previo a \\(\\hspace{0.1cm}|\\hspace{0.1cm}\\) , es decir \\(z\\), son los datos de los predictores que el modelo \\(M\\) usa para generar predicciones de la respuesta. Y devuelve un vector con esas predicciones de la respuesta. \\(\\widehat{\\hspace{0.02cm}Y}^{\\hspace{0.1cm}test}= (\\hat{y}_1^{\\hspace{0.08cm}test},..., \\hat{y}_h^{\\hspace{0.1cm}test})^t\\) es un vector con las predicciones de la respuesta hechas por el modelo entrenado \\(M\\) usando la muestra test de observaciones de los predictores \\(X_1^{test},...,X_p^{test}\\). \\(\\hat{y}_i^{test} = M \\left(\\hspace{0.1cm} x_i^{test} \\hspace{0.15cm}|\\hspace{0.15cm} X_1^{train},...,X_p^{train}, Y^{train}\\hspace{0.1cm} \\right) = M\\left( \\hspace{0.08cm} x^{test}_{i1},...,x^{test}_{ip} \\hspace{0.15cm}|\\hspace{0.15cm} X_1^{train},...,X_p^{train}, Y^{train}\\right)\\) es la predicción de la respuesta que el modelo entrenado \\(M\\) genera para la observación de test de los predictores \\(x_i^{test}=(x^{test}_{i1},...,x^{test}_{ip})\\) . Teniendo todo lo anterior en cuenta, la evaluación de los modelos de aprendizaje supervisado \\(M\\) se realiza, sin entrar en particularidades, comparando las predicciones de test de la variable respuesta generadas por el modelo \\(M\\), es decir, \\(\\widehat{\\hspace{0.02cm}Y}^{\\hspace{0.08cm}test}\\), con la muestra de observaciones test de la respuesta, \\(Y^{test}\\) La muestra test juega el rol de muestra de nuevas observaciones. Y el modelo \\(M\\) interesa para predecir nuevas observaciones. Por ello se utiliza la muestra test como muestra para evaluar el rendimiento del modelo al predecir la respuesta para nuevas observaciones de los predictores. "],["métricas-para-evaluar-modelos-de-regresión.html", "Chapter 3 Métricas para evaluar modelos de regresión 3.1 Error cuadrático medio (ECM) 3.2 Raiz del error cuadrático medio (RECM) 3.3 Error cuadratico relativo (ECR) 3.4 Coeficiente de determinación 3.5 Error absoluto medio (EAM) 3.6 Error absoluto relativo (EAR)", " Chapter 3 Métricas para evaluar modelos de regresión Dado un modelo de regresión \\(M\\) , existen varias métricas para evaluar la capacidad predictiva del modelo. Cada una de estas métricas tienen una versión de train (son calculadas usando las predicciones de train) y otra de test (son calculadas usando las predicciones de test). Se recomienda al lector haber leido previamente el articulo sobre los problemas de regresión y clasificación supervisada y no supervisada. A continuación vamos a exponer las métricas de evaluación más habituales para modelos de regresión: 3.1 Error cuadrático medio (ECM) ECM de train: \\[ECM(M)_{train} \\hspace{0.15cm}=\\hspace{0.15cm} \\dfrac{1}{n} \\cdot \\sum_{i=1}^n \\hspace{0.1cm} (\\hspace{0.08cm} y_i^{\\hspace{0.08cm}train} - \\hat{y}_i^{\\hspace{0.08cm}train} \\hspace{0.08cm} )^2 \\\\\\] Donde: \\(y_i^{test}\\) es la observación de test \\(i\\)-esima de la variable respuesta, es decir, \\(y_i^{test} \\hspace{0.1cm}=\\hspace{0.1cm} Y^{test}[\\hspace{0.1cm} i\\hspace{0.1cm}]\\). \\(\\hat{y}_i^{test} \\hspace{0.1cm}=\\hspace{0.1cm} M\\left(\\hspace{0.1cm} x_i^{test} \\hspace{0.2cm}|\\hspace{0.2cm} X_1^{train},...,X_p^{train}, Y^{train}\\hspace{0.1cm} \\right)\\). \\(n \\hspace{0.1cm}=\\hspace{0.1cm} \\# \\hspace{0.1cm} Y^{train}\\). ECM de test \\[ECM(M)_{test} \\hspace{0.15cm} =\\hspace{0.15cm} \\dfrac{1}{h} \\cdot \\sum_{i=1}^h \\hspace{0.1cm} (\\hspace{0.1cm} y_i^{\\hspace{0.1cm}test} - \\hat{y}_i^{\\hspace{0.1cm}test} \\hspace{0.1cm} )^2\\] Donde: \\(y_i^{test}\\hspace{0.1cm}\\) es la observación de test \\(\\hspace{0.1cm}i\\)-esima de la variable respuesta, es decir, \\(\\hspace{0.15cm}y_i^{test} \\hspace{0.1cm}=\\hspace{0.1cm} Y^{test}[\\hspace{0.1cm} i \\hspace{0.1cm}]\\). \\(\\hat{y}_i^{test} \\hspace{0.1cm}=\\hspace{0.1cm} M\\left(\\hspace{0.1cm} x_i^{test} \\hspace{0.15cm}|\\hspace{0.15cm} X_1^{train},...,X_p^{train}, Y^{train} \\hspace{0.1cm} \\right)\\). \\(h\\hspace{0.1cm}=\\hspace{0.1cm}\\# \\hspace{0.1cm} Y^{test}\\) Observación: El \\(\\hspace{0.08cm} ECM\\hspace{0.08cm}\\) tiene se mide en la unidad de medida que la respuesta al cuadrado. Interpretación: Cuanto menor sea \\(\\hspace{0.08cm}ECM(M)_{test}\\hspace{0.08cm}\\) , mayor capacidad predictiva del modelo \\(\\hspace{0.08cm}M\\hspace{0.08cm}\\), y a la inversa. ¿Por qué el ECM es tan usado en la práctica? Teóricamente puede demostraste que en un modelo de regresión el error cuadrático medio de la predicción puede descomponerse como sigue: \\[ECM(\\hspace{0.08cm}\\widehat{\\mathcal{Y}}\\hspace{0.08cm}) = E\\hspace{0.1cm}[\\hspace{0.1cm}(\\mathcal{Y} - \\widehat{\\mathcal{Y}})^2\\hspace{0.1cm}]\\hspace{0.1cm} =\\hspace{0.1cm} Var(\\hspace{0.08cm}\\widehat{\\mathcal{Y}}\\hspace{0.08cm}) \\hspace{0.1cm}+\\hspace{0.1cm} Sesgo(\\hspace{0.08cm}\\widehat{\\mathcal{Y}}\\hspace{0.08cm})^2 \\hspace{0.1cm}+\\hspace{0.1cm} \\sigma_{\\varepsilon}^2 \\\\\\] Donde: \\(Var(\\hspace{0.08cm}\\widehat{\\mathcal{Y}}\\hspace{0.08cm})\\hspace{0.1cm} =\\hspace{0.1cm} E[\\hspace{0.1cm}(\\hspace{0.08cm}\\widehat{\\mathcal{Y}} - E[\\hspace{0.08cm}\\widehat{\\mathcal{Y}}\\hspace{0.08cm}]\\hspace{0.08cm})^2\\hspace{0.1cm}]\\hspace{0.1cm}\\) es la varianza de las predicciones del modelo. \\(Sesgo(\\hspace{0.08cm}\\widehat{\\mathcal{Y}}\\hspace{0.08cm})\\hspace{0.1cm}=\\hspace{0.1cm}E[\\hspace{0.08cm}\\hspace{0.1cm}\\widehat{\\mathcal{Y}}\\hspace{0.1cm}] \\hspace{0.1cm}-\\hspace{0.1cm} \\mathcal{Y}\\hspace{0.15cm}\\) es el sesgo de las predicciones del modelo. \\(\\sigma_{\\varepsilon}^2\\hspace{0.1cm}\\) es la varianza del ruido aleatorio o perturbación del modelo. Por tanto, en un modelo de regresion, el ECM del estimador de la respuesta puede descomponerse como la suma de la varianza y sesgo al cuadrado de dicho estimador de la respuesta, y tambien de la varianza del ruido aleatorio del modelo. Los dos primeros terminos (varianza y sesgo del estimador de la respuesta) pueden se reducidos, en función del modelo utilizado. En cambio el tercer componente (la varianza del ruido) es irreducible, no depende del modelo. Esta propiedad del ECM es una de las razones por las que es tan usado, ya que nos da información sobre el ratio varianza-sego de las predicciones de un modelo. Un modelo con baja varianza y sesgo en sus predicciones tendra bajo ECM. Un modelo con alta varianza o alto sesgo en sus predicciones tendra alto ECM. Los modelos con menor ECM de entre una colección de modelos serán aquellos con un mayor equilibrio en el ratio varianza-sesgo en sus predicciones. Un modelo con mucha varianza en sus predicciones es un modelo cuyas predicciones sobre la respuesta varian mucho de una muestra de train a otra. Es decir, al ser entrenado con múltiples muestras, las predicciones que el modelo hace de la respuesta usando una determinada muestra fija de test varían mucho de un modelo entrenado a otro. En un modelo con poca varianza en sus predicciones ocurre al revés, la precisión de sus predicciones es alta en el sentido de que varian poco en funcion de la muestra de train empleada. Otra cuestión es si estas predicciones son mas o menos acertadas (esto tiene que ver mas con el sesgo). Un modelo con mucho sesgo en sus predicciones es un modelo cuyas predicciones sobre la respuesta están en media muy lejos de los verdaderos valores de la respuesta. Es decir, si el modelo es entrenado con múltiples muestras de train y se predice la respuesta para una muestra fija de test, la media de las predicciones de la respuesta obtenidas con cada uno de los modelos entrenados aleja bastante de la verdadera respuesta. En cambio un modelo con bajo sesgo genera predicciones que en media son bastante acertadas. Es decir, si se entrena el modelo con múltiples muestras de train y se predice la respuesta usando una muestra fija de test, la media de esas predicciones obtenidas con cada uno de los modelos entrenados es bastante cercana a la respuesta real. 3.2 Raiz del error cuadrático medio (RECM) RECM de train: \\[RECM(M)_{train} \\hspace{0.08cm}=\\hspace{0.08cm} \\sqrt{\\dfrac{1}{n} \\cdot \\sum_{i=1}^n \\hspace{0.1cm} (y_i^{train} - \\hat{y}_i^{train})^2 \\hspace{0.2cm}} \\\\\\] RECM de test: \\[RECM(M)_{test} \\hspace{0.08cm}=\\hspace{0.08cm} \\sqrt{ \\dfrac{1}{h} \\cdot \\sum_{i=1}^h \\hspace{0.1cm} (\\hspace{0.1cm} y_i^{\\hspace{0.1cm} test} - \\hat{y}_{i} ^{\\hspace{0.1cm} test} \\hspace{0.1cm} )^2 \\hspace{0.2cm}} \\\\[0.2cm]\\] Observación: El \\(\\hspace{0.08cm}RECM\\hspace{0.08cm}\\) tiene la misma unidad de medida que la respuesta. Interpretación: Cuanto menor sea \\(\\hspace{0.08cm}RECM(M)_{test}\\hspace{0.08cm}\\) , mayor bondad predictiva del modelo \\(\\hspace{0.08cm}M\\hspace{0.08cm}\\), y a la inversa. 3.3 Error cuadratico relativo (ECR) ECR de train: \\[ECR(M)_{train} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{\\hspace{0.5cm} \\sum_{i=1}^n \\hspace{0.1cm} ( y_i^{train}- \\hat{y}_i^{train} )^2 \\hspace{0.5cm}}{\\sum_{i=1}^n ( y_i^{train} - \\overline{Y \\hspace{0.1cm}}^{\\hspace{0.1cm} train} )^2 } \\\\\\] ECR de test: \\[ECR(M)_{test} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{ \\hspace{0.5cm} \\sum_{i=1}^h \\hspace{0.1cm} (\\hspace{0.1cm} y_i^{\\hspace{0.1cm} test} - \\hat{y}_i^{\\hspace{0.1cm} test} \\hspace{0.1cm})^2 \\hspace{0.5cm} }{\\sum_{i=1}^h (\\hspace{0.1cm} y_i^{\\hspace{0.1cm} test} - \\overline{Y \\hspace{0.1cm}}^{\\hspace{0.1cm} test} \\hspace{0.1cm} )^2 } \\\\\\] Interpretación: Cuanto menor sea \\(\\hspace{0.08cm}ECR(M)_{test}\\hspace{0.08cm}\\) , mayor bondad predictiva del modelo \\(\\hspace{0.08cm}M\\hspace{0.08cm}\\), y a la inversa. 3.4 Coeficiente de determinación Coeficiente de determinación de train: \\[R(M)^2_{train} \\hspace{0.1cm}=\\hspace{0.1cm} 1 - ECR(M)_{train} \\\\\\] Coeficiente de determinación de train: \\[R(M)^2_{test} \\hspace{0.1cm}=\\hspace{0.1cm} 1 - ECR(M)_{test} \\\\\\] Interpretación: Cuanto mayor sea \\(\\hspace{0.05cm}R(M)^2_{test}\\hspace{0.05cm}\\) , mayor bondad predictiva del modelo \\(\\hspace{0.05cm}M\\hspace{0.05cm}\\), y a la inversa. 3.5 Error absoluto medio (EAM) EAM de train: \\[EAM(M)_{train} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{n} \\cdot \\sum_{i=1}^n \\hspace{0.1cm} \\left|\\hspace{0.1cm} y_i^{train} - \\hat{y}_i^{train} \\hspace{0.1cm}\\right| \\\\\\] EAM de test: \\[EAM(M)_{test} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{h} \\cdot \\sum_{i=1}^h \\hspace{0.1cm} \\left| \\hspace{0.1cm} y_i^{\\hspace{0.1cm}test} - \\hat{y}_i^{\\hspace{0.1cm}test} \\hspace{0.1cm} \\right|\\] 3.6 Error absoluto relativo (EAR) EAR de train: \\[EAR(M)_{train} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{\\sum_{i=1}^n \\hspace{0.1cm} \\left|\\hspace{0.1cm} y_i^{train} - \\hat{y}_i^{\\hspace{0.08cm} train} \\hspace{0.1cm} \\right| \\hspace{0.2cm} }{\\sum_{i=1}^n \\hspace{0.1cm} \\left| \\hspace{0.1cm} y_i^{train} - \\overline{y}^{train} \\hspace{0.1cm} \\right| \\hspace{0.1cm}} \\\\\\] EAR de test: \\[EAR(M)_{test} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{ \\sum_{i=1}^h \\hspace{0.1cm} \\hspace{0.1cm} \\left| \\hspace{0.1cm} y_i^{\\hspace{0.1cm} test} - \\hat{y}_i^{\\hspace{0.1cm} test} \\hspace{0.1cm} \\right| \\hspace{0.2cm} }{\\sum_{i=1}^h \\hspace{0.1cm}\\left|\\hspace{0.1cm} y_i^{\\hspace{0.1cm} test} - \\overline{y}^{\\hspace{0.1cm} test} \\hspace{0.1cm}\\right|\\hspace{0.1cm}}\\] "],["métricas-para-evaluar-modelos-de-clasificación-supervisada.html", "Chapter 4 Métricas para evaluar modelos de clasificación supervisada 4.1 Tasa de acierto en la clasificación (TAC) 4.2 Tasa de error en la clasificación (TEC) 4.3 Kappa", " Chapter 4 Métricas para evaluar modelos de clasificación supervisada A continuación vamos a exponer las métricas de evaluación más habituales para modelos de clasificación supervisada: 4.1 Tasa de acierto en la clasificación (TAC) TAC de train: \\[TAC(M)_{train} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{n} \\cdot \\sum_{i=1}^{n} \\hspace{0.2cm} \\mathbf {I} ( \\hspace{0.1cm} \\hat{y}_i^{train} = y_i^{train} \\hspace{0.1cm} )\\\\\\] TAC de test: \\[TAC(M)_{train} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{n} \\cdot \\sum_{i=1}^{n} \\hspace{0.2cm} \\mathbf{I} ( \\hspace{0.1cm} \\hat{y}_i^{train} = y_i^{train} \\hspace{0.1cm})\\] 4.2 Tasa de error en la clasificación (TEC) TEC de train: \\[TEC(M)_{train} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{n} \\cdot \\sum_{i=1}^{n} \\hspace{0.2cm} \\mathbf {I} \\left( \\hspace{0.05cm} \\hat{y}_i^{train} \\neq y_i^{train} \\hspace{0.05cm} \\right)\\\\\\] TEC de test: \\[TEC(M)_{test} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{h} \\cdot \\sum_{i=1}^{h} \\hspace{0.2cm} \\mathbf {I} \\left( \\hspace{0.1cm} \\hat{y}_i^{\\hspace{0.05cm} test} \\neq y_i^{\\hspace{0.05cm} test} \\hspace{0.1cm}\\right)\\] 4.3 Kappa Suponemos que una variable respuesta tiene \\(\\hspace{0.05cm}k \\geq 2\\hspace{0.05cm}\\) categorias Si usamos un modelo de clasificación basado en la distribución de probabilidad uniforme discreta, este predice cada categoria de la respuesta con igual probabilidad \\(\\hspace{0.05cm}1/k\\hspace{0.05cm}\\), por lo que la tasa de acierto del modelo esperada es \\(\\hspace{0.05cm}1/k\\), es decir, si el modelo se aplica muchas veces, la \\(\\hspace{0.08cm} TAC\\hspace{0.08cm}\\) media será de \\(1/k\\). Kappa de train: \\[Kappa(M)_{\\hspace{0.05cm} train} \\hspace{0.1cm} = \\hspace{0.1cm} \\dfrac{\\hspace{0.1cm} TAC_{\\hspace{0.05cm} train} \\hspace{0.1cm}-\\hspace{0.1cm} 1/k \\hspace{0.1cm}}{1 \\hspace{0.1cm} -\\hspace{0.1cm} 1/k} \\\\\\] Kappa de test: \\[Kappa(M)_{\\hspace{0.05cm} test} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{\\hspace{0.1cm} TAC_{\\hspace{0.05cm} test} \\hspace{0.1cm}-\\hspace{0.1cm} 1/k \\hspace{0.1cm}}{1 \\hspace{0.1cm}-\\hspace{0.1cm} 1/k}\\] 4.3.1 Modelo de clasificación aleatoria uniforme Dada una respuesta \\(\\mathcal{Y}\\) con \\(k\\geq 2\\) categorías y de una serie de predictores \\(\\mathcal{X}_1 , ..., \\mathcal{X}_p\\). Dada una muestra de \\(n\\) observaciones de \\(Y\\) del predictor y una muestra \\(X_j\\) del predictor \\(\\mathcal{X}_j\\). Un modelo de clasificación aleatorio basado en la distribución discreta \\(U\\lbrace 0,1,..., k-1 \\rbrace\\) es un modelo tal que: \\[\\widehat{y}_i \\sim U\\lbrace 0,1,..., k-1 \\rbrace\\] Por lo que: \\[P(\\widehat{y}_i = h) = 1/k = p\\] para todo \\(h=0,1,...,k-1\\) Por tanto, para cada \\(\\hspace{0.15cm}i=1,...,n\\hspace{0.15cm}\\) tenemos que: \\(\\\\[0.15cm]\\) \\(P\\left( \\hspace{0.1cm} \\mathbf{I}(\\hat{y}_i = y_i) = 1\\hspace{0.1cm} \\right) = P \\left( \\hspace{0.1cm} \\hat{y}_i = y_i \\hspace{0.1cm} \\right) \\hspace{0.1cm}=\\hspace{0.1cm} 1/k \\hspace{0.1cm}=\\hspace{0.1cm} p\\). \\(P\\left( \\hspace{0.1cm} \\mathbf{I}(\\hat{y}_i = y_i) = 0 \\hspace{0.1cm} \\right) \\hspace{0.1cm} =\\hspace{0.1cm} P\\left( \\hspace{0.1cm} \\hat{y}_i \\neq y_i \\hspace{0.1cm} \\right) \\hspace{0.1cm}=\\hspace{0.1cm} 1- 1/k \\hspace{0.1cm}=\\hspace{0.1cm} 1 - p\\). Por lo que \\(\\mathbf{I}(\\hat{y}_i = y_i)\\) es una variable binaria, con probabilidad de exito \\(p\\) y de fracaso \\(1-p\\), es decir: \\[\\mathbf{I}(\\hat{y}_i = y_i) \\hspace{0.1cm}\\sim\\hspace{0.1cm} Bernoulli\\left( \\hspace{0.1cm} p=1/k \\hspace{0.1cm} \\right)\\] Por lo tanto: \\[\\sum_{i=1}^n \\mathbf{I}(\\hat{y}_i = y_i) \\hspace{0.1cm}\\sim\\hspace{0.1cm} Binomial(n\\cdot p )\\] Así que, la tasa de acierto esperada del modelo de clasificación aleatoria uniforme es: \\(\\\\[0.2cm]\\) \\[E\\left[TAC\\right] \\hspace{0.1cm}=\\hspace{0.1cm} E\\left[ \\dfrac{1}{n} \\sum_{i=1}^n \\mathbf{I}(\\hat{y}_i = y_i) \\right] \\hspace{0.1cm}=\\hspace{0.1cm} E\\left[ \\dfrac{1}{n} \\cdot Binomial(n\\cdot p ) \\right] \\hspace{0.1cm} = \\\\[1.2cm] = \\hspace{0.1cm} \\dfrac{1}{n} \\cdot E\\left[ Binomial(n\\cdot p ) \\right] \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{n} \\cdot (n\\cdot p) = p = 1/k\\] "],["algoritmos-de-validación-de-modelos-de-aprendizaje-supervisado.html", "Chapter 5 Algoritmos de validación de modelos de aprendizaje supervisado 5.1 Validación simple no aleatoria 5.2 Validación simple aleatoria 5.3 Validación simple aleatoria repetida 5.4 Leave-one-out 5.5 k-fold 5.6 Repeted k-fold ", " Chapter 5 Algoritmos de validación de modelos de aprendizaje supervisado Los algoritmos de validación de modelos de aprendizaje supervisado permiten medir la capacidad predictiva de dichos modelos. Estoas algoritmos se suelen basar en: División del data-set inicial de la respuesta y los predictores en parte de train y parte de test. Entrenamiento del modelo con la parte de train. Obtención de predicciones de la respuesta con la parte de test. Cálculo de una métrica de evaluación usando las predicciones obtenidas en el paso 4) y las observaciones de test de la respuesta. \\(\\\\[0.3cm]\\) Tenemos un modelo de aprendizaje supervisado \\(\\hspace{0.1cm}M\\hspace{0.1cm}\\) y una muestra de \\(\\hspace{0.1cm}N\\hspace{0.1cm}\\) observaciones de \\(\\hspace{0.1cm}p\\hspace{0.1cm}\\) predictores \\(\\hspace{0.1cm}\\mathcal{X}_1,...,\\mathcal{X}_p\\hspace{0.1cm}\\) y de la respuesta \\(\\hspace{0.1cm}\\mathcal{Y}\\\\[0.7cm]\\) \\[D\\hspace{0.1cm}=\\hspace{0.1cm}[X_1,...,X_p,Y]\\hspace{0.1cm}=\\hspace{0.1cm}\\begin{pmatrix} x_{11}&amp;x_{12}&amp;...&amp;x_{1p}&amp; y_1\\\\ x_{21}&amp;x_{22}&amp;...&amp;x_{2p} &amp; y_2\\\\ &amp;...&amp;\\\\ x_{N1}&amp;x_{N2}&amp;...&amp;x_{Np}&amp; y_N \\end{pmatrix}=\\begin{pmatrix} x_{1}&amp; y_1\\\\ x_{2}&amp; y_2\\\\ ...&amp;...\\\\ x_{N}&amp; y_N \\end{pmatrix}\\] 5.1 Validación simple no aleatoria Descripción no formal del algoritmo: Este algoritmo de validación consiste en dividir el data-set inicial en una parte de train y otra de test de manera no aleatoria. El \\(\\hspace{0.1cm} k\\% \\hspace{0.1cm}\\) de las primeras filas del data-set serán la parte de train, y el resto la parte de test. El modelo es entrenado con la muestra train y testado calculando una métrica de evaluación con la muestra test. Este valor de la métrica de evaluación es el que será usado para medir la capacidad predictiva del modelo y compararlo con otros modelos. \\(\\\\[0.4cm]\\) Decripción formal del algoritmo: El algoritmo de validación simple no aleatoria tiene los siguientes pasos: \\(\\\\[0.3cm]\\) Se divide \\(D\\) en parte de train y parte de test del siguiente modo: Sea \\(\\hspace{0.05cm}k\\in (0,1)\\hspace{0.05cm}\\) la proporción de filas de \\(D\\) que formaran parte del muestra de train : Las primeras \\(\\hspace{0.2cm}\\lfloor k \\cdot N \\rfloor\\hspace{0.2cm}\\) observaciones (filas) definen el conjunto de train: \\(\\\\[0.15cm]\\) \\[D_{train}= \\begin{pmatrix} x_{11}&amp;...&amp;x_{1p}&amp; y_1\\\\ x_{21}&amp;...&amp;x_{2p} &amp; y_2\\\\ ...&amp;...&amp;...&amp;... \\\\ x_{\\lfloor k \\cdot N \\rfloor1}&amp; ...&amp;x_{\\lfloor k \\cdot N \\rfloor p}&amp; y_{\\lfloor k \\cdot N \\rfloor} \\end{pmatrix}=\\begin{pmatrix} x_{1}^{train} &amp; y_{1}^{train}\\\\ x_{2}^{train} &amp; y_{2}^{train}\\\\ ....&amp;...\\\\ x_{\\# D_{train}}^{train} &amp; y_{\\# D_{train}}^{train}\\\\ \\end{pmatrix} \\\\\\] \\[D_{train}=\\left[\\hspace{0.01cm} X_1^{train} ,..., X_p^{train} , Y^{train}\\hspace{0.01cm}\\right] \\\\\\] Donde: \\(\\# D_{train}\\hspace{0.01cm}\\) es el número de filas de \\(\\hspace{0.01cm}D_{train}\\). Las siguientes \\(\\hspace{0.2cm} N - \\lfloor k \\cdot N \\rfloor\\hspace{0.2cm}\\) observaciones definen (filas) el conjunto de test: \\(\\\\[0.6cm]\\) \\[D_{test}= \\begin{pmatrix} x_{(\\lfloor k \\cdot N \\rfloor + 1) \\hspace{0.05cm} 1 } &amp; x_{(\\lfloor k \\cdot N \\rfloor + 1) \\hspace{0.05cm} 2}&amp;...&amp;x_{(\\lfloor k \\cdot N \\rfloor + 1) \\hspace{0.05cm} p}&amp; y_{\\lfloor k \\cdot N \\rfloor + 1} \\\\ x_{(\\lfloor k \\cdot N \\rfloor + 2) \\hspace{0.05cm} 1 } &amp; x_{(\\lfloor k \\cdot N \\rfloor + 2) \\hspace{0.05cm} 2}&amp;...&amp;x_{(\\lfloor k \\cdot N \\rfloor + 2) \\hspace{0.05cm} p}&amp; y_{\\lfloor k \\cdot N \\rfloor + 2}\\\\ &amp;...&amp;\\\\ x_{N \\hspace{0.05cm} 1 } &amp; x_{N 2}&amp;...&amp;x_{N p}&amp; y_{N} \\end{pmatrix}=\\begin{pmatrix} x_{1}^{test} &amp; y_{1}^{test}\\\\ x_{2}^{test} &amp; y_{2}^{test}\\\\ ....&amp;...\\\\ x_{\\# D_{test}}^{test} &amp; y_{\\# D_{test}}^{test}\\\\ \\end{pmatrix} \\hspace{0.1cm}=\\hspace{0.1cm} \\left[\\hspace{0.1cm} X_1^{train} ,..., X_p^{train} , Y^{train}\\hspace{0.1cm}\\right] \\\\\\] Donde: \\(\\# D_{test}\\hspace{0.1cm}\\) es el número de filas de \\(\\hspace{0.1cm}D_{test}\\) \\(\\\\[0.2cm]\\) \\(\\lfloor \\cdot \\rfloor\\hspace{0.1cm}\\) es la funcion suelo, que dado un número como argumento te devuelve el mayor entero menor que dicho número \\(\\\\[1cm]\\) Se entrena el modelo \\(\\hspace{0.1cm} M\\hspace{0.1cm}\\) con la muestra de train \\(\\hspace{0.1cm} D_{train}\\hspace{0.1cm}\\) \\(\\hspace{0.25cm}\\Rightarrow\\hspace{0.25cm}\\) \\(\\widehat{M}\\\\\\) Se calcula una métrica de evaluación sobre el modelo entrenado \\(\\hspace{0.1cm}\\widehat{M}\\hspace{0.1cm}\\) usando la muestra de test \\(\\hspace{0.1cm} D_{test}\\hspace{0.1cm}\\) \\(\\hspace{0.2cm}\\Rightarrow\\hspace{0.2cm}\\) Si por ejemplo se calcula el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\) , se obtendria el \\(\\hspace{0.1cm}ECM_{test}\\\\\\) La métrica de evaluación final del modelo es la obtenida en el paso anterior. Si la métrica empleada en el paso anterior es el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\) , entonces la métrica de evaluación calculada con el algoritmo de validación simple no aleatoria es la siguiente: \\[ECM(M)_{test}^* \\hspace{0.1cm}=\\hspace{0.1cm} ECM(\\widehat{M})_{test} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{\\# D_{test}} \\cdot \\sum_{i=1}^{\\# D_{test}} \\hspace{0.1cm} \\left( \\hspace{0.1cm} y_i^{test} - \\hat{y}_i^{test} \\hspace{0.1cm}\\right)^2\\] Donde: \\(\\hat{y}_i^{test} \\hspace{0.1cm}=\\hspace{0.1cm} \\widehat{M}(\\hspace{0.1cm} x_i^{test} \\hspace{0.1cm}|\\hspace{0.1cm} D_{train}) \\hspace{0.1cm}=\\hspace{0.1cm} M(\\hspace{0.1cm} x_i^{test} \\hspace{0.1cm}|\\hspace{0.1cm} X_1^{train},...,X_p^{train},Y^{train})\\hspace{0.1cm}=\\hspace{0.1cm} \\widehat{M}(\\hspace{0.1cm} x_i^{test} \\hspace{0.1cm})\\) Problemas Toda la validación queda condicionada a solo una muestra de train y otra de test. Si alguna de estas muestras tienen defectos, estos se van a trasladar a la validación, que será por tanto defectuosa. Es la primera aproximación naive a los algoritmos de validación. 5.2 Validación simple aleatoria Decripción no formal del algoritmo: Este algoritmo de validacion consiste en dividir el data-set inicial en una parte de train y otra de test de manera aleatoria. Se obtiene una muestra aleatoria sin remplazo de un \\(\\hspace{0.1cm} k\\% \\hspace{0.1cm}\\) de las filas del data-set inicial, las cuales serán la parte de train, y el resto la parte de test. El modelo es entrenado con la muestra train y testado calculando un métrica de evaluación con la muestra de test. Este valor de la métrica de evaluación es el que será usado para medir el poder predictivo del modelo y compararlo con otros modelos. Decripción formal del algoritmo: El algoritmo de validación simple aleatoria tiene los siguientes pasos: \\(\\\\[0.3cm]\\) Se divide \\(\\hspace{0.1cm}D\\hspace{0.1cm}\\) en parte de train y parte de test del siguiente modo: \\(\\\\[0.4cm]\\) \\(\\hspace{0.2cm}\\) Sea \\(\\hspace{0.1cm}k\\in (0,1)\\hspace{0.1cm}\\) la proporción de filas de \\(\\hspace{0.1cm}D\\hspace{0.1cm}\\) que formarán parte de la muestra de train (es un hiper-parametro del algoritmo): \\(\\\\[0.4cm]\\) Se genera una muestra aleatoria sin reemplazamiento de tamaño \\(\\hspace{0.1cm} \\lfloor k \\cdot N \\rfloor\\hspace{0.1cm}\\) del vector \\(\\hspace{0.1cm}(\\hspace{0.05cm}1,2,...,N\\hspace{0.05cm})\\) \\(\\\\[0.2cm]\\) \\[m=(m_1 ,m_2,...,m_{\\lfloor k \\cdot N \\rfloor}) \\\\\\] Las observaciones (filas) \\(\\hspace{0.1cm}m=(m_1,m_2 ,...,m_{\\lfloor k \\cdot N \\rfloor})\\hspace{0.1cm}\\) de \\(\\hspace{0.1cm}D\\hspace{0.1cm}\\) definen la muestra de train: \\(\\\\[0.5cm]\\) \\[D_{train}= D[m , :] = \\begin{pmatrix} x_{m_11}&amp;x_{m_12}&amp;...&amp;x_{m_1p}&amp; y_{m_1}\\\\ x_{m_21}&amp;x_{m_22}&amp;...&amp;x_{m_2p} &amp; y_{m_2}\\\\ &amp;...&amp;\\\\ x_{m_{\\lfloor k \\cdot N \\rfloor} 1}&amp;x_{m_{\\lfloor k \\cdot N \\rfloor} 2}&amp;...&amp;x_{m_{\\lfloor k \\cdot N \\rfloor} p}&amp; y_{m_{\\lfloor k \\cdot N \\rfloor}} \\end{pmatrix} = \\begin{pmatrix} x_{1}^{train} &amp; y_{1}^{train}\\\\ x_{2}^{train} &amp; y_{2}^{train}\\\\ ....&amp;...\\\\ x_{\\# D_{train}}^{train} &amp; y_{\\# D_{train}}^{train}\\\\ \\end{pmatrix} \\\\\\] Donde: \\(\\# D_{train}\\hspace{0.1cm}\\) es el número de filas de \\(\\hspace{0.1cm}D_{train} \\\\[1cm]\\) Las observaciones (filas) de \\(\\hspace{0.1cm}D\\hspace{0.1cm}\\) complementarias a \\(\\hspace{0.1cm}m\\hspace{0.1cm}\\) , es decir, las filas de \\(\\hspace{0.1cm}D\\hspace{0.1cm}\\) que no estan en \\(\\hspace{0.1cm}D_{train}\\hspace{0.1cm}\\), es decir, las filas de \\(\\hspace{0.1cm}m^c\\hspace{0.1cm}\\), definen la muestra de test: \\(\\\\[0.5cm]\\) \\[D_{test} = D[m^c , :] = \\begin{pmatrix} x_{1}^{test} &amp; y_{1}^{test}\\\\ x_{2}^{test} &amp; y_{2}^{test}\\\\ ....&amp;...\\\\ x_{\\# D_{test}}^{test} &amp; y_{\\# D_{test}}^{test}\\\\ \\end{pmatrix} \\\\\\] Donde: \\(\\# D_{test}\\hspace{0.1cm}\\) es el número de filas de \\(\\hspace{0.1cm}D_{test}\\\\\\) \\(\\lfloor \\cdot \\rfloor\\hspace{0.1cm}\\) es la funcion suelo, que dado un número como argumento te devuelve el mayor entero menor que dicho número \\(\\\\[0.5cm]\\) \\(m^c \\hspace{0.1cm}= \\hspace{0.1cm}\\Bigl(\\hspace{0.1cm} i =1,...,N \\hspace{0.13cm} : \\hspace{0.13cm} i\\neq m_j \\hspace{0.1cm},\\hspace{0.1cm} \\forall j=1,...,\\lfloor k \\cdot N \\rfloor \\hspace{0.1cm} \\Bigr) \\\\[1cm]\\) Se entrena el modelo \\(\\hspace{0.1cm}M\\hspace{0.1cm}\\) con la muestra de train \\(\\hspace{0.1cm}D_{train}\\hspace{0.1cm}\\) \\(\\hspace{0.1cm}\\Rightarrow\\hspace{0.2cm}\\) \\(\\widehat{M}\\\\\\) Se calcula una métrica de evaluación sobre el modelo entrenado \\(\\hspace{0.1cm}\\widehat{M}\\hspace{0.1cm}\\) con la muestra de test \\(\\hspace{0.1cm}D_{test}\\) \\(\\hspace{0.2cm}\\Rightarrow\\hspace{0.2cm}\\) Si por ejemplo se calcula el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\) , se obtendria el \\(\\hspace{0.1cm}ECM_{test}(\\widehat{M})\\\\\\) La métrica de evaluación final del modelo es la obtenida en el paso anterior: Si la métrica empleada en el paso anterior fue el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\) , entonces: \\[ECM(M)_{test}^* \\hspace{0.1cm}=\\hspace{0.1cm} ECM(\\widehat{M})_{test} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{\\# D_{test}} \\cdot \\sum_{i=1}^{\\# D_{test}} (y_i^{test} - \\hat{y}_i^{test})\\] Donde: \\(\\hat{y}_i^{test} \\hspace{0.1cm}=\\hspace{0.1cm} M(\\hspace{0.1cm} x_i^{test} \\hspace{0.1cm}|\\hspace{0.1cm} D_{train})\\hspace{0.1cm}=\\hspace{0.1cm} M(\\hspace{0.1cm} x_i^{test} \\hspace{0.1cm}|\\hspace{0.1cm} X_1^{train},...,X_p^{train},Y^{train})\\hspace{0.1cm}=\\hspace{0.1cm} \\widehat{M}(\\hspace{0.1cm} x_i^{test} \\hspace{0.1cm})\\) Problemas Toda la validación queda condicionada a solo una muestra de train y otra de test. Si alguna de estas muestras tienen defectos, estos se van a trasladar a la validación, que será por tanto defectuosa. Ademas la métrica de evaluacion calculada por validación simple aleatoria tiene generalmente una varianza alta, en comparación con otros métodos de validación. Esto será ilustrado en la práctica. La varianza de una metrica de evaluación calculada con un algorimo de validación se puede entender como como la varianza de los valores obtenidos de la métrica al ejecutar el algoritmo un número elevado de veces. Es la segunda aproximación naive a los algoritmos de validación. 5.3 Validación simple aleatoria repetida Decripción no formal del algoritmo: Este algoritmo de validacion consiste en dividir el data-set inicial en una parte de train y otra de test de manera aleatoria. Se obtiene una muestra aleatoria sin remplazo de un \\(\\hspace{0.1cm}k\\%\\hspace{0.1cm}\\) de las filas del data-set inicial, las cuales serán la parte de train, y el resto la parte de test. El modelo es enetrenado con la muestra train y testado con la muestra test a través de una métrica de evaluación como las vistas en la sección anterior. Este proceso se repite un número \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) de veces, asi se obtienen \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) valores de la métrica de evaluación. La métrica de evaluacion calculada usando este método de validación es la media de dichos \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) valores obtenidos para la métrica de evaluación escogida. Este valor medio final es la que será usado para medir el poder predictivo del modelo y compararlo con otros modelos. Decripción formal del algoritmo: El algoritmo de validación simple aleatoria tiene los siguientes pasos: \\(\\\\[0.4cm]\\) Se obtienen \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) particiones de la muestra de observaciones \\(\\hspace{0.1cm}D\\hspace{0.1cm}\\) en parte de train y parte de test del siguiente modo: \\(\\\\[0.4cm]\\) \\(\\hspace{0.2 cm}\\) Sea \\(\\hspace{0.1cm}k\\in (0,1)\\hspace{0.1cm}\\) la proporción de filas de \\(\\hspace{0.1cm}D\\hspace{0.1cm}\\) que formarán parte de la muestra de train : \\(\\\\[0.4cm]\\) Se generan \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) muestras aleatoria sin reemplazamiento de tamaño \\(\\hspace{0.1cm}\\lfloor k \\cdot N \\rfloor\\hspace{0.1cm}\\) del vector \\(\\hspace{0.1cm}(1,2,...,N)\\hspace{0.1cm}\\) : \\(\\\\[0.25cm]\\) \\[m_1 \\hspace{0.1cm},\\hspace{0.1cm} m_2 \\hspace{0.1cm},\\hspace{0.1cm} ...\\hspace{0.1cm},\\hspace{0.1cm} m_B \\\\[0.4cm]\\] Donde: \\(\\hspace{0.45cm}m_r=(m_{r1} ,...,m_{r\\lfloor k \\cdot N \\rfloor})\\hspace{0.15cm} \\hspace{0.25cm} , \\hspace{0.25cm} \\forall \\hspace{0.1cm} r\\in\\lbrace 1,...,B\\rbrace\\) \\(\\hspace{0.45cm}\\lfloor \\cdot \\rfloor\\hspace{0.1cm}\\) es la función suelo, que dado un número como argumento devuelve el mayor entero menor que dicho número. \\(\\\\[1cm]\\) Se obtienen las siguientes \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) muestras de train del data-set original \\(\\hspace{0.1cm}D \\\\\\) \\[D_{train, 1}= D[\\hspace{0.1cm}m_1\\hspace{0.1cm} ,\\hspace{0.1cm} :\\hspace{0.1cm}] \\hspace{0.15cm},\\hspace{0.15cm} D_{train, 2}= D[\\hspace{0.1cm}m_2\\hspace{0.1cm} ,\\hspace{0.1cm} :\\hspace{0.1cm}] \\hspace{0.15cm}, \\dots ,\\hspace{0.15cm} D_{train, B}= D[\\hspace{0.1cm}m_B\\hspace{0.1cm} ,\\hspace{0.1cm} :\\hspace{0.1cm}] \\\\\\] \\(\\hspace{0.5cm}\\) Donde: \\[D_{train, r} \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.1cm}m_r\\hspace{0.1cm} ,\\hspace{0.1cm} :\\hspace{0.1cm}] \\hspace{0.1cm}=\\hspace{0.1cm} \\begin{pmatrix} x_{m_{r1},1} &amp; x_{m_{r1},2} &amp; ... &amp; x_{m_{r1},p} &amp; y_{m_{r1}} \\\\ x_{m_{r1},1} &amp; x_{m_{r1},2} &amp; ... &amp; x_{m_{r1},p} &amp; y_{m_{r2}} \\\\ ....&amp;...\\\\ x_{m_{r\\lfloor k \\cdot N \\rfloor} ,1} &amp; x_{m_{r\\lfloor k \\cdot N \\rfloor},2} &amp; ... &amp; x_{m_{r\\lfloor k \\cdot N \\rfloor},p} &amp; y_{m_{r\\lfloor k \\cdot N \\rfloor}} \\end{pmatrix} = \\begin{pmatrix} x_{1}^{train, r} &amp; y_{1}^{train, r}\\\\ x_{2}^{train, r} &amp; y_{2}^{train, r}\\\\ ....&amp;...\\\\ x_{\\# D_{train, r}}^{train, r} &amp; y_{\\# D_{train, r}}^{train, r} \\end{pmatrix} \\\\\\] \\(\\hspace{1cm}\\) es la submatriz que resulta de quedarse solo con las filas de \\(\\hspace{0.1cm}D\\hspace{0.1cm}\\) definidas por la muestra \\(\\hspace{0.1cm}m_r\\hspace{0.1cm}\\) de \\(\\hspace{0.1cm}(1,...,N) \\\\[1cm]\\) Se obtienen las siguientes \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) muestras de test del data-set original \\(\\hspace{0.1cm}D \\\\\\) \\[D_{test, 1}= D[\\hspace{0.1cm}m_1^c\\hspace{0.1cm} ,\\hspace{0.1cm} :\\hspace{0.1cm}] \\hspace{0.15cm},\\hspace{0.15cm} D_{test, 2}= D[\\hspace{0.1cm}m_2^c\\hspace{0.1cm} ,\\hspace{0.1cm} :\\hspace{0.1cm}] \\hspace{0.15cm}, ... ,\\hspace{0.15cm} D_{test, B}= D[\\hspace{0.1cm}m_B^c\\hspace{0.1cm} , \\hspace{0.1cm}:\\hspace{0.1cm}] \\\\\\] \\(\\hspace{0.5cm}\\)Donde: \\[D_{test, r} \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.1cm}m_r^c\\hspace{0.1cm} ,\\hspace{0.1cm} :\\hspace{0.1cm}]\\hspace{0.1cm}=\\hspace{0.1cm} \\begin{pmatrix} x_{1}^{test, r} &amp; y_{1}^{test, r}\\\\ x_{2}^{test, r} &amp; y_{2}^{test, r}\\\\ ....&amp;...\\\\ x_{\\# D_{test, r}}^{test, r} &amp; y_{\\# D_{test, r}}^{test, r} \\end{pmatrix} \\\\\\] \\(\\hspace{1cm}\\) es la submatriz que resulta de quedarse solo con las filas de \\(\\hspace{0.1cm}D\\hspace{0.1cm}\\) que no están en \\(\\hspace{0.1cm}D_{train}\\hspace{0.1cm}\\), es decir, las filas de \\(\\hspace{0.1cm}m_r^c \\\\\\) Formalmente: \\(\\hspace{0.15cm} m_r^c \\hspace{0.1cm}=\\hspace{0.1cm}\\left(\\hspace{0.1cm} i \\in \\lbrace 1,...,N \\rbrace \\hspace{0.15cm} / \\hspace{0.15cm} i\\hspace{0.1cm}\\neq\\hspace{0.1cm} m_{rj} \\hspace{0.15cm} , \\hspace{0.15cm} \\forall \\hspace{0.1cm} j\\in \\lbrace 1,...,\\lfloor k \\cdot N \\rfloor \\rbrace \\hspace{0.1cm} \\right) \\hspace{0.15cm}\\) es el complementario de \\(\\hspace{0.1 cm}m_r \\\\[0.5cm]\\) En conclusión: \\(\\hspace{0.03cm}\\) se obtienen \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) particiones de train y test de \\(\\hspace{0.1cm}D \\\\[1cm]\\) Para cada \\(\\hspace{0.1cm}r\\in \\lbrace 1,...,B\\rbrace\\) \\(\\\\[0.15cm]\\) Se entrena el modelo \\(\\hspace{0.1cm}M\\hspace{0.1cm}\\) con cada una de las muestras de train \\(\\hspace{0.1cm} D_{train,r} \\hspace{0.1cm}\\) \\(\\hspace{0.2cm}\\Rightarrow\\hspace{0.2cm}\\) \\(\\hspace{0.1cm}\\widehat{M}_r \\\\\\) Se calcula una misma métrica de evaluación sobre el modelo entrenado \\(\\hspace{0.1cm}\\widehat{M}_r\\hspace{0.1cm}\\) con la muestra de test \\(\\hspace{0.1cm}D_{test,r} \\\\\\) Supongamos que la métrica de evaluación usada es el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\) , entonces se obtienen \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) valores de esta métrica : \\[ECM_{test }(\\widehat{M}_1) \\hspace{0.1cm},\\hspace{0.1cm} ECM_{test }(\\widehat{M}_2)\\hspace{0.1cm} , ... ,\\hspace{0.1cm} ECM_{test}(\\widehat{M}_B)\\\\\\] Donde: \\(\\hspace{0.5cm}\\) \\(ECM_{test , r} \\hspace{0.1cm}\\) es el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\) calculado sobre \\(\\hspace{0.1cm}\\widehat{M}_r\\hspace{0.1cm}\\) usando \\(\\hspace{0.1cm}D_{test,r}\\\\\\) \\[ECM_{test }(\\widehat{M}_r) \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{\\# D_{test,r}} \\cdot \\sum_{i=1}^{\\# D_{test,r}} \\hspace{0.1cm} (\\hspace{0.1cm} y_i^{\\hspace{0.1cm}test,r} - \\hat{\\hspace{0.1cm}y\\hspace{0.1cm}}_i^{\\hspace{0.1cm}test,r} \\hspace{0.1cm})^2 \\\\\\] Teniendo en cuenta que: \\(\\\\[0.3cm]\\) \\(\\hat{y}_i^{test,r} \\hspace{0.1cm}=\\hspace{0.1cm} M(\\hspace{0.1cm} x_i^{test, r} \\hspace{0.1cm}|\\hspace{0.1cm} D_{train,r}) \\hspace{0.1cm}=\\hspace{0.1cm} \\widehat{M}_r (\\hspace{0.1cm} x_i^{test, r} \\hspace{0.1cm} ) \\\\[1cm]\\) Se calcula la métrica final de evaluación del modelo como el promedio de las \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) métricas calculadas en el apartado anterior. Si la métrica usada en el apartado anterior es el ECM, entonces: \\(\\\\[0.4cm]\\) \\[ECM_{test}^{\\hspace{0.08cm}*}( {M}) = \\dfrac{1}{B} \\cdot \\sum_{r=1}^B ECM_{test}(\\widehat{M}_r)\\] Ventajas Permite reducir la varianza de la métrica de validación. En la validación simple aleatoria no repetida la métrica de validación obtenida usando validacion simple tiene mayor varianza, en el sentido de que si se implementa el algoritmo un número elevado de veces, la varianza de los valores obtenidos de la métrica es mayor si se aplica el mismo procedimiento con validación simple aleatoria repetida. Esto será ilustrado en la práctica. 5.4 Leave-one-out Descripción no formal del algoritmo: Este algoritmo de validación consiste en dividir el data-set inicial en una parte de train y otra de test de una forma tal que la primera observación forma parte del conjunto de test y el resto de la de train. Se entrena el modelo con la muestra de train y se calculan las predicciones de la respuesta para las observaciones de test de los predictores. Con las observaciones de test de la respuesta y las predicciones de esta misma se calcula una métrica de validación. Se repite el proceso anterior, pero tomando la segunda observación como muestra de test y las restantes como muestra de train. Se vuelve a repetir con la tercera observación, luego con la cuarta, y asi sucesivamente hasta llegar al punto en el que la última observación es la muestra de test. Tras este proceso se habrán obtenido \\(\\hspace{0.05cm} N \\hspace{0.05cm}\\) valores de la métrica de validación. El valor final de la métrica por el algoritmo de validación leave-one-out es la media de esos \\(\\hspace{0.05cm} N \\hspace{0.05cm}\\) valores. \\(\\\\[0.05cm]\\) Decripción formal del algoritmo: El algoritmo de validación simple no aleatoria tiene los siguientes pasos: \\(\\\\[0.3cm]\\) Se obtienen \\(\\hspace{0.1cm} B\\hspace{0.1cm}\\) particiones de \\(\\hspace{0.1cm} D\\hspace{0.1cm}\\) en parte de train y parte de test del siguiente modo: \\(\\\\[0.5cm]\\) Se obtienen las siguientes \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) muestras test del data-set original \\(\\hspace{0.1cm}D \\\\\\) \\[D_{test,1}=D[1, :] \\hspace{0.1cm} ,\\hspace{0.15cm} D_{test,2}=D[2, :]\\hspace{0.15cm},...,\\hspace{0.15cm} D_{test,B}=D[B, :] \\\\\\] Donde : \\(\\hspace{0.5cm} D_{test,r}=D[\\hspace{0.1cm}r\\hspace{0.1cm}, \\hspace{0.1cm}:\\hspace{0.1cm}]\\hspace{0.1cm}\\) es la submatriz que resulta de considerar solo la fila \\(r\\) de \\(D\\) , es decir, es la observación \\(r\\)-esima del data-set inicial \\(\\hspace{0.1cm}D \\\\\\) \\[D_{test,r} \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.1cm}r\\hspace{0.1cm},\\hspace{0.1cm} :\\hspace{0.1cm}] \\hspace{0.1cm}=\\hspace{0.1cm} (x_{i1} , ..., x_{rp} , y_r)=(x_r \\hspace{0.1cm} ,\\hspace{0.1cm} y_r) \\\\[1cm]\\] Se obtienen las siguientes \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) muestras train del data-set original \\(\\hspace{0.1cm}D \\\\\\) \\[D_{train,1}=D[\\hspace{0.1cm}-1 \\hspace{0.1cm},\\hspace{0.1cm} :\\hspace{0.1cm}] \\hspace{0.2cm},\\hspace{0.2cm} D_{train,2}=D[\\hspace{0.1cm}-2\\hspace{0.1cm}, \\hspace{0.1cm}:\\hspace{0.1cm}]\\hspace{0.2cm},...,\\hspace{0.2cm} D_{train,B}=D[\\hspace{0.1cm}-B\\hspace{0.1cm},\\hspace{0.1cm} :\\hspace{0.1cm}] \\\\\\] Donde: \\(\\hspace{0.5cm}D_{train,r}\\hspace{0.1cm}=\\hspace{0.1cm}D[\\hspace{0.1cm}-r\\hspace{0.1cm},\\hspace{0.1cm} :\\hspace{0.1cm}]\\hspace{0.15cm}\\) es la submatriz que resulta de eliminar la fila \\(i\\) de \\(\\hspace{0.1cm} D\\hspace{0.1cm}\\), es decir: \\(\\\\[1cm]\\) \\[D_{train,r}\\hspace{0.1cm}=\\hspace{0.1cm}D[\\hspace{0.1cm}-r\\hspace{0.1cm},\\hspace{0.1cm} :\\hspace{0.1cm}] \\hspace{0.1cm}= \\hspace{0.1cm}\\begin{pmatrix} x_{11}&amp;x_{12}&amp;...&amp;x_{1p}&amp; y_1\\\\ ...&amp;...&amp;...&amp;...&amp;...\\\\ x_{(r-1)1}&amp;x_{(r-1)2}&amp;...&amp;x_{(r-1)p} &amp; y_{(r-1)}\\\\ x_{(r+1)1}&amp;x_{(r+1)2}&amp;...&amp;x_{(r+1)p} &amp; y_{(r+1)}\\\\ &amp;...&amp;\\\\ x_{N1}&amp;x_{N2}&amp;...&amp;x_{Np}&amp; y_N \\end{pmatrix}=\\begin{pmatrix} x_{1}&amp; y_1\\\\ ...&amp;...\\\\ x_{(r-1)}&amp;y_{(r-1)}\\\\ x_{(r+1)}&amp; y_{(r+1)}\\\\ &amp;...&amp;\\\\ x_{N}&amp; y_N \\end{pmatrix}\\] \\(\\\\[0.7cm]\\) Para \\(\\hspace{0.1cm}r\\in \\lbrace 1,...,B \\rbrace \\hspace{0.1cm} \\\\\\) Se entrena el modelo \\(\\hspace{0.1cm}M\\hspace{0.1cm}\\) con la muestra de train \\(\\hspace{0.1cm} D_{train,r}\\hspace{0.1cm}\\) \\(\\hspace{0.1cm}\\Rightarrow\\hspace{0.1cm}\\) \\(\\hspace{0.1cm}\\widehat{M}_r \\\\\\) Se calcula una misma métrica de evaluación sobre el modelo entrenado \\(\\hspace{0.1cm}\\widehat{M}_r\\hspace{0.1cm}\\) con la muestra de test \\(\\hspace{0.1cm}D_{test,r}\\hspace{0.1cm} \\\\\\) Supongamos que la métrica de evaluación usada es el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\) , entonces se obtienen \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) valores de esta métrica : \\[ECM(\\widehat{M}_1)_{test } \\hspace{0.1cm} ,\\hspace{0.1cm} ECM(\\widehat{M}_2)_{test } \\hspace{0.1cm} , ... ,\\hspace{0.1cm} ECM(\\widehat{M}_B)_{test}\\\\\\] Donde: \\(\\hspace{0.5cm} ECM_{test , r}\\hspace{0.1cm}\\) es el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\) calculado sobre \\(\\hspace{0.1cm}\\widehat{M}_r\\hspace{0.1cm}\\) usando \\(\\hspace{0.1cm}D_{test,r} \\\\\\) \\[ECM(\\widehat{M}_r)_{test } = (\\hspace{0.1cm} y_r - \\hat{y}_r \\hspace{0.1cm})^2\\] Teniendo en cuenta que: \\(\\hat{y}_r \\hspace{0.1cm}=\\hspace{0.1cm} M(\\hspace{0.1cm} x_r \\hspace{0.1cm}|\\hspace{0.1cm} D_{train,r}) \\hspace{0.1cm}=\\hspace{0.1cm} \\widehat{M}_r (\\hspace{0.1cm} x_r \\hspace{0.1cm} ) \\\\\\) \\(y_r\\hspace{0.1cm}\\) es la única observación de la muestra de test \\(\\hspace{0.1cm} r\\)-esima de la variable respuesta. \\(\\\\[1cm]\\) Se calcula la métrica final de evaluación del modelo como el promedio de las \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) métricas calculadas en el paso anterior. Si la métrica usada es el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\), entonces: \\[ECM( M )_{test}^{\\hspace{0.08cm}*} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{B} \\cdot \\sum_{r=1}^B \\hspace{0.1cm} ECM(\\widehat{M}_r)_{test}\\] Ventaja Una vez aplicado el algoritmo todas las observaciones han formado parte de conjunto de train (en alguna iteracion), y lo mismo para el conjunto de test. Problema Algunos autores (vease la referencia 1) consideran que, al emplearse todas las observaciones como entrenamiento, se puede estar cayendo en overfitting (sobre-ajuste). 5.5 k-fold Decripción no formal del algoritmo: Este algoritmo de validación consiste en dividir el data-set inicial en \\(\\hspace{0.1cm} k\\hspace{0.1cm}\\) partes, y usar de manera secuencial cada una de esas partes como muestra test, y las unión de las partes restantes como muestra train. Por tanto con este método se usan \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) muestras de test y de train. El modelo es entrenado secuencialmente con cada una de las \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) muestras de train disitntas, y se testea con la correspondiente muestra de test (que es el complementario de la de train), usando una métrica de evaluación. Es decir, tras dividir el data-set inicial en \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) partes, la parte 1 se usa como test y el resto como train, se calcula la métrica de evaluación. Luego la parte 2 se usa como test y el resto como train, y se calcula la métrica de evaluación. Asi sucesivamente hasta haber usado las \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) partes como muestras de test. Tras este proceso se habrán obtenido \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) valores de dicha métrica de evaluacion. La métrica de evaluacion calculada usando este método de validación es la media de dichos \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) valores obtenidos para la métrica de evaluación escogida. Este valor medio final es la que será usado para medir el poder predictivo del modelo y compararlo con otros modelos. Decripción formal del algoritmo: El algoritmo de validación k-folds tiene los siguientes pasos: Se divide aleatoriamente el data-set inicial \\(\\hspace{0.1cm}D\\hspace{0.1cm}\\) en \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) partes de manera que cada parte tenga aproximadamente el mismo número de observaciones (sean lo mas balanceadas posibles). Existen diferentes métodos para hacer esta división. La problematica de la división es cómo hacer que las partes resultantes estén lo más balanceadas posibles respecto al numero de observaciones que contienen. \\(\\\\[0.7cm]\\) Hemos desarrollado un método basado en cuantiles que permite obtener este balanceo, el cual ha sido implementado en Python con buenos resultados en este aspecto, como se podrá ver posteriormente en la parte de implementación. \\(\\\\[0.7cm]\\) Vamos a explicar la mecánica del método ideado: \\(\\\\[0.2cm]\\) Obtenemos una muestra aleatoria sin remplazamiento \\(\\hspace{0.1cm}m=(m_1,...,m_N)\\hspace{0.1cm}\\) de tamaño \\(N\\) del vector \\(\\hspace{0.1cm}(1,...,N) \\\\\\) El siguiente paso es dividir la muestra \\(\\hspace{0.1cm}m\\hspace{0.1cm}\\) en \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) partes lo mas balanceadas posibles. No queremos que unas partes tenga muchos elementos, y otras pocos. Queremos que la repartición de los elementos de \\(\\hspace{0.1cm}m\\hspace{0.1cm}\\) en las \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) partes sea lo mas igualitaria posible. \\(\\\\[0.5cm]\\) La idea es que si, por ejemplo \\(\\hspace{0.1cm}k=10\\hspace{0.1cm}\\), cada una de las 10 partes en las que dividimos \\(\\hspace{0.1cm}m\\hspace{0.1cm}\\) tenga un 10% de los elementos totales de \\(m \\\\\\) Si \\(\\hspace{0.1cm}k=4\\hspace{0.1cm}\\) se busca que cada una de las 4 partes en las que dividimos \\(\\hspace{0.1cm}m\\hspace{0.1cm}\\) tenga el 25% de los elementos de \\(\\hspace{0.1cm}m . \\\\\\) En general, se busca que cada una de las \\(\\hspace{0.1cm} k\\hspace{0.1cm}\\) partes en las que dividimos \\(\\hspace{0.1cm}m\\hspace{0.1cm}\\) tengan \\(\\hspace{0.1cm}(1/k)\\cdot 100 \\%\\hspace{0.1cm}\\) de elementos de \\(\\hspace{0.1cm}m\\hspace{0.1cm}\\), es decir, \\(\\hspace{0.1cm} N/k\\hspace{0.1cm}\\) elementos de \\(\\hspace{0.1cm}m\\hspace{0.1cm}\\) , puesto que m tiene N elementos. \\(\\\\[0.5cm]\\) Una forma de hacer esto es usando los cuantiles \\(\\hspace{0.1cm} Q_0 \\hspace{0.1cm} , \\hspace{0.1cm} Q_{1/k} \\hspace{0.1cm} ,\\hspace{0.1cm} Q_{2/k} \\hspace{0.1cm} ,...,\\hspace{0.1cm} Q_{(k-1)/k}\\hspace{0.1cm} ,\\hspace{0.1cm} Q_1\\hspace{0.1cm}\\) del vector \\(\\hspace{0.1cm}(1,...,N)\\hspace{0.1cm}\\) como los limites que definen las partes en las que dividiremos \\(\\hspace{0.1cm} m=(m_1,...,m_N) \\\\\\) Dichos cuantiles permiten separar \\(\\hspace{0.1cm} m \\hspace{0.1cm}\\) en \\(\\hspace{0.1cm} k \\hspace{0.1cm}\\) partes de un tamaño aproximadamente igual. \\(\\\\[0.5cm]\\) Si \\(\\hspace{0.1cm} k=10\\hspace{0.1cm}\\), entonces esos cuantiles serian \\(\\hspace{0.1cm} Q_0 \\hspace{0.1cm},\\hspace{0.1cm} Q_{0.1} \\hspace{0.1cm},\\hspace{0.1cm} Q_{0.2} \\hspace{0.1cm}, ...,\\hspace{0.1cm} Q_{0.8} \\hspace{0.1cm},\\hspace{0.1cm} Q_{0.9} \\hspace{0.1cm},\\hspace{0.1cm} Q_1 \\\\\\) Si \\(\\hspace{0.1cm} k=4\\hspace{0.1cm}\\) , los cuantiles serían \\(\\hspace{0.1cm} Q_0 \\hspace{0.1cm},\\hspace{0.1cm} Q_{0.25} \\hspace{0.1cm},\\hspace{0.1cm} Q_{0.5} \\hspace{0.1cm},\\hspace{0.1cm} Q_{0.75} \\hspace{0.1cm},\\hspace{0.1cm} Q_1\\hspace{0.1cm} \\\\\\) Notese que: \\(\\hspace{0.2cm} Q_0 = Min(1,...,N) = 1\\hspace{0.2cm}\\) y \\(\\hspace{0.2cm} Q_1=Max(1,...,N)=N \\\\[0.6cm]\\) Definimos las \\(\\hspace{0.1cm} k\\hspace{0.1cm}\\) particiones de \\(\\hspace{0.1cm} m \\hspace{0.1cm}\\) usando los cuantiles \\(\\hspace{0.15cm} Q_0=1 \\hspace{0.1cm},\\hspace{0.1cm} Q_{1/k} \\hspace{0.1cm},\\hspace{0.1cm} Q_{2/k}\\hspace{0.1cm},...,\\hspace{0.1cm}Q_{(k-1)/k}\\hspace{0.1cm},\\hspace{0.1cm} Q_1=N\\hspace{0.2cm}\\) como sigue: \\(\\\\[0.8cm]\\) \\(\\hspace{2cm} p_{1,m} \\hspace{0.1cm}=\\hspace{0.1cm} m\\left[\\hspace{0.1cm}1:(\\lfloor Q_{1/k} \\rfloor -1)\\hspace{0.1cm}\\right]\\hspace{0.1cm}=\\hspace{0.1cm}(m_1,...,m_{\\lfloor Q_{1/k} \\rfloor - 1} ) \\\\\\) \\(\\hspace{2cm} p_{2,m} \\hspace{0.1cm}=\\hspace{0.1cm} m\\left[\\hspace{0.1cm}\\lfloor Q_{1/k} \\rfloor:(\\lfloor Q_{2/k} \\rfloor-1)\\hspace{0.1cm}\\right]\\hspace{0.1cm}=\\hspace{0.1cm}(m_{\\lfloor Q_{1/k} \\rfloor},...,m_{\\lfloor Q_{2/k} \\rfloor - 1})\\) \\(\\hspace{2.4cm} \\dots \\\\\\) \\(\\hspace{2cm} p_{k,m} \\hspace{0.1cm}=\\hspace{0.1cm} m\\left[\\hspace{0.1cm}\\lfloor Q_{(k-1)/k} \\rfloor : N\\hspace{0.1cm}\\right]\\hspace{0.1cm}=\\hspace{0.1cm}(m_{\\lfloor Q_{(k-1)/k} \\rfloor},...,m_{N})\\\\[1cm]\\) Se puede demostrar que \\(\\hspace{0.1cm} p_{1,m}\\hspace{0.1cm},...,\\hspace{0.1cm} p_{k,m}\\hspace{0.1cm}\\) tienen un nº de elementos aproximadamente igual , por lo que son particiones aproximadamente igualitarias (balanceadas), que era lo que buscabamos. \\(\\\\[0.7cm]\\) La siguiente matriz ilustra por qué este método funciona: \\(\\\\[0.7cm]\\) \\[\\begin{pmatrix} 1 &amp; m_1\\\\ 2 &amp; m_2\\\\ ... &amp; ... \\\\ \\lfloor Q_{1/k} \\rfloor - 1 &amp; m_{\\lfloor Q_{1/k} \\rfloor - 1} \\\\ ----- &amp; ----- \\\\ \\lfloor Q_{1/k} \\rfloor &amp; m_{\\lfloor Q_{1/k} \\rfloor} \\\\ ... &amp; ... \\\\ \\lfloor Q_{2/k} \\rfloor - 1 &amp; m_{\\lfloor Q_{2/k} \\rfloor - 1} \\\\ ----- &amp; -----\\\\ \\lfloor Q_{2/k} \\rfloor &amp; m_{\\lfloor Q_{2/k} \\rfloor} \\\\ ... &amp; ... \\\\ \\lfloor Q_{3/k} \\rfloor - 1 &amp; m_{\\lfloor Q_{3/k} \\rfloor - 1} \\\\ ----- &amp; -----\\\\ ... &amp; ... \\\\ ... &amp; ... \\\\ ----- &amp; -----\\\\ \\lfloor Q_{(k-1)/k} \\rfloor &amp; m_{\\lfloor Q_{(k-1)/k} \\rfloor} \\\\ ... &amp; ... \\\\ N &amp; m_N \\end{pmatrix}\\hspace{0.1cm} = \\hspace{0.1cm} \\begin{pmatrix} ... &amp; ...\\\\ ... &amp; ...\\\\ \\text{Parte 1} \\hspace{0.15cm}(p_{1,m}) &amp; \\hspace{0.2cm} \\approx N/k \\hspace{0.15cm} \\text{elementos} \\\\ ... &amp; ...\\\\ ----- &amp; -----\\\\ ... &amp; ...\\\\ \\text{Parte 2}\\hspace{0.15cm}(p_{2,m}) &amp; \\hspace{0.2cm} \\approx N/k \\hspace{0.15cm} \\text{elementos} \\\\ ... &amp; ...\\\\ ----- &amp; -----\\\\ ... &amp; ...\\\\ \\text{Parte 3}\\hspace{0.15cm}(p_{3,m}) &amp; \\hspace{0.2cm} \\approx N/k \\hspace{0.15cm} \\text{elementos} \\\\ ... &amp; ...\\\\ ----- &amp; -----\\\\ ... &amp; ...\\\\ ... &amp; ...\\\\ ----- &amp; -----\\\\ ... &amp; ...\\\\ \\text{Parte k}\\hspace{0.15cm}(p_{k,m}) &amp; \\hspace{0.2cm} \\approx N/k \\hspace{0.15cm} \\text{elementos} \\\\ ... &amp; ...\\\\ \\end{pmatrix}\\] \\(\\\\[0.3cm]\\) Se obtienen la siguientes \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) muestras de test: \\(\\\\[0.4cm]\\) \\[D_{test, 1} \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.1cm} p_{1,m} \\hspace{0.1cm} ,\\hspace{0.1cm} : \\hspace{0.1cm}] \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.12cm}m[1:(\\lfloor Q_{1/k} \\rfloor -1)] \\hspace{0.12cm},\\hspace{0.12cm} : \\hspace{0.12cm}]\\] \\[D_{test, 2} \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.1cm}p_{2,m} \\hspace{0.1cm} ,\\hspace{0.1cm} :\\hspace{0.1cm}] \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.12cm}m[\\lfloor Q_{1/k} \\rfloor : (\\lfloor Q_{2/k} \\rfloor - 1 )]\\hspace{0.12cm} ,\\hspace{0.12cm}:\\hspace{0.12cm}]\\] \\[\\dots\\] \\[D_{test, k}\\hspace{0.1cm} =\\hspace{0.1cm} D[\\hspace{0.1cm}p_{k,m} \\hspace{0.12cm} ,\\hspace{0.12cm} :\\hspace{0.12cm}] \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.12cm}m[\\lfloor Q_{(k-1)/k} \\rfloor : N]\\hspace{0.12cm} ,\\hspace{0.12cm} :\\hspace{0.12cm}] \\\\[0.7cm]\\] \\(\\hspace{0.5cm}\\) Siguiendo la notación habitual del articulo, podemos expresar: \\(\\\\[0.6cm]\\) \\[ D_{test, r} \\hspace{0.1cm}=\\hspace{0.1cm}\\begin{pmatrix} x_{1}^{test, r} &amp; y_{1}^{test, r}\\\\ x_{2}^{test, r} &amp; y_{2}^{test, r}\\\\ ....&amp;...\\\\ x_{\\# D_{test, r}}^{test, r} &amp; y_{\\# D_{test, r}}^{test, r} \\end{pmatrix} \\] \\(\\\\[0.3cm]\\) Se obtiene las siguientes \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) muestras de train: \\(\\\\[0.4cm]\\) \\[D_{train, 1} \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.12cm}-\\hspace{0.12cm}p_{1,m} \\hspace{0.12cm},\\hspace{0.12cm} :\\hspace{0.12cm}] \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.12cm}-\\hspace{0.12cm} m[1:(\\lfloor Q_{1/k} \\rfloor -1)] \\hspace{0.12cm},\\hspace{0.12cm}:\\hspace{0.12cm}]\\] \\[D_{train, 2} \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.12cm}-\\hspace{0.12cm} p_{2,m} \\hspace{0.12cm},\\hspace{0.12cm} :\\hspace{0.12cm}] \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.12cm}-\\hspace{0.12cm}m[\\lfloor Q_{1/k} \\rfloor : (\\lfloor Q_{2/k} \\rfloor - 1 )] \\hspace{0.12cm},\\hspace{0.12cm}:\\hspace{0.12cm}]\\] \\[\\dots\\] \\[D_{train, k} \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.12cm}-\\hspace{0.12cm} p_{k,m} \\hspace{0.12cm},\\hspace{0.12cm} : \\hspace{0.12cm}] \\hspace{0.1cm}=\\hspace{0.1cm} D[\\hspace{0.12cm}-\\hspace{0.12cm} m[\\lfloor Q_{(k-1)/k} \\rfloor : N] \\hspace{0.12cm},\\hspace{0.12cm} : \\hspace{0.12cm}] \\\\\\] \\(\\hspace{0.5cm}\\) Siguiendo la notación habitual del articulo, podemos expresar: \\(\\\\[0.6cm]\\) \\[\\hspace{0.5cm}D_{train, r} = \\begin{pmatrix} x_{1}^{train, r} &amp; y_{1}^{train, r}\\\\ x_{2}^{train, r} &amp; y_{2}^{train, r}\\\\ ....&amp;...\\\\ x_{\\# D_{train, r}}^{train, r} &amp; y_{\\# D_{train, r}}^{train, r} \\end{pmatrix}\\] \\(\\\\[0.2cm]\\) Para \\(\\hspace{0.1cm}r \\in \\lbrace 1,...,k \\rbrace\\hspace{0.1cm}\\) Se entrena el modelo \\(\\hspace{0.1cm}M\\hspace{0.1cm}\\) con la muestra de train \\(\\hspace{0.1cm} D_{train,r}\\) \\(\\hspace{0.2cm}\\Rightarrow\\hspace{0.2cm}\\) \\(\\hspace{0.1cm}\\widehat{M}_r\\) \\(\\\\[0.25cm]\\) Se calcula una misma métrica de evaluación sobre el modelo entrenado \\(\\hspace{0.1cm}\\widehat{M}_r\\hspace{0.1cm}\\) con la muestra de test \\(\\hspace{0.1cm}D_{test,r}\\hspace{0.1cm}\\) \\(\\\\[0.25cm]\\) Supongamos que la métrica de evaluación usada es el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\) , entonces se obtienen \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) valores de esta métrica : \\[ECM_{test }\\left(\\widehat{M}_1\\right) \\hspace{0.1cm},\\hspace{0.1cm} ECM_{test }\\left(\\widehat{M}_2\\right) \\hspace{0.1cm}, ... ,\\hspace{0.1cm} ECM_{test}\\left( \\widehat{M}_k \\right)\\\\\\] Donde: \\(\\hspace{0.5cm} ECM_{test , r}\\hspace{0.1cm}\\) es el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\) calculado sobre \\(\\hspace{0.1cm}\\widehat{M}_r\\hspace{0.1cm}\\) usando \\(\\hspace{0.1cm}D_{test,r}\\hspace{0.1cm} \\\\\\) \\[ECM_{test }\\left( \\hspace{0.05cm} \\widehat{M}_r \\hspace{0.05cm}\\right) \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{\\# D_{test, r}} \\cdot \\sum_{i=1}^{\\# D_{test, r}} \\hspace{0.1cm} \\left( \\hspace{0.1cm} y_i^{\\hspace{0.1cm}test,r} - \\hat{\\hspace{0.1cm}y\\hspace{0.1cm}}_i^{\\hspace{0.1cm}test,r} \\hspace{0.1cm} \\right)^2 \\\\\\] Teniendo en cuenta que : \\(\\hat{\\hspace{0.1cm}y\\hspace{0.1cm}}_i^{\\hspace{0.1cm}test,r} \\hspace{0.1cm}=\\hspace{0.1cm} M\\left(\\hspace{0.1cm} x_i^{test, r} \\hspace{0.1cm}|\\hspace{0.1cm} D_{train,r} \\right) \\hspace{0.1cm}=\\hspace{0.1cm} \\widehat{M}_r \\left(\\hspace{0.1cm} x_i^{test, r} \\hspace{0.1cm} \\right) \\\\[0.7cm]\\) \\(x_i^{\\hspace{0.1cm}test,r}\\hspace{0.1cm}\\) es la observación \\(\\hspace{0.1cm}i\\)-esima de la muestra de test \\(\\hspace{0.1cm}r\\)-esima de los predictores. \\(\\\\[0.7cm]\\) \\(y_i^{\\hspace{0.1cm}test,r}\\hspace{0.1cm}\\) es la observación \\(\\hspace{0.1cm}i\\)-esima de la muestra de test \\(\\hspace{0.1cm}r\\)-esima de la variable respuesta. \\(\\\\[1cm]\\) Se calcula la métrica final de evaluación del modelo como el promedio de las \\(\\hspace{0.1cm}k\\hspace{0.1cm}\\) métricas calculadas en el paso anterior. Si la métrica usada fuera el ECM, entonces: \\[ECM( {M})_{test}^{\\hspace{0.08cm}*} \\hspace{0.1cm}=\\hspace{0.1cm} \\dfrac{1}{k} \\cdot \\sum_{r=1}^k ECM_{test}(\\widehat{M}_r)\\] Ventajas La metrica de validacion calculada por k-fold tiene menor varianza que con los métodos anteriores, luego es el mas preciso de todos ellos. 5.6 Repeted k-fold Este algoritmo consiste en repetir el algoritmo k-fold un número \\(\\hspace{0.1cm} B \\hspace{0.1cm}\\) de veces. No vamos a hacer aquí una descripción tan detallada del algoritmo como las anteriores, puesto que buena parte es repetir \\(\\hspace{0.1cm}B\\hspace{0.1cm}\\) veces la estructura del k-fold. \\(\\\\[0.5cm]\\) Sintetizando, los pasos del algoritmo \\(\\hspace{0.08cm}B\\)-repeated \\(\\hspace{0.08cm}k\\)-fold son los siguientes: Se itera el algoritmo \\(\\hspace{0.08cm}k\\)-fold un total de \\(\\hspace{0.08cm}B\\hspace{0.08cm}\\) veces. Con ello se obtienen \\(\\hspace{0.08cm}k\\cdot B\\hspace{0.08cm}\\) valores de la métrica de validacion, ya que cada iteracion del algoritmo k-fold produce \\(\\hspace{0.08cm}k\\hspace{0.08cm}\\) valores de la métrica, y el algoritmo se itera \\(\\hspace{0.08cm}B\\hspace{0.08cm}\\) veces. \\(\\\\[0.2cm]\\) Si la métrica usada para evaluar el modelo fuera el \\(\\hspace{0.1cm}ECM\\hspace{0.05cm}\\) , entocnes se obtendrian los siguientes \\(\\hspace{0.1cm}k\\cdot B\\hspace{0.1cm}\\) valores de esta métrica: \\[ECM_{test }\\left(\\hspace{0.1cm}\\widehat{M}_1^{\\hspace{0.1cm}1}\\hspace{0.1cm}\\right) \\hspace{0.05cm}, ... ,\\hspace{0.05cm} ECM_{test}\\left(\\hspace{0.1cm}\\widehat{M}_k^{\\hspace{0.1cm}1}\\hspace{0.1cm}\\right) \\hspace{0.05cm},...,\\hspace{0.05cm}ECM_{test }\\left(\\hspace{0.1cm}\\widehat{M}_1^{\\hspace{0.1cm}B}\\hspace{0.1cm}\\right) \\hspace{0.05cm}, ... ,\\hspace{0.05cm} ECM_{test}\\left(\\hspace{0.1cm}\\widehat{M}_k^{\\hspace{0.1cm}B} \\hspace{0.1cm} \\hspace{0.1cm}\\right) \\\\\\] Donde: Para \\(\\hspace{0.1cm}r\\in \\lbrace 1,...,k \\rbrace\\hspace{0.15cm}\\) y \\(\\hspace{0.15cm} j\\in \\lbrace 1,...,B \\rbrace\\) \\(\\\\[0.45cm]\\) \\(\\widehat{M}_r^{\\hspace{0.1cm}j}\\hspace{0.1cm}\\) es el modelo \\(\\hspace{0.1cm}M\\hspace{0.1cm}\\) entrenado con la muestra de train \\(\\hspace{0.1cm}r\\)-esima obtenida en la iteración \\(\\hspace{0.1cm}j\\)-esima del algoritmo k-fold, es decir, es el modelo entrenado con la muestra de train \\(\\hspace{0.1cm}D_{train, r}^{\\hspace{0.1cm}j}\\hspace{0.1cm} \\\\[0.5cm]\\) \\(ECM_{test }\\left(\\hspace{0.1cm}\\widehat{M}_r^{\\hspace{0.1cm}j} \\hspace{0.1cm}\\right)\\hspace{0.1cm}\\) es el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\) calculado sobre el modelo \\(\\hspace{0.1cm}\\widehat{M}_r^{\\hspace{0.1cm}j}\\hspace{0.1cm}\\) con la muestra de test \\(\\hspace{0.1cm}r\\)-esima obtenida en la repetición \\(\\hspace{0.1cm}j\\)-esima del algoritmo k-fold, es decir, con la muestra de test \\(\\hspace{0.1cm}D_{test, r}^{\\hspace{0.1cm}j}\\) \\(\\\\[0.5cm]\\) \\[ECM_{test }\\left( \\hspace{0.1cm} \\widehat{M}_r^{\\hspace{0.1cm}j} \\hspace{0.1cm}\\right) = \\dfrac{1}{\\# D_{test, r}^{\\hspace{0.1cm}j}} \\cdot \\sum_{i=1}^{\\# D_{test, r}^{\\hspace{0.1cm}j}} \\hspace{0.1cm} \\left(\\hspace{0.1cm} y_i^{\\hspace{0.1cm}test,r,j} - \\hat{\\hspace{0.1cm}y\\hspace{0.1cm}}_i^{\\hspace{0.1cm}test,r,j} \\hspace{0.1cm} \\right)^2 \\\\[1cm]\\] Considerando lo siguiente : \\(\\\\[0.5cm]\\) \\(\\hat{y}_i^{\\hspace{0.1cm}test,r,j} \\hspace{0.1cm}=\\hspace{0.1cm} M(\\hspace{0.1cm} x_i^{\\hspace{0.1cm}test, r,j} \\hspace{0.12cm}|\\hspace{0.12cm} D_{train,r}^{\\hspace{0.1cm}j}) \\hspace{0.1cm}=\\hspace{0.1cm} \\widehat{M}_r^{\\hspace{0.1cm}j} (\\hspace{0.1cm} x_i^{test, r,j} \\hspace{0.1cm} ) \\\\[0.7cm]\\) \\(\\left( \\hspace{0.1cm} x_i^{\\hspace{0.1cm} test, r,j} , y_i^{test, r,j} \\hspace{0.1cm}\\right)\\hspace{0.1cm}\\) es la observación (fila) \\(\\hspace{0.1cm}i\\)-esima de \\(\\hspace{0.1cm}D_{test,r}^{\\hspace{0.1cm} j} \\\\[0.7cm]\\) \\(x_i^{\\hspace{0.1cm}test, r,j}\\hspace{0.1cm}\\) es la observación \\(\\hspace{0.1cm}i\\)-esima de la muestra de test \\(\\hspace{0.1cm}r\\)-esima de los predictores obtenida en la repetición \\(\\hspace{0.1cm}j\\)-esima del algoritmo k-folds. \\(\\\\[0.7cm]\\) \\(y_i^{\\hspace{0.1cm}test, r,j}\\hspace{0.1cm}\\) es la observacion \\(\\hspace{0.1cm}i\\)-esima de la muestra de test \\(\\hspace{0.1cm}r\\)-esima de la variable respuesta obtenida en la repetición \\(\\hspace{0.1cm}j\\)-esima del algoritmo k-folds. \\(\\\\[0.7cm]\\) Nótese que debido al componente aleatorio presente en el algoritmo k-folds, cada vez que se repita el algoritmo se obtendran muestras de train y test diferentes. \\(\\\\[1cm]\\) Se calcula la métrica final de evaluación del modelo como el promedio de las \\(\\hspace{0.1cm}k\\cdot B\\hspace{0.1cm}\\) métricas calculadas en el paso anterior. Es decir, como el promedio de las \\(\\hspace{0.08cm}B\\hspace{0.08cm}\\) metricas obtenidas al iterar \\(\\hspace{0.08cm}B\\hspace{0.08cm}\\) veces el algoritmo k-fold. Si la métrica considerada es el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\), entonces: \\(\\\\[0.15cm]\\) En la iteración \\(\\hspace{0.08cm}j\\)-esima del algoritmo \\(\\hspace{0.08cm}k\\)-fold se obtiene como métrica de validación final: \\[ECM( {M})_{test}^{\\hspace{0.08cm}j \\hspace{0.05cm} *} \\hspace{0.13cm} = \\hspace{0.13cm}\\dfrac{1}{k} \\hspace{0.1cm} \\cdot\\hspace{0.1cm} \\sum_{r=1}^k \\hspace{0.15cm} ECM_{test}\\left(\\hspace{0.1cm}\\widehat{M}_r^{\\hspace{0.1cm}j}\\hspace{0.1cm}\\right)\\] \\(\\\\[0.15cm]\\) Por lo tanto, la métrica de validación final obtenida con el algoritmo \\(\\hspace{0.08cm}B\\)-repeated \\(\\hspace{0.08cm}k\\)-fold es: \\[ECM( {M})_{test}^{\\hspace{0.08cm}*} \\hspace{0.13cm} = \\hspace{0.13cm} \\dfrac{1}{ B} \\hspace{0.1cm} \\cdot\\hspace{0.1cm} \\sum_{j=1}^B ECM( {M})_{test}^{\\hspace{0.08cm}j \\hspace{0.05cm} *} \\hspace{0.13cm} = \\hspace{0.13cm} \\dfrac{1}{k\\cdot B} \\hspace{0.1cm} \\cdot\\hspace{0.1cm} \\sum_{j=1}^B \\hspace{0.1cm} \\sum_{r=1}^k \\hspace{0.15cm} ECM_{test}\\left(\\hspace{0.1cm}\\widehat{M}_r^{\\hspace{0.1cm}j}\\hspace{0.1cm}\\right)\\] Ventajas La métrica de validación calculada por repeted k-fold tiene menor varianza que con los métodos anteriores, luego es el mas preciso de todos ellos. Este debería ser el método empleado en la práctica, siempre que se pueda, ya que también es el que mas requerimientos computacionales tiene. "],["selección-de-modelos-basada-en-validación-cruzada.html", "Chapter 6 Selección de modelos basada en validación cruzada ", " Chapter 6 Selección de modelos basada en validación cruzada Dado un conjunto de modelos de aprendizaje supervisado, nos interesa establecer un criterio para seleccionar uno de ellos como el mejor. A continuación se expone un criterio basado en las métricas y algoritmos de validación que se han visto anteriormente. \\(\\\\[0.5cm]\\) Supongamos que estamos ante un problema de regresión o clasificación supervisada. Tenemos \\(\\hspace{0.1cm}h\\hspace{0.1cm}\\) modelos de aprendizaje supervisado \\(\\hspace{0.1cm}M_1\\hspace{0.05cm},\\hspace{0.05cm}M_2\\hspace{0.05cm},...,\\hspace{0.05cm}M_h\\hspace{0.05cm} \\\\\\) Se validan estos modelos usando un mismo algoritmo de validación, con una misma métrica de evaluación. Se obtendrán \\(\\hspace{0.1cm}h\\hspace{0.1cm}\\) valores de la métrica, una para cada modelo. \\(\\\\[0.3cm]\\) Si la métrica fuera el ECM se tendrian por ejemplo los siguientes valores: \\(\\\\[0.15cm]\\) \\[ECM(M_1)_{test}^{\\hspace{0.05cm} *} \\hspace{0.1cm}, ...,\\hspace{0.1cm} ECM(M_h)_{test}^{\\hspace{0.05cm} *} \\\\\\] El criterio es seleccionar el modelo que tienen mejor valor de la métrica. Si es una métrica de error, el que menor valor tiene. Si es una métrica de acierto, el que mayor valor tiene. \\(\\\\[0.8cm]\\) Si la métrica es de error, como por ejemplo el \\(\\hspace{0.1cm}ECM\\hspace{0.1cm}\\), entonces: \\(\\\\[0.7cm]\\) El modelo seleccionado es \\(\\hspace{0.1cm}M_{j^{\\hspace{0.08cm}*}}\\hspace{0.1cm}\\) , donde: \\(\\\\[0.5cm]\\) \\[j^{\\hspace{0.08cm}*} \\hspace{0.1cm}=\\hspace{0.1cm} arg \\hspace{0.1cm} \\underset{j \\in \\lbrace 1,...,h\\rbrace }{Min} \\hspace{0.15cm} ECM(M_j)_{test}^{\\hspace{0.05cm} *} \\\\[1cm]\\] Si la métrica es de acierto, como por ejemplo la \\(\\hspace{0.1cm}TAC\\hspace{0.1cm}\\), entonces: \\(\\\\[0.5cm]\\) El modelo seleccionado es \\(\\hspace{0.1cm}M_{j^{\\hspace{0.08cm}*}}\\hspace{0.1cm}\\) , donde: \\(\\\\[0.5cm]\\) \\[j^{\\hspace{0.08cm}*} \\hspace{0.1cm}=\\hspace{0.1cm} arg \\hspace{0.1cm} \\underset{j \\in \\lbrace 1,...,h\\rbrace }{Max} \\hspace{0.15cm} TAC(M_j)_{test}^{\\hspace{0.05cm} *}\\] "],["algoritmos-de-validación-cruzada-programados-en-python.html", "Chapter 7 Algoritmos de validación cruzada programados en Python 7.1 Validación simple no aleatoria 7.2 Validación simple aleatoria 7.3 Validación simple aleatoria repetida 7.4 Leave one out 7.5 k-fold 7.6 Repeated k-folds ", " Chapter 7 Algoritmos de validación cruzada programados en Python Importamos las librerias que vamos a utilizar: import pandas as pd import numpy as np import math import sklearn from sklearn.utils import resample from sklearn.neighbors import NearestNeighbors Cargamos los datos con los que vamos a trabajar, los cuales fueron detallados en nuestro artículo sobre regresión lineal Data = pd.read_csv(&#39;House_Price_Regression.csv&#39;) Data = Data.loc[:, [&#39;no_of_bedrooms&#39; , &#39;no_of_bathrooms&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;price&#39;, &#39;size_in_m_2&#39;, &#39;balcony_recode&#39;, &#39;private_garden_recode&#39;, &#39;quality_recode&#39;]] Data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } no_of_bedrooms no_of_bathrooms latitude longitude price size_in_m_2 balcony_recode private_garden_recode quality_recode 0 1 2 25.113208 55.138932 2700000 100.242337 1.0 0.0 2.0 1 2 2 25.106809 55.151201 2850000 146.972546 1.0 0.0 2.0 2 3 5 25.063302 55.137728 1150000 181.253753 1.0 0.0 2.0 3 2 3 25.227295 55.341761 2850000 187.664060 1.0 0.0 1.0 4 0 1 25.114275 55.139764 1729200 47.101821 0.0 0.0 2.0 A continuación se van a programar los algoritmos de validación cruzada que se han expuesto a nivel teórico anteriormente. Además se probarán los algoritmos con el modelo KNN tanto en su versión para regresión como para clasificación. 7.1 Validación simple no aleatoria class NotRandomSimpleValidation : # D --&gt; have to be a pandas data frame. # k --&gt; is the proportion of observation of D that define D_train. # model --&gt; object containing the initialized model to use. # The function has been created thinking that the model to be used will be one from the `sklearn` library. # response_name --&gt; have to be a string with the name of the response variable. # metric --&gt; It&#39;s the name of the validation metric. def __init__(self, k, metric, model): self.k = k self.metric = metric self.model = model def fit(self, D, response_name): N = len(D) self.D_train = D.iloc[0:(math.floor(self.k * N) + 1), :] self.D_test = D.iloc[(math.floor(self.k * N) + 1):N, :] self.X_train = self.D_train.loc[:, self.D_train.columns != response_name] self.Y_train = self.D_train.loc[:, response_name] self.X_test = self.D_test.loc[:, self.D_test.columns != response_name] self.Y_test = self.D_test.loc[:, response_name] self.model.fit(self.X_train, self.Y_train) def predict(self): self.Y_predict_test = self.model.predict(self.X_test) def compute_metric(self): if self.metric == &#39;ECM&#39;: self.ECM_test = np.mean((self.Y_predict_test - self.Y_test) ** 2) return self.ECM_test elif self.metric == &#39;TAC&#39;: self.TAC_test = np.mean((self.Y_predict_test == self.Y_test)) return self.TAC_test Definimos el modelo KNN para regresión con el que vamos a testear el algoritmo que se acaba de programar: knn_regression_init = sklearn.neighbors.KNeighborsRegressor(n_neighbors=10 , p=2, metric=&#39;minkowski&#39;) Aplicamos el algoritmo de validación sobre el modelo KNN para regresión, usando como métrica de validación el error cuadratico medio (ECM): NotRandomSimpleValidation_init = NotRandomSimpleValidation(k=0.75, metric=&#39;ECM&#39;, model=knn_regression_init) NotRandomSimpleValidation_init.fit(D=Data, response_name=&#39;price&#39;) NotRandomSimpleValidation_init.predict() ECM_test_Not_Random_Simple_Validation = NotRandomSimpleValidation_init.compute_metric() ECM_test_Not_Random_Simple_Validation 2176125958588.6355 Definimos el modelo KNN para clasificación con el que vamos a testear el algoritmo que se acaba de programar: knn_classification_init = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10, p=2, metric=&#39;minkowski&#39;) Aplicamos el algoritmo de validación simple no aleatoria sobre el modelo KNN para clasificación, usando como métrica de validación la tasa de acierto (TA): NotRandomSimpleValidation_init = NotRandomSimpleValidation(k=0.75, metric=&#39;TAC&#39;, model=knn_classification_init) NotRandomSimpleValidation_init.fit(D=Data, response_name=&#39;quality_recode&#39;) NotRandomSimpleValidation_init.predict() TAC_test_Not_Random_Simple_Validation = NotRandomSimpleValidation_init.compute_metric() TAC_test_Not_Random_Simple_Validation 0.5609243697478992 7.2 Validación simple aleatoria class RandomSimpleValidation : # D --&gt; have to be a pandas data frame. # k --&gt; is the proportion of observation of D that define D_train. # response --&gt; have to be a string with the name of the response variable. # model --&gt; object containing the initialized model to use. # The function has been created thinking that the model to be used will be one from the `sklearn` library. # metric --&gt; It&#39;s the name of the validation metric. # random_seed --&gt; seed to replicate the random process. def __init__(self, k, metric, model, random_seed): self.k = k self.metric = metric self.model = model self.random_seed = random_seed def fit(self, D, response_name): N = len(D) self.D_train = D.sample(frac=self.k, replace=False, random_state=self.random_seed) self.D_test = D.drop( self.D_train.index , ) self.X_train = self.D_train.loc[: , self.D_train.columns != response_name] self.Y_train = self.D_train.loc[: , response_name] self.X_test = self.D_test.loc[: , self.D_test.columns != response_name] self.Y_test = self.D_test.loc[: , response_name] self.model.fit(self.X_train, self.Y_train) def predict(self): self.Y_predict_test = self.model.predict(self.X_test) def compute_metric(self): if self.metric == &#39;ECM&#39;: self.ECM_test = np.mean((self.Y_predict_test - self.Y_test) ** 2) return self.ECM_test elif self.metric == &#39;TAC&#39;: self.TAC_test = np.mean((self.Y_predict_test == self.Y_test)) return self.TAC_test Definimos el modelo KNN para regresión con el que vamos a testear el algoritmo que se acaba de programar: knn_regression_init = sklearn.neighbors.KNeighborsRegressor(n_neighbors=10 , p=2, metric=&#39;minkowski&#39;) Aplicamos el algoritmo de validación simple aleatoria sobre el modelo KNN para regresión, usando como métrica de validación el error cuadratico medio (ECM): RandomSimpleValidation_init = RandomSimpleValidation(k=0.75, metric=&#39;ECM&#39;, model=knn_regression_init, random_seed=123) RandomSimpleValidation_init.fit(D=Data, response_name=&#39;price&#39;) RandomSimpleValidation_init.predict() ECM_test_Random_Simple_Validation = RandomSimpleValidation_init.compute_metric() ECM_test_Random_Simple_Validation 2464363295205.937 Definimos el modelo KNN para clasificacion con el que vamos a testear el algoritmo que se acaba de programar: knn_classification_init = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10 , p=2, metric=&#39;minkowski&#39;) Aplicamos el algoritmo de validación simple aleatoria sobre el modelo KNN para clasificación, usando como métrica de validación la tasa de acierto (TA): RandomSimpleValidation_init = RandomSimpleValidation(k=0.75, metric=&#39;TAC&#39;, model=knn_classification_init, random_seed=123) RandomSimpleValidation_init.fit(D=Data, reesponse_name=&#39;quality_recode&#39;) RandomSimpleValidation_init.predict() TAC_test_Random_Simple_Validation = RandomSimpleValidation_init.compute_metric() TAC_test_Random_Simple_Validation 0.5777310924369747 7.3 Validación simple aleatoria repetida class RepeatedRandomSimpleValidation : # D --&gt; It have to be a pandas data frame. # B --&gt; It&#39;s the number of iterations of the Random Simple Validation algorithm. # k --&gt; It&#39;s the proportion of observation of D that define D_train. # response --&gt; It have to be a string with the name of the response variable. # model --&gt; It&#39;s an object containing the initialized model to use. # The function has been created thinking that the model to be used will be one from the `sklearn` library. # metric --&gt; It&#39;s the name of the validation metric. # random_seed --&gt; It&#39;s the seed to replicate the random process def __init__(self, B, k, metric, model, random_seed): self.B = B self.k = k self.metric = metric self.model = model self.random_seed = random_seed def fit(self, D, response_name): self.D = D self.response_name = response_name np.random.seed(self.random_seed) self.seed_array = np.random.randint(9999999, size=(self.B)) def compute_metric(self): Metric_test_list = [ ] for b in range(0,self.B) : RandomSimpleValidation_init = RandomSimpleValidation(k=self.k, metric=self.metric, model=self.model, random_seed=self.seed_array[b]) RandomSimpleValidation_init.fit(D=self.D, response_name=self.response_name) RandomSimpleValidation_init.predict() Metric_test_list.append( RandomSimpleValidation_init.compute_metric() ) self.Metric_test = np.mean(Metric_test_list) return self.Metric_test Definimos el modelo KNN para regresión con el que vamos a testear el algoritmo que se acaba de programar: knn_regression_init = sklearn.neighbors.KNeighborsRegressor(n_neighbors=10 , p=2, metric=&#39;minkowski&#39;) Aplicamos el algoritmo de validación simple aleatoria repetida sobre el modelo KNN para regresión, usando como métrica de validación el error cuadratico medio (ECM): RepeatedRandomSimpleValidation_init = RepeatedRandomSimpleValidation(B=200, k=0.75, metric=&#39;ECM&#39;, model=knn_regression_init, random_seed=123) RepeatedRandomSimpleValidation_init.fit(D=Data, response_name=&#39;price&#39;) ECM_test_Repeated_Random_Simple_Validation = RepeatedRandomSimpleValidation_init.compute_metric() ECM_test_Repeated_Random_Simple_Validation 2273253730249.1133 Definimos el modelo KNN para clasificación con el que vamos a testear el algoritmo que se acaba de programar: knn_classification_init = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10, p=2, metric=&#39;minkowski&#39;) Aplicamos el algoritmo de validación simple aleatoria repetida sobre el modelo KNN para clasificación, usando como métrica de validación la tasa de acierto (TA): RepeatedRandomSimpleValidation_init = RepeatedRandomSimpleValidation(B=200, k=0.75, metric=&#39;TAC&#39;, model=knn_classification_init, random_seed=123) RepeatedRandomSimpleValidation_init.fit(D=Data, response_name=&#39;quality_recode&#39;) TAC_test_Repeated_Random_Simple_Validation = RepeatedRandomSimpleValidation_init.compute_metric() TAC_test_Repeated_Random_Simple_Validation 0.5515546218487395 7.4 Leave one out class LeaveOneOutValidation: # D --&gt; It have to be a pandas data frame. # response_name --&gt; It have to be a string with the name of the response variable. # model --&gt; It&#39;s an object containing the initialized model to use. # The function has been created thinking that the model to be used will be one from the `sklearn` library. # metric --&gt; It&#39;s the name of the validation metric. # random_seed --&gt; It&#39;s the seed to replicate the random process. def __init__(self, metric, model): self.metric = metric self.model = model self.ECM_test_list = [] self.TA_test_list = [] def fit(self, D, response_name): N = len(D) for r in range(0, N): D_test = D.iloc[r, :] D_train = D.drop(r, ) X_train = D_train.loc[:, D_train.columns != response_name] Y_train = D_train.loc[:, response_name] X_test = D_test.loc[D_test.index != response_name] Y_test = D_test.loc[response_name] self.model.fit(X_train, Y_train) Y_predict_test = self.model.predict(pd.DataFrame([X_test])) if self.metric == &#39;ECM&#39;: self.ECM_test_list.append(np.mean((Y_predict_test - Y_test)**2)) elif self.metric == &#39;TAC&#39;: self.TA_test_list.append(np.mean((Y_predict_test == Y_test))) def compute_metric(self): if self.metric == &#39;ECM&#39;: return np.mean(self.ECM_test_list) elif self.metric == &#39;TAC&#39;: return np.mean(self.TA_test_list) Definimos el modelo KNN para regresión con el que vamos a testear el algoritmo que se acaba de programar: knn_regression_init = sklearn.neighbors.KNeighborsRegressor(n_neighbors=10, p=2, metric=&#39;minkowski&#39;) Aplicamos el algoritmo de validación leave one out sobre el modelo KNN para regresión, usando como métrica de validación el error cuadratico medio (ECM): LeaveOneOutValidation_init = LeaveOneOutValidation(metric=&#39;ECM&#39;, model=knn_regression_init) LeaveOneOutValidation_init.fit(D=Data, response_name=&#39;price&#39;) ECM_test_leave_one_out = LeaveOneOutValidation_init.compute_metric() ECM_test_leave_one_out 2268581861335.2305 Definimos el modelo KNN para clasificación con el que vamos a testear el algoritmo que se acaba de programar: knn_classification_init = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10, p=2, metric=&#39;minkowski&#39;) Aplicamos el algoritmo de validación leave one out sobre el modelo KNN para clasificación, usando como métrica de validación la tasa de acierto (TAC): LeaveOneOutValidation_init = LeaveOneOutValidation(metric=&#39;TAC&#39;, model=knn_classification_init) LeaveOneOutValidation_init.fit(D=Data, response_name=&#39;quality_recode&#39;) TAC_test_leave_one_out = LeaveOneOutValidation_init.compute_metric() TAC_test_leave_one_out 0.5511811023622047 7.5 k-fold class KFoldCV: # D --&gt; It have to be a pandas data frame. # K --&gt; It&#39;s the number of folds of K-fold algorithm.. # response_name --&gt; It have to be a string with the name of the response variable. # model --&gt; It&#39;s an object containing the initialized model to use. # The function has been created thinking that the model to be used will be one from the `sklearn` library. # metric --&gt; It&#39;s the name of the validation metric. # random_seed --&gt; It&#39;s the seed to replicate the random process. def __init__(self, D, K, response_name, random_seed, metric, model): self.D = D self.K = K self.response_name = response_name self.random_seed = random_seed self.metric = metric self.model = model self.ECM_K_FOLDS_vector = [] self.TA_K_FOLDS_vector = [] self.df_sample = None def __resample_df(self): np.random.seed(self.random_seed) sample = resample(range(0, len(self.D)), n_samples=len(self.D), replace=False) self.df_sample = pd.DataFrame({&#39;index&#39;: range(0,len(self.D)) , &#39;sample&#39;:sample}) def __get_quantiles(self): Q = [] for q in np.arange(0 , 1 + 1/self.K , 1/self.K): Q.append( np.quantile( range(0, len(self.D)) , q ).round(0) ) return Q def __train_test_split(self, q, Q): X_test = self.D.loc[self.df_sample.loc[Q[q]:(math.floor(Q[q+1])-1), &#39;sample&#39;] , self.D.columns != self.response_name ] Y_test = self.D.loc[self.df_sample.loc[Q[q]:(math.floor(Q[q+1])-1), &#39;sample&#39;] , self.D.columns == self.response_name ] X_train = self.D.loc[ : , self.D.columns != self.response_name ].drop(self.df_sample.loc[Q[q]:(math.floor(Q[q+1])-1), &#39;sample&#39;] ) Y_train = self.D.loc[ : , self.D.columns == self.response_name ].drop(self.df_sample.loc[Q[q]:(math.floor(Q[q+1])-1), &#39;sample&#39;]) Y_test = Y_test.to_numpy() return X_test, Y_test, X_train, Y_train def fit(self): self.__resample_df() Q = self.__get_quantiles() for j in range(0, len(Q)-1): X_test, Y_test, X_train, Y_train = self.__train_test_split(j, Q) self.model.fit(X_train, Y_train) Y_predict_test = self.model.predict(X_test) if self.metric == &#39;ECM&#39;: self.ECM_K_FOLDS_vector.append(np.mean((Y_predict_test - Y_test)**2)) elif self.metric == &#39;TAC&#39;: self.TA_K_FOLDS_vector.append(np.mean((Y_predict_test == Y_test))) def get_metric(self): if self.metric == &#39;ECM&#39;: return np.mean(self.ECM_K_FOLDS_vector) elif self.metric == &#39;TAC&#39;: return np.mean(self.TA_K_FOLDS_vector) Definimos el modelo KNN para regresión con el que vamos a testear el algoritmo que se acaba de programar: knn_regression_init = sklearn.neighbors.KNeighborsRegressor(n_neighbors=10 , p=2, metric=&#39;minkowski&#39;) Aplicamos el algoritmo de validación cruzada K-fold sobre el modelo KNN para regresión, usando como métrica de validación el error cuadrático medio (ECM): KFoldCV_init = KFoldCV(D=Data, K=10, response_name=&#39;price&#39;, random_seed=123, metric=&#39;ECM&#39;, model=knn_regression_init) KFoldCV_init.fit() ECM_K_Folds = KFoldCV_init.get_metric() ECM_K_Folds 2220503635404.005 Definimos el modelo KNN para clasificación con el que vamos a testear el algoritmo que se acaba de programar: knn_classification_init = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10 , p=2, metric=&#39;minkowski&#39;) Aplicamos el algoritmo de validación cruzada K-fold sobre el modelo KNN para clasificación, usando como métrica de validación la tasa de acierto (TA): KFoldCV_init = KFoldCV(D=Data, K=10, response_name=&#39;quality_recode&#39;, random_seed=123, metric=&#39;TAC&#39;, model=knn_classification_init) KFoldCV_init.fit() TAC_K_Folds = KFoldCV_init.get_metric() TAC_K_Folds 0.5363984159477089 7.6 Repeated k-folds class RepeatedKFoldCV: # D --&gt; It have to be a pandas data frame. # B --&gt; It&#39;s the number of iterations of the K-Fold algorithm. # K --&gt; It&#39;s the number of folds of K-fold algorithm.. # response_name --&gt; It have to be a string with the name of the response variable. # model --&gt; It&#39;s an object containing the initialized model to use. # The function has been created thinking that the model to be used will be one from the `sklearn` library. # metric --&gt; It&#39;s the name of the validation metric. # random_seed --&gt; It&#39;s the seed to replicate the random process. def __init__(self, B, K, random_seed, metric, model): self.B = B self.K = K self.random_seed = random_seed self.metric = metric self.model = model def fit(self, D, response_name): self.Metric_Repeted_K_Folds_list = [ ] np.random.seed(self.random_seed) for b in range(0, self.B): KFoldCV_init = KFoldCV(D=D, K=self.K, response_name=response_name, random_seed=123, metric=self.metric, model=self.model) KFoldCV_init.fit() self.Metric_Repeted_K_Folds_list.append( KFoldCV_init.get_metric() ) def get_metric(self): return np.mean(self.Metric_Repeted_K_Folds_list) Definimos el modelo KNN para regresión con el que vamos a testear el algoritmo que se acaba de programar: knn_regression_init = sklearn.neighbors.KNeighborsRegressor(n_neighbors=10, p=2, metric=&#39;minkowski&#39;) Aplicamos el algoritmo de validación cruzada K-fold repetida sobre el modelo KNN para regresión, usando como métrica de validación el error cuadrático medio (ECM): RepeatedKFoldCV_init = RepeatedKFoldCV(B=100, K=10, random_seed=123, metric=&#39;ECM&#39;, model=knn_regression_init) RepeatedKFoldCV_init.fit(D=Data, response_name=&#39;price&#39;) ECM_repeated_K_Folds = RepeatedKFoldCV_init.get_metric() ECM_repeated_K_Folds 2220503635404.004 Definimos el modelo KNN para clasificación con el que vamos a testear el algoritmo que se acaba de programar: knn_classification_init = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10 , p=2, metric=&#39;minkowski&#39;) Aplicamos el algoritmo de validación cruzada K-fold repetida sobre el modelo KNN para clasificación, usando como métrica la tasa de acierto (TA): RepeatedKFoldCV_init = RepeatedKFoldCV(B=100, K=10, random_seed=123, metric=&#39;TAC&#39;, model=knn_classification_init) RepeatedKFoldCV_init.fit(D=Data, response_name=&#39;quality_recode&#39;) TAC_repeated_K_Folds = RepeatedKFoldCV_init.get_metric() TAC_repeated_K_Folds 0.5363984159477088 "],["algoritmos-de-valicación-cruzada-con-sklearn.html", "Chapter 8 Algoritmos de valicación cruzada con Sklearn 8.1 k-fold 8.2 Repeated k-fold ", " Chapter 8 Algoritmos de valicación cruzada con Sklearn Importamos los módulos que usaremos en esta sección: from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedKFold from sklearn.model_selection import KFold 8.1 k-fold Inicializamos el algoritmo k-fold. Usaremos los parámetros k=10 y un inicio aleatorio del algoritmo con semilla 123. cv_k_fold = KFold(n_splits=10, random_state=123, shuffle=True) Vamos a aplicar el algoritmo de validacion k-fold a un porblema de regresión usando la métrica ECM. Creamos por un lado el vector de la respuesta y por otro la matriz de los predictores. En este caso la respuesta es la variable price, y los predictores son el resto de variables. Y_reg = Data.loc[:,&#39;price&#39;] X_reg = Data.loc[:, Data.columns != &#39;price&#39;] Calculamos la métrica ECM usando el algoritmo de validación cruzada k-folds con k=10: ECM_K_Folds_sklearn = cross_val_score(knn_regression, X_reg, Y_reg, cv=cv_k_fold, scoring=&#39;neg_mean_squared_error&#39;) ECM_K_Folds_sklearn = np.mean( - ECM_K_Folds_sklearn ) ECM_K_Folds_sklearn 2220103512647.2095 Vamos a aplicar el algoritmo de validacion k-fold a un porblema de clasificación supervisada usando la métrica TAC. Creamos por un lado el vector de la respuesta y por otro la matriz de los predictores. En este caso la respuesta es la variable quality_recode, y los predictores son el resto de variables. Y_class = Data.loc[:,&#39;quality_recode&#39;] X_class = Data.loc[:, Data.columns != &#39;quality_recode&#39;] TAC_K_Folds_sklearn = cross_val_score(knn_classification, X_class, Y_class, cv=cv_k_fold, scoring=&#39;accuracy&#39;) TAC_K_Folds_sklearn = np.mean( TAC_K_Folds_sklearn ) TAC_K_Folds_sklearn 0.5606558280518048 8.2 Repeated k-fold Inicializamos el algoritmo repeated k-fold. Usaremos los parámetros k=10, un número de 100 repeticiones y un inicio aleatorio con semilla 123. cv_repeated_k_fold = RepeatedKFold(n_splits=10, n_repeats=100, random_state=123) Vamos a aplicar el algoritmo de validacion k-fold a un problema de regresión usando la métrica TAC. Calculamos la métrica ECM usando el algoritmo de validación cruzada repeated-k-folds con k=10 y B=100 repeticiones: ECM_repeated_K_Folds_sklearn = cross_val_score(knn_regression, X_reg, Y_reg, cv=cv_repeated_k_fold, scoring=&#39;neg_mean_squared_error&#39;) ECM_repeated_K_Folds_sklearn = np.mean( - ECM_repeated_K_Folds_sklearn ) ECM_repeated_K_Folds_sklearn 2269036673148.278 Vamos a aplicar el algoritmo de validacion repeated k-fold a un problema de clasificación supervisada usando la métrica TAC. Calculamos la métrica TAC usando el algoritmo de validación cruzada repeated-k-folds con k=10 y B=100 repeticiones: TAC_repeated_K_Folds_sklearn = cross_val_score(knn_classification, X_class, Y_class, cv=cv_repeated_k_fold, scoring=&#39;accuracy&#39;) TAC_repeated_K_Folds_sklearn = np.mean( TAC_repeated_K_Folds_sklearn ) TAC_repeated_K_Folds_sklearn 0.554644943510609 "],["comparación-final.html", "Chapter 9 Comparación final ", " Chapter 9 Comparación final Importamos las librerias de visualización de datos que usaremos para realizar la comparación final. import seaborn as sns import matplotlib.pyplot as plt Creamos un data-frame con los distintos valores de la métrica ECM obtenidos con los distintos algoritmos de validación utilizados: ECM_df = pd.DataFrame({&#39;ECM&#39; : [ECM_test_Simple_Validation_not_random , ECM_test_Simple_Validation_random, ECM_test_Simple_Validation_repeated, ECM_test_leave_one_out, ECM_K_Folds, ECM_K_Folds_sklearn , ECM_repeated_K_Folds, ECM_repeated_K_Folds_sklearn], &#39;names&#39; : [&#39;Simple validation (not random)&#39; , &#39;Simple validation (random)&#39;, &#39;Simple validation (repeted)&#39;, &#39;Leave one out&#39;, &#39;k-folds&#39;, &#39;k-folds (sklearn)&#39;, &#39;repeted-k-folds&#39;, &#39;repeted-k-folds (sklearn)&#39;]}) fig, ax = plt.subplots() p3 = sns.barplot(x=&quot;ECM&quot;, y =&#39;names&#39; , data=ECM_df) fig.savefig(&#39;p3.jpg&#39;, format=&#39;jpg&#39;, dpi=1200) Creamos un data-frame con los distintos valores de la métrica TAC obtenidos con los distintos algoritmos de validación utilizados: TAC_df = pd.DataFrame({&#39;TAC&#39; : [TAC_test_Simple_Validation_not_random , TAC_test_Simple_Validation_random, TAC_test_Simple_Validation_repeated, TAC_test_leave_one_out, TAC_K_Folds, TAC_K_Folds_sklearn, TAC_repeated_K_Folds, TAC_repeated_K_Folds_sklearn], &#39;names&#39; : [&#39;Simple validation (not random)&#39; , &#39;Simple validation (random)&#39;, &#39;Simple validation (repeted)&#39;, &#39;Leave one out&#39;, &#39;k-folds&#39;, &#39;k-folds (sklearn)&#39;, &#39;repeted-k-folds&#39;, &#39;repeted-k-folds (sklearn)&#39;]}) fig, ax = plt.subplots() p4 = sns.barplot(x=&quot;TAC&quot;, y =&#39;names&#39; , data=TAC_df) fig.savefig(&#39;p4.jpg&#39;, format=&#39;jpg&#39;, dpi=1200) "],["bibliografía.html", "Chapter 10 Bibliografía ", " Chapter 10 Bibliografía Amat Rodrigo, J. (Noviembre 2020). Validación de modelos predictivos: Cross-validation, OneLeaveOut, Bootstraping. Cienciadedatos. https://www.cienciadedatos.net/documentos/30_cross-validation_oneleaveout_bootstrap \\(\\\\[0.5cm]\\) Aler Mur, Ricardo. (2022). Metodología: evaluación de modelos. [Presentación de PowerPoint]. Aula Global UC3M. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
