--- 
title: "K-means"
author: "Fabio Scielzo Ortiz"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::gitbook,
  set in the _output.yml file.
link-citations: yes
github-repo: rstudio/bookdown-demo
---


# Introduction

<center>

![](pic_ajuste.jpg){width="45%"}

</center> 

<div class="warning" style='background-color:#FCF2EC; color: #000000; border-left: solid #FE9554 7px; border-radius: 3px; size:1px ; padding:0.1em;'>
<span>
 
<p style='margin-left:10em;'>


- **More articles: $\hspace{0.1cm}$ [Estadistica4all](https://estadistica4all.com/)**

- **Author:** $\hspace{0.1cm}$ [Fabio Scielzo Ortiz](https:/estadistica4all.com/creador.html)

- **It's recommended to open the article on a computer, tablet or on a phone in desktop version.**

</p>
 
</p></span>
</div>


 
<br>



 
# Unsupervised Classification Problem

An unsupervised classification problem is an statistical problem that consist on predict a **categorical** response variable, using only information about ones predictor variables, because there isnÂ´t data about the response. 

 Unsupervised classification problems are also called **clustering** problems.




- We have $\hspace{0.02cm}p\hspace{0.02cm}$ predictors  $\hspace{0.02cm}\mathcal{X}_1 ,...,\mathcal{X}_p\hspace{0.02cm}$ and a **categorical** response variable $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$ for which we haven't any data sample. 


- We have an $\hspace{0.02cm}n\hspace{0.02cm}$ size sample of the $\hspace{0.02cm}p\hspace{0.02cm}$ predictors $\hspace{0.02cm}X_1,...,X_p\hspace{0.02cm}$  but we haven't a sample for the response, and we also don't know it's range.

    Therefore, we have the following data matrix:

     $$D=[X_1,...,X_p]=\begin{pmatrix}
    x_{11}&x_{12}&...&x_{1p} \\
    x_{21}&x_{22}&...&x_{2p} \\
    ... &...& ...&...\\
    x_{n1}&x_{n2}&...&x_{np}
    \end{pmatrix} = \begin{pmatrix}
    x_{1} \\
    x_{2}  \\
     ...\\
     x_{n} 
    \end{pmatrix}\\$$
    

- The goal is to predict the response variable   $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$ for any observations vector of the predictors, using only the available sample information about the predictors.


<br>


<br>


# K-means 

K-means is an unsupervised classification algorithm, in the sense that it allows solving unsupervised classification problems.

 
Next, a formal description of the algorithm will be made. But first, it should be noted that there are several versions of K-means, and here, the version proposed by Park and Jun in the paper referenced in the bibliography, will be taken .

 

 

<br>

## K-means algorithm

The response variable $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$ is considered to have $\hspace{0.02cm} k\hspace{0.02cm}$ categories $\hspace{0.02cm } g_1,...,g_k\hspace{0.01 cm}$ .

<div class="warning" style='background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;'>
<span>


- **1) Initial clusters definition.**

    - The available observations of the predictors are randomly assigned to one of the response categories, this is how the the initial clusters $\hspace{0.1cm} C_1,...,C_k\hspace{0.1cm}$ are built.

    - The clusters must be of similar size, all the observations in the sample must belong to some cluster, and two different clusters must not contain the same observation.  

       
    - Formally, the cluster $C_r$ will be defined as follows:

      $$C_r = \Bigl( \hspace{0.12cm} x_i^t = (x_{i1},...,x_{ip}) \hspace{0.2cm}: \hspace{0.2cm} i \in I_r \subset \lbrace 1,...,n \rbrace \hspace{0.12cm} \Bigr)^t$$

      where: 

      - $\# I_r \hspace{0.1cm} \approx \hspace{0.1cm} \# I_h \hspace{0.25cm} , \hspace{0.25cm}  \forall r\neq h \in \lbrace 1,...,n\rbrace$

      - $\overset{n}{\underset{r=1}{\cup}}  I_r \hspace{0.1cm}=\hspace{0.1cm} \lbrace 1,...,n\rbrace$

      - $I_r \cap I_h \hspace{0.1cm}=\hspace{0.1cm} \varnothing \hspace{0.25cm} , \hspace{0.25cm}  \forall r\neq h \in \lbrace 1,...,n\rbrace$
      
      - $I_r\hspace{0.05cm}$ is the set with the index of the predictors observations that belongs to the cluster $\hspace{0.02cm}C_r\hspace{0.02cm}$. So, it can be seen as the set of the individuals associated to the cluster  $\hspace{0.02cm} C_r$ .

      - $I_r\hspace{0.05cm}$   is randomly defined in this first stage.  


       -  $C_r\hspace{0.05cm}$ is a vector whose components are row vectors, so, is a matrix. As in addition that vectors are  observations of the predictors, it can be seen as a data matrix.  
       

<br>


- **2) Computing the centroids of the initial clusters.**


    In this step,the centroids of the initial clusters are computed.

    - The **centroid** of the cluster $\hspace{0.02cm} C_r\hspace{0.02cm}$ is defined in K-means as the mean of the cluster observations. So, if the observation are  $\hspace{0.02cm}p$-dimensional, the centroid will be a mean vector.


    - Formally, the **centroid** of the  cluster $\hspace{0.02cm}C_r\hspace{0.02cm}$ is defined as $\hspace{0.05cm}\overline{x}_{C_r} = \left(\hspace{0.04cm} \overline{X \hspace{0.1cm}}_{1, C_r} ,...,\overline{X\hspace{0.1cm}}_{p, C_r}\hspace{0.02cm} \right)$

      where:

      $$X_{j , C_r } = \bigl(\hspace{0.1cm} x_{ij} \hspace{0.15cm}:\hspace{0.15cm} i\in I_r \hspace{0.1cm} \bigr)  \hspace{0.3cm} , \hspace{0.3cm} \forall j\in \lbrace 1,...,p \rbrace$$

      is the  sample of observations   of the variable $\hspace{0.02cm}\mathcal{X}_j\hspace{0.02cm}$ that belong to the cluster $\hspace{0.02cm}C_r\hspace{0.02cm}$.  
    
      Therefore,   $\hspace{0.02cm}\overline{X \hspace{0.01cm}}_{j, C_r}\hspace{0.04cm}$ is the mean of the observations of $\hspace{0.02cm}\mathcal{X}_j\hspace{0.02cm}$ that belong to the cluster $\hspace{0.02cm}C_r$ .


 <br>


- **3) Computing the sum of intra-clusters variances.**


    - Given a distance measure $\hspace{0.02cm}\delta\hspace{0.02cm}$ defined for each pair $\hspace{0.2cm}(x_i,x_r)\hspace{0.2cm}$ of observations of the predictors, the **sum of intra-clusters variances** for the clusters configuration $\hspace{0.02cm}C_1,...,C_k\hspace{0.02cm}$ is defined as follows:


      $$V(C_1,...,C_k)  \hspace{0.1cm}= \hspace{0.1cm} \sum_{r=1}^{k} \hspace{0.2cm} \sum_{i \in I_r} \hspace{0.2cm} \delta(x_i , \overline{x}_{C_r})^2 \\$$



      $V(C_1,...,C_k)\hspace{0.12cm}$, is the metric based on which the stopping criteria will be defined.



    - It measures how similar are the observations that belong to a same cluster. The lower it is, the more similar they are, and vice versa.


<br> 


- **4) Reassignment of observations to clusters.**


    -  $\hspace{0.03cm} \delta(x_i , \overline{x}_{C_r}) \hspace{0.02cm}$ is computed.


    -  $\hspace{0.03cm}x_i\hspace{0.12cm}$ is reassigned to it's nearest cluster.

    - Formally, $\hspace{0.02cm}x_i\hspace{0.02cm}$ is reassigned to the cluster $\hspace{0.02cm}C_{r^*}$
    
      where:

      $$r^* \hspace{0.12cm}=\hspace{0.12cm} arg \hspace{0.14cm} \underset{r }{Min} \hspace{0.16cm} \delta(x_i , \overline{x}_{C_r} ) \hspace{0.3cm} ,  \hspace{0.3cm} r \in \lbrace 1,..., k \rbrace$$

      Then, $\hspace{0.02cm}x_i\hspace{0.02cm}$ es is reassigned to the cluster $\hspace{0.02cm}C_{r^*}\hspace{0.02cm}$, which could be the same cluster that it already belonged to, or not. 


    - Once all the observations $\hspace{0.02cm}x_1,...,x_n\hspace{0.02cm}$ have been reassigned, a new configuration of clusters $\hspace{0.02cm}C_1^{1 },... ,C_k^{1 }\hspace{0.02cm}$ is obtained. , which, in general, will be different from the previous one.

      The superscript 1 indicates that this new cluster configuration was obtained in the first iteration of the algorithm.


<br>

 
- **5) Computing the centroids of the new clusters.**



    - The centroids of the new clusters are computed:  

       $$\overline{x}_{C_r^1} \hspace{0.1cm}=\hspace{0.1cm} \left(\hspace{0.04cm} \overline{X \hspace{0.01cm}}_{1, C_r^1} ,...,\overline{X\hspace{0.01cm}}_{p, C_r^1} \right) \hspace{0.3cm} , \hspace{0.3cm} r\in \lbrace 1,..., k \rbrace$$


<br>


- **6) Computing the sum of the intra-clusters variances for the new clusters.**

    - The sum of intra-clusters variances is competed for the new clusters configuration, that was obtained in the previous step:

      $$V(C_1^{1 },...,C_k^{1 })$$

 
 <br>

 

- **7) Iterate steps 4) , 5) and 6).**


    - Steps **4)** , **5)** and **6)** are iterate a  $\hspace{0.02cm}b\hspace{0.02cm}$  times, so are obtained $\hspace{0.02cm}b\hspace{0.02cm}$ clusters configurations, and with it  $\hspace{0.02cm}b\hspace{0.02cm}$ values of the sum of the intra-clusters variances.


 
<br>

- **8) Selecting the optimal clusters configuration**


    - Once the previous steps are done, a total of $\hspace{0.02cm}b+2\hspace{0.02cm}$ clusters configurations have been obtained $\hspace{0.1cm}\Rightarrow \hspace{0.1cm} b\hspace{0.1cm}$ in step **5)** , $1$ in step **1)** and $1$ in step **4)**:



    $$\Bigl\lbrace \hspace{0.1cm} V(C_1^{h},....,C_k^h) \hspace{0.15cm} : \hspace{0.15cm} h \in \lbrace 1,...,b+2\rbrace \hspace{0.1cm} \Bigr\rbrace$$




    - The clusters configuration that minimize the sum of intra-clusters variances is selected as optimal configuration.

    -  Formally, the clusters configuration $\hspace{0.1cm} C_1^{h^*},...,C_k^{h^*}$ is selected as optimal.

       where:
     
       $$h^*  \hspace{0.15cm}=\hspace{0.12cm} arg \hspace{0.12cm} \underset{h }{Min} \hspace{0.16cm} V(C_1^{h},....,C_k^h) \hspace{0.3cm} ,  \hspace{0.3cm} h \in \lbrace 1,..., b+2 \rbrace$$
    

</p>
 
</p></span>
</div>

 


<br>


**Observations:**

 

- **K-means predictors should be quantitative**


    - Why? $\hspace{0.15cm}\Rightarrow\hspace{0.15cm}$ Due to the centroid definition as mean vector. The mean should only be applied to quantitative variables.

 


- **What distance metrics should be used in K-means?**

    - Due to K-means is only for quantitative variables, only quantitative distance metric should  be used, such as Euclidea, Minkowski, Canberra, Pearson or Mahalanobis.


<br>

## Clusters interpretation


The mean could be compute for the quantitative variables grouped by each optimal cluster. And the mode for the categorical ones, grouped by the same. Then we can make an interpretation of each cluster comparing these values.



<br>


## Clustering validation algorithms 

One of the most popular clustering validation algorithm is the Silhouette.

This topic will be seen in an specific article about clustering validation.



<br>


## Ajuste del hiper-parametro k

An optimal tuning of the hyper-parameter $k$ can be performed using the Silhouette algorithm.

As before, this question will be addressed in future articles.

 


<br>



## K-means programmed in `Python` 

 
 
 
 
 
 
 
<br>



## K-means with `sklearn`
 
 
 
 
 
 
<br>
 
 
 
# Bibliographi

- Park and Jun paper
 
 
 








