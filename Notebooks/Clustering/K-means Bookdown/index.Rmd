--- 
title: "K-means"
author: "Fabio Scielzo Ortiz"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::gitbook,
  set in the _output.yml file.
link-citations: yes
github-repo: rstudio/bookdown-demo
---


# Introduction

<center>

![](pic_ajuste.jpg){width="45%"}

</center> 

<div class="warning" style='background-color:#FCF2EC; color: #000000; border-left: solid #FE9554 7px; border-radius: 3px; size:1px ; padding:0.1em;'>
<span>
 
<p style='margin-left:10em;'>


- **More articles: $\hspace{0.1cm}$ [Estadistica4all](https://estadistica4all.com/)**

- **Author:** $\hspace{0.1cm}$ [Fabio Scielzo Ortiz](https://estadistica4all.com/creador.html)

- **It's recommended to open the article on a computer, tablet or on a phone in desktop version.**

</p>
 
</p></span>
</div>


 
<br>



 
# Unsupervised Classification Problem

An unsupervised classification problem is an statistical problem that consist on predict a **categorical** response variable, using only information about ones predictor variables, because there isn´t data about the response. 

 Unsupervised classification problems are also called **clustering** problems.




- We have $\hspace{0.02cm}p\hspace{0.02cm}$ predictors  $\hspace{0.02cm}(\mathcal{X}_1 ,...,\mathcal{X}_p)\hspace{0.02cm}$ and a **categorical** response variable $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$ for which we haven't any data sample. 


- We have an $\hspace{0.02cm}n\hspace{0.02cm}$ size sample of the $\hspace{0.02cm}p\hspace{0.02cm}$ predictors $\hspace{0.02cm}X_1,...,X_p\hspace{0.02cm}$  but we haven't a sample for the response, and we also don't know it's range.

    Therefore, we have the following data matrix:

     $$D=[X_1,...,X_p]=\begin{pmatrix}
    x_{11}&x_{12}&...&x_{1p} \\
    x_{21}&x_{22}&...&x_{2p} \\
    ... &...& ...&...\\
    x_{n1}&x_{n2}&...&x_{np}
    \end{pmatrix} = \begin{pmatrix}
    x_{1} \\
    x_{2}  \\
     ...\\
     x_{n} 
    \end{pmatrix}\\$$
    

- The goal is to predict the response variable   $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$ for any observations vector of the predictors, using only the available sample information about the predictors.


<br>


<br>


# k-means

K-means is an unsupervised classification algorithm, in the sense that it allows solving unsupervised classification problems.

 
Next, a formal description of the algorithm will be made. But first, it should be noted that there are several versions of K-means, and here, the version proposed by Park and Jun in the paper referenced in the bibliography, will be taken .

 

 

<br>


**K-means**    


<div class="warning" style='background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;'>
<span>

 
- It's considered that the response variable  $\hspace{0.1cm}\mathcal{Y}\hspace{0.1cm}$ has $\hspace{0.1cm} k\hspace{0.1cm}$ categories $\hspace{0.1cm}g_1,...,g_k$ 


- **Initial clusters definition.**

 - The available observations of the predictors are randomly assigned to one of the response categories, this is how the the initial clusters $\hspace{0.1cm} C_1,...,C_k\hspace{0.1cm}$ are built.

    - The clusters must be of similar size, all the observations in the sample must belong to some cluster, and two different clusters must not contain the same observation.  

       
    Formally, the clusters will be defined as follows:

    $$C_r = \Bigl( \hspace{0.12cm} x_i^t = (x_{i1},...,x_{ip}) \hspace{0.2cm}: \hspace{0.2cm} i \in I_r \subset \lbrace 1,...,n \rbrace \hspace{0.12cm} \Bigr)^t \\$$

    Where: 

    $$\# I(C_r) \hspace{0.1cm} \approx \hspace{0.1cm} \# I(C_h) \hspace{0.25cm} , \hspace{0.25cm}  \forall r\neq h \in \lbrace 1,...,n\rbrace$$

    $$\overset{n}{\underset{r=1}{\cup}}  I(C_r) \hspace{0.1cm}=\hspace{0.1cm} \lbrace 1,...,n\rbrace \\$$

      $$I(C_r) \cap I(C_h) \hspace{0.1cm}=\hspace{0.1cm} \varnothing \hspace{0.25cm} , \hspace{0.25cm}  \forall r\neq h \in \lbrace 1,...,n\rbrace$$

       $I_r$ is randomly defined in this first stage. 



       - $I(C_r)\hspace{0.1cm}$ es el conjunto de indices de las observaciones de los predictores que pertenecen al cluster $\hspace{0.1cm}C_r$. $\hspace{0.08cm}$ Por lo que puede verse como el conjunto de los individuos de la muestra que están asociados al cluster $\hspace{0.1cm} C_r \\$


       -  $C_r\hspace{0.1cm}$ es un vector cuyas componentes son vectores fila, luego es una matriz. Como además estos vectores fila son observaciones de variables estadisticas, puede verse como una matriz de datos. 
       




- **Computing the centroids of the initial clusters**


    -  The centroids of the initial clusters are computed.

    The **centroid** of the cluster $\hspace{0.1cm} C_r\hspace{0.1cm}$ is defined in K-means as the mean of the cluster observation.  if the observation are  $\hspace{0.1cm}p$-dimensional, the centroid will be a mean vector.


    The **centroid** of the  cluster $\hspace{0.1cm}C_r\hspace{0.1cm}$ is defined as $\hspace{0.2cm}\overline{x}_{C_r} = \left( \overline{X \hspace{0.1cm}}_{1, C_r} ,...,\overline{X\hspace{0.1cm}}_{p, C_r} \right)$

    Where:

    $$X_{j , C_r } = (\hspace{0.1cm} x_{ij} \hspace{0.1cm}/\hspace{0.1cm} i\in I_r \hspace{0.1cm})  \hspace{0.2cm} , \hspace{0.2cm} \forall j\in \lbrace 1,...,p \rbrace$$

    is the sample observations if the variable $\hspace{0.1cm}\mathcal{X}_j\hspace{0.1cm}$ that belong to the cluster $\hspace{0.1cm}C_r$ $\\[0.8cm]$
    
    $\hspace{1cm}$ Therefore,   $\hspace{0.1cm}\overline{X \hspace{0.1cm}}_{j, C_r}\hspace{0.1cm}$ is the mean of the observations of $\hspace{0.1cm}\mathcal{X}_j\hspace{0.1cm}$ that belong to the cluster $\hspace{0.1cm}C_r$ .


 


- **Computing the sum of intra-clusters variances**


    Given a distance measure $\hspace{0.1cm}\delta\hspace{0.1cm}$ defined for each pair $\hspace{0.1cm}(x_i,x_r)\hspace{0.1cm}$ of observations of the predictors, the **sum if intra-clusters variances** for the clusters configuration $\hspace{0.1cm}C_1,...,C_k\hspace{0.1cm}$ is defined as follows:


    $$V(C_1,...,C_k) = \sum_{r=1}^{k} \hspace{0.2cm} \sum_{i \in I_r} \hspace{0.2cm} \delta(x_i , \overline{x}_{C_r}) \\$$



    $\hspace{0.12cm}V(C_1,...,C_k)\hspace{0.12cm}$, is the metric based on which the stopping criteria will be defined.



    - Measures how similar are the observations that belong to the same cluster. The lower it is, the more similar they are, and vice versa.

 

- **Reassignment of observations to clusters**


    -  $\hspace{0.12cm} \delta(x_i , \overline{x}_{C_r}) \hspace{0.02cm}$ is computed.


    -  $\hspace{0.12cm}x_i\hspace{0.12cm}$ is reassigned to it's nearest cluster.

    - Formally, $\hspace{0.12cm}x_i\hspace{0.12cm}$ is reassigned to the cluster $\hspace{0.12cm}C_{r^*}$
    
    Where:

    $$r^* \hspace{0.12cm}=\hspace{0.12cm} arg \hspace{0.12cm} \underset{r}{Min} \hspace{0.16cm} \delta(x_i , \overline{x}_{C_r} )$$

    - Then, $\hspace{0.12cm}x_i\hspace{0.12cm}$ es is reassigned to the cluster $\hspace{0.12cm}C_{r^*}\hspace{0.12cm}$, which could be the same cluster that it already belonged to, or not. 


    - Once all the observations $\hspace{0.12cm}x_1,...,x_n\hspace{0.12cm}$ have been reassigned, a new configuration of clusters $\hspace{0.12cm}C_1^{1 },... is obtained. ,C_k^{1 }\hspace{0.12cm}$ , which, in general, will be different from the previous one.

    The superscript 1 indicates that this new cluster configuration was obtained in the first iteration of the algorithm.


 
- **Computing the centroids of the new clusters**



    - The centroids of the new clusters are computed :  

    $$\overline{x}_{C_r^1} = \left( \overline{X \hspace{0.1cm}}_{1, C_r^1} ,...,\overline{X\hspace{0.1cm}}_{p, C_r^1} \right)$$

    for $\hspace{0.1cm} r\in \lbrace 1,..., k \rbrace$


  


- **Computing the sum of the intra-clusters variances for the new clusters**

    - The sum of intra-clusters variances is competed for the new clusters configuration, that was obtained in the previous step:

    $$V(C_1^{1 },...,C_k^{1 })$$

 

- **Iterate steps 4) , 5) and 6)**


    - Steps **4)** , **5)** and **6)** are iterate a number $\hspace{0.1cm}b\hspace{0.1cm}$ of times, so are obtained $\hspace{0.1cm}b\hspace{0.1cm}$ clusters configurations, and with it  $\hspace{0.1cm}b\hspace{0.1cm}$ values of the sum of the intra-clusters variances.


 

- **8) Selecting the optimal clusters configuration**


    - Once the previous steps are done, a total of \hspace{0.1cm}b+2\hspace{0.1cm}$ clusters configurations have been obtained $\hspace{0.1cm}\Rightarrow \hspace{0.1cm} b\hspace{0.1cm}$ in step **5)** , $1$ in step **1)** and $1$ in step **4)**.



    $$\left\lbrace \hspace{0.15cm} V(C_1^{h},....,C_k^h) \hspace{0.12cm} / \hspace{0.12cm} h \in \lbrace 1,...,b+2\rbrace \hspace{0.15cm} \right\rbrace$$




    - The clusters configuration that minimize the sum of intra-clusters variances is selected as optimal configuration.

    -  Formally, the clusters configuration $\hspace{0.1cm} C_1^{h^*},...,C_k^{h^*}$ is selected as optimal.

     - Where:
     
       $$h^*  \hspace{0.15cm}=\hspace{0.12cm} arg \hspace{0.12cm} \underset{h}{Min} \hspace{0.16cm} V(C_1^{h},....,C_k^h)$$ 
    

</p>
 
</p></span>
</div>

 


<br>


***Observations:***

 

- **K-means predictors should be quantitative**


    - Why? \hspace{0.15cm}\Rightarrow\hspace{0.15cm}$ Due to the centroid definition as mean vector. The mean should only be applied to quantitative variables.

<br>


- **What distance metrics should be used in K-means?**

    - Due to K-means is only for quantitative variables, only quantitative distance metric should  be used, such as Euclidea, Minkowski, Canberra, Pearson or Mahalanobis.


<br>

## Clusters interpretation


The mean could be compute for the quantitative variables grouped by each optimal cluster. And the mode for the categorical ones, grouped by the same. Then we can make an interpretation of each cluster comparing these values.



<br>


## Clustering validation algorithms 

One of the most popular clustering validation algorithm is the Silhouette.

This topic will be seen in an specific article about clustering validation.



<br>


## Ajuste del hiper-parametro k

An optimal fit of the hyper-parameter $k$ can be done using the Silhouette algorithm.

As before, this question will be addressed in future articles.

 


<br>



## K-means programmed in `Python` 

 
 
 
 
 
 
 
<br>



## K-means with `sklearn`
 
 
 
 
 
 
<br>
 
 
 
# Bibliographi

- Park and Jun paper
 
 
 








