[["index.html", "K-means Chapter 1 Introduction", " K-means Fabio Scielzo Ortiz 2023-03-26 Chapter 1 Introduction More articles: \\(\\hspace{0.1cm}\\) Estadistica4all Author: \\(\\hspace{0.1cm}\\) Fabio Scielzo Ortiz It`s recommended to open the article on a computer, tablet or on a phone in desktop version. "],["unsupervised-classification-problem.html", "Chapter 2 Unsupervised Classification Problem", " Chapter 2 Unsupervised Classification Problem An unsupervised classification problem is an statistical problem that consist on predict a categorical response variable, using only information about ones predictor variables, because there isn´t data about the response. Unsupervised classification problems are also called clustering problems. We have \\(\\hspace{0.02cm}p\\hspace{0.02cm}\\) predictors \\(\\hspace{0.02cm}(\\mathcal{X}_1 ,...,\\mathcal{X}_p)\\hspace{0.02cm}\\) and a categorical response variable \\(\\hspace{0.02cm}\\mathcal{Y}\\hspace{0.02cm}\\) for which we haven’t any data sample. We have an \\(\\hspace{0.02cm}n\\hspace{0.02cm}\\) size sample of the \\(\\hspace{0.02cm}p\\hspace{0.02cm}\\) predictors \\(\\hspace{0.02cm}X_1,...,X_p\\hspace{0.02cm}\\) but we haven’t a sample for the response, and we also don’t know it’s range. Therefore, we have the following data matrix: \\[D=[X_1,...,X_p]=\\begin{pmatrix} x_{11}&amp;x_{12}&amp;...&amp;x_{1p} \\\\ x_{21}&amp;x_{22}&amp;...&amp;x_{2p} \\\\ ... &amp;...&amp; ...&amp;...\\\\ x_{n1}&amp;x_{n2}&amp;...&amp;x_{np} \\end{pmatrix} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ ...\\\\ x_{n} \\end{pmatrix}\\] The goal is tio predict the response variable \\(\\hspace{0.02cm}\\mathcal{Y}\\hspace{0.02cm}\\) for any observations vector of the predictors, using only the available sample information about the predictors. "],["k-means.html", "Chapter 3 k-means 3.1 Interpretación de los clusters 3.2 Validación de algoritmos de clustering 3.3 Ajuste del hiper-parametro k 3.4 K-means programmed in Python 3.5 K-means with sklearn", " Chapter 3 k-means K-means is an unsupervised classification algorithm, in the sense that it allows solving unsupervised classification problems. Next, a formal description of the algorithm will be made. But first, it should be noted that there are several versions of K-means, and here, the version proposed by Park and Jun in the paper referenced in the bibliography, will be taken . K-means It’s considered that the response variable \\(\\hspace{0.1cm}\\mathcal{Y}\\hspace{0.1cm}\\) has \\(\\hspace{0.1cm} k\\hspace{0.1cm}\\) categories \\(\\hspace{0.1cm}g_1,...,g_k\\) Initial clusters definition. The available observations of the predictors are randomly assigned to one of the response categories, this is how the the initial clusters \\(\\hspace{0.1cm} C_1,...,C_k\\hspace{0.1cm}\\) are built. The clusters must be of similar size, all the observations in the sample must belong to some cluster, and two different clusters must not contain the same observation. Formally, the clusters will be defined as follows: \\[C_r = \\Bigl( \\hspace{0.12cm} x_i^t = (x_{i1},...,x_{ip}) \\hspace{0.2cm}: \\hspace{0.2cm} i \\in I_r \\subset \\lbrace 1,...,n \\rbrace \\hspace{0.12cm} \\Bigr)^t \\\\\\] Where: \\[\\# I(C_r) \\hspace{0.1cm} \\approx \\hspace{0.1cm} \\# I(C_h) \\hspace{0.25cm} , \\hspace{0.25cm} \\forall r\\neq h \\in \\lbrace 1,...,n\\rbrace\\] \\[\\overset{n}{\\underset{r=1}{\\cup}} I(C_r) \\hspace{0.1cm}=\\hspace{0.1cm} \\lbrace 1,...,n\\rbrace \\\\\\] \\[I(C_r) \\cap I(C_h) \\hspace{0.1cm}=\\hspace{0.1cm} \\varnothing \\hspace{0.25cm} , \\hspace{0.25cm} \\forall r\\neq h \\in \\lbrace 1,...,n\\rbrace\\] $I_r$ is randomly defined in this first stage. - $I(C_r)\\hspace{0.1cm}$ es el conjunto de indices de las observaciones de los predictores que pertenecen al cluster $\\hspace{0.1cm}C_r$. $\\hspace{0.08cm}$ Por lo que puede verse como el conjunto de los individuos de la muestra que están asociados al cluster $\\hspace{0.1cm} C_r \\\\$ - $C_r\\hspace{0.1cm}$ es un vector cuyas componentes son vectores fila, luego es una matriz. Como además estos vectores fila son observaciones de variables estadisticas, puede verse como una matriz de datos. Computing the centroids of the initial clusters The centroids of the initial clusters are computed. The centroid of the cluster \\(\\hspace{0.1cm} C_r\\hspace{0.1cm}\\) is defined in K-means as the mean of the cluster observation. if the observation are \\(\\hspace{0.1cm}p\\)-dimensional, the centroid will be a mean vector. The centroid of the cluster \\(\\hspace{0.1cm}C_r\\hspace{0.1cm}\\) is defined as \\(\\hspace{0.2cm}\\overline{x}_{C_r} = \\left( \\overline{X \\hspace{0.1cm}}_{1, C_r} ,...,\\overline{X\\hspace{0.1cm}}_{p, C_r} \\right)\\) Where: \\[X_{j , C_r } = (\\hspace{0.1cm} x_{ij} \\hspace{0.1cm}/\\hspace{0.1cm} i\\in I_r \\hspace{0.1cm}) \\hspace{0.2cm} , \\hspace{0.2cm} \\forall j\\in \\lbrace 1,...,p \\rbrace\\] is the sample observations if the variable \\(\\hspace{0.1cm}\\mathcal{X}_j\\hspace{0.1cm}\\) that belong to the cluster \\(\\hspace{0.1cm}C_r\\) \\(\\\\[0.8cm]\\) \\(\\hspace{1cm}\\) Therefore, \\(\\hspace{0.1cm}\\overline{X \\hspace{0.1cm}}_{j, C_r}\\hspace{0.1cm}\\) is the mean of the observations of \\(\\hspace{0.1cm}\\mathcal{X}_j\\hspace{0.1cm}\\) that belong to the cluster \\(\\hspace{0.1cm}C_r\\) . Computing the sum of intra-clusters variances Given a distance measure \\(\\hspace{0.1cm}\\delta\\hspace{0.1cm}\\) defined for each pair \\(\\hspace{0.1cm}(x_i,x_r)\\hspace{0.1cm}\\) of observations of the predictors, the sum if intra-clusters variances for the clusters configuration \\(\\hspace{0.1cm}C_1,...,C_k\\hspace{0.1cm}\\) is defined as follows: \\[V(C_1,...,C_k) = \\sum_{r=1}^{k} \\hspace{0.2cm} \\sum_{i \\in I_r} \\hspace{0.2cm} \\delta(x_i , \\overline{x}_{C_r}) \\\\\\] \\(\\hspace{0.12cm}V(C_1,...,C_k)\\hspace{0.12cm}\\), is the metric based on which the stopping criteria will be defined. Measures how similar are the observations that belong to the same cluster. The lower it is, the more similar they are, and vice versa. Reassignment of observations to clusters \\(\\hspace{0.12cm} \\delta(x_i , \\overline{x}_{C_r}) \\hspace{0.02cm}\\) is computed. \\(\\hspace{0.12cm}x_i\\hspace{0.12cm}\\) is reassigned to it’s nearest cluster. Formally, \\(\\hspace{0.12cm}x_i\\hspace{0.12cm}\\) is reassigned to the cluster \\(\\hspace{0.12cm}C_{r^*}\\) Where: \\[r^* \\hspace{0.12cm}=\\hspace{0.12cm} arg \\hspace{0.12cm} \\underset{r}{Min} \\hspace{0.16cm} \\delta(x_i , \\overline{x}_{C_r} )\\] Then, \\(\\hspace{0.12cm}x_i\\hspace{0.12cm}\\) es is reassigned to the cluster \\(\\hspace{0.12cm}C_{r^*}\\hspace{0.12cm}\\), which could be the same cluster that it already belonged to, or not. Once all the observations \\(\\hspace{0.12cm}x_1,...,x_n\\hspace{0.12cm}\\) have been reassigned, a new configuration of clusters \\(\\hspace{0.12cm}C_1^{1 },... is obtained. ,C_k^{1 }\\hspace{0.12cm}\\) , which, in general, will be different from the previous one. The superscript 1 indicates that this new cluster configuration was obtained in the first iteration of the algorithm. Computing the centroids of the new clusters The centroids of the new clusters are computed : \\[\\overline{x}_{C_r^1} = \\left( \\overline{X \\hspace{0.1cm}}_{1, C_r^1} ,...,\\overline{X\\hspace{0.1cm}}_{p, C_r^1} \\right)\\] for \\(\\hspace{0.1cm} r\\in \\lbrace 1,..., k \\rbrace\\) Computing the sum of the intra-clusters variances for the new clusters The sum of intra-clusters variances is competed for the new clusters configuration, that was obtained in the previous step: \\[V(C_1^{1 },...,C_k^{1 })\\] Iterate steps 4) , 5) and 6) Steps 4) , 5) and 6) are iterate a number \\(\\hspace{0.1cm}b\\hspace{0.1cm}\\) of times, so are obtained \\(\\hspace{0.1cm}b\\hspace{0.1cm}\\) clusters configurations, and with it \\(\\hspace{0.1cm}b\\hspace{0.1cm}\\) values of the sum of the intra-clusters variances. 8) Selecting the optimal clusters configuration Once the previous steps are done, a total of b+2$ clusters configurations have been obtained \\(\\hspace{0.1cm}\\Rightarrow \\hspace{0.1cm} b\\hspace{0.1cm}\\) in step 5) , \\(1\\) in step 1) and \\(1\\) in step 4). \\[\\left\\lbrace \\hspace{0.15cm} V(C_1^{h},....,C_k^h) \\hspace{0.12cm} / \\hspace{0.12cm} h \\in \\lbrace 1,...,b+2\\rbrace \\hspace{0.15cm} \\right\\rbrace\\] The clusters configuration that minimize the sum of intra-clusters variances is selected as optimal configuration. Formally, the clusters configuration \\(\\hspace{0.1cm} C_1^{h^*},...,C_k^{h^*}\\) is selected as optimal. Where: \\[h^* \\hspace{0.15cm}=\\hspace{0.12cm} arg \\hspace{0.12cm} \\underset{h}{Min} \\hspace{0.16cm} V(C_1^{h},....,C_k^h)\\] Observations: Los predictores en k-medias deben ser cuantitativos ¿Por qué? \\(\\hspace{0.15cm}\\Rightarrow\\hspace{0.15cm}\\) Por como se definen los centroides, son medias de variables, y la media solo deberia aplicarse a variables cuantitativas. ¿Qué medidas de distancias pueden usarse en k-medias ? Como k-medias solo debe usarse con variables cuantitativas, solo deben usarse distancias adecuadas para variables cuantitativas. Por tanto, ejemplos de distancias que podrían usarse son la Euclidea, Minkowski, Canberra, Pearson y Mahalanobis. 3.1 Interpretación de los clusters 3.2 Validación de algoritmos de clustering Uno de los métodos de validación de algoritmos de clustering más populares es el método silhouette. Se tratará este tema en un articulo sobre métodos de validación de modelos de aprendizaje no supervisado (modelos de clustering). 3.3 Ajuste del hiper-parametro k Usando el método silhouette se puede realizar una selección óptima del hiper-parametro \\(k\\) Se tratará este tema en un artículo sobre ajuste de hiper-parametros de modelos de aprendizaje no supervisado (modelos de clustering). 3.4 K-means programmed in Python 3.5 K-means with sklearn "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
