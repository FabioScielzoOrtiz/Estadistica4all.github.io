[["index.html", "K-means Chapter 1 Introduction", " K-means Fabio Scielzo Ortiz 2023-03-27 Chapter 1 Introduction More articles: \\(\\hspace{0.1cm}\\) Estadistica4all Author: \\(\\hspace{0.1cm}\\) Fabio Scielzo Ortiz It’s recommended to open the article on a computer, tablet or on a phone in desktop version. "],["unsupervised-classification-problem.html", "Chapter 2 Unsupervised Classification Problem", " Chapter 2 Unsupervised Classification Problem An unsupervised classification problem is an statistical problem that consist on predict a categorical response variable, using only information about ones predictor variables, because there isn´t data about the response. Unsupervised classification problems are also called clustering problems. We have \\(\\hspace{0.02cm}p\\hspace{0.02cm}\\) predictors \\(\\hspace{0.02cm}\\mathcal{X}_1 ,...,\\mathcal{X}_p\\hspace{0.02cm}\\) and a categorical response variable \\(\\hspace{0.02cm}\\mathcal{Y}\\hspace{0.02cm}\\) for which we haven’t any data sample. We have an \\(\\hspace{0.02cm}n\\hspace{0.02cm}\\) size sample of the \\(\\hspace{0.02cm}p\\hspace{0.02cm}\\) predictors \\(\\hspace{0.02cm}X_1,...,X_p\\hspace{0.02cm}\\) but we haven’t a sample for the response, and we also don’t know it’s range. Therefore, we have the following data matrix: \\[D=[X_1,...,X_p]=\\begin{pmatrix} x_{11}&amp;x_{12}&amp;...&amp;x_{1p} \\\\ x_{21}&amp;x_{22}&amp;...&amp;x_{2p} \\\\ ... &amp;...&amp; ...&amp;...\\\\ x_{n1}&amp;x_{n2}&amp;...&amp;x_{np} \\end{pmatrix} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ ...\\\\ x_{n} \\end{pmatrix}\\\\\\] The goal is to predict the response variable \\(\\hspace{0.02cm}\\mathcal{Y}\\hspace{0.02cm}\\) for any observations vector of the predictors, using only the available sample information about the predictors. "],["k-means.html", "Chapter 3 K-means 3.1 K-means algorithm 3.2 Clusters interpretation 3.3 Clustering validation algorithms 3.4 Ajuste del hiper-parametro k 3.5 K-means programmed in Python 3.6 K-means with sklearn", " Chapter 3 K-means K-means is an unsupervised classification algorithm, in the sense that it allows solving unsupervised classification problems. Next, a formal description of the algorithm will be made. But first, it should be noted that there are several versions of K-means, and here, the version proposed by Park and Jun in the paper referenced in the bibliography, will be taken . 3.1 K-means algorithm The response variable \\(\\hspace{0.02cm}\\mathcal{Y}\\hspace{0.02cm}\\) is considered to have \\(\\hspace{0.02cm} k\\hspace{0.02cm}\\) categories \\(\\hspace{0.02cm } g_1,...,g_k\\hspace{0.01 cm}\\) . 1) Initial clusters definition. The available observations of the predictors are randomly assigned to one of the response categories, this is how the the initial clusters \\(\\hspace{0.1cm} C_1,...,C_k\\hspace{0.1cm}\\) are built. The clusters must be of similar size, all the observations in the sample must belong to some cluster, and two different clusters must not contain the same observation. Formally, the cluster \\(C_r\\) will be defined as follows: \\[C_r = \\Bigl( \\hspace{0.12cm} x_i^t = (x_{i1},...,x_{ip}) \\hspace{0.2cm}: \\hspace{0.2cm} i \\in I_r \\subset \\lbrace 1,...,n \\rbrace \\hspace{0.12cm} \\Bigr)^t\\] where: \\(\\# I_r \\hspace{0.1cm} \\approx \\hspace{0.1cm} \\# I_h \\hspace{0.25cm} , \\hspace{0.25cm} \\forall r\\neq h \\in \\lbrace 1,...,n\\rbrace\\) \\(\\overset{n}{\\underset{r=1}{\\cup}} I_r \\hspace{0.1cm}=\\hspace{0.1cm} \\lbrace 1,...,n\\rbrace\\) \\(I_r \\cap I_h \\hspace{0.1cm}=\\hspace{0.1cm} \\varnothing \\hspace{0.25cm} , \\hspace{0.25cm} \\forall r\\neq h \\in \\lbrace 1,...,n\\rbrace\\) \\(I_r\\hspace{0.05cm}\\) is the set with the index of the predictors observations that belongs to the cluster \\(\\hspace{0.02cm}C_r\\hspace{0.02cm}\\). So, it can be seen as the set of the individuals associated to the cluster \\(\\hspace{0.02cm} C_r\\) . \\(I_r\\hspace{0.05cm}\\) is randomly defined in this first stage. \\(C_r\\hspace{0.05cm}\\) is a vector whose components are row vectors, so, is a matrix. As in addition that vectors are observations of the predictors, it can be seen as a data matrix. 2) Computing the centroids of the initial clusters. In this step,the centroids of the initial clusters are computed. The centroid of the cluster \\(\\hspace{0.02cm} C_r\\hspace{0.02cm}\\) is defined in K-means as the mean of the cluster observations. So, if the observation are \\(\\hspace{0.02cm}p\\)-dimensional, the centroid will be a mean vector. Formally, the centroid of the cluster \\(\\hspace{0.02cm}C_r\\hspace{0.02cm}\\) is defined as \\(\\hspace{0.05cm}\\overline{x}_{C_r} = \\left(\\hspace{0.04cm} \\overline{X \\hspace{0.1cm}}_{1, C_r} ,...,\\overline{X\\hspace{0.1cm}}_{p, C_r}\\hspace{0.02cm} \\right)\\) where: \\[X_{j , C_r } = \\bigl(\\hspace{0.1cm} x_{ij} \\hspace{0.15cm}:\\hspace{0.15cm} i\\in I_r \\hspace{0.1cm} \\bigr) \\hspace{0.3cm} , \\hspace{0.3cm} \\forall j\\in \\lbrace 1,...,p \\rbrace\\] is the sample of observations of the variable \\(\\hspace{0.02cm}\\mathcal{X}_j\\hspace{0.02cm}\\) that belong to the cluster \\(\\hspace{0.02cm}C_r\\hspace{0.02cm}\\). Therefore, \\(\\hspace{0.02cm}\\overline{X \\hspace{0.01cm}}_{j, C_r}\\hspace{0.04cm}\\) is the mean of the observations of \\(\\hspace{0.02cm}\\mathcal{X}_j\\hspace{0.02cm}\\) that belong to the cluster \\(\\hspace{0.02cm}C_r\\) . 3) Computing the sum of intra-clusters variances. Given a distance measure \\(\\hspace{0.02cm}\\delta\\hspace{0.02cm}\\) defined for each pair \\(\\hspace{0.2cm}(x_i,x_r)\\hspace{0.2cm}\\) of observations of the predictors, the sum of intra-clusters variances for the clusters configuration \\(\\hspace{0.02cm}C_1,...,C_k\\hspace{0.02cm}\\) is defined as follows: \\[V(C_1,...,C_k) \\hspace{0.1cm}= \\hspace{0.1cm} \\sum_{r=1}^{k} \\hspace{0.2cm} \\sum_{i \\in I_r} \\hspace{0.2cm} \\delta(x_i , \\overline{x}_{C_r}) \\\\\\] \\(V(C_1,...,C_k)\\hspace{0.12cm}\\), is the metric based on which the stopping criteria will be defined. It measures how similar are the observations that belong to a same cluster. The lower it is, the more similar they are, and vice versa. 4) Reassignment of observations to clusters. \\(\\hspace{0.03cm} \\delta(x_i , \\overline{x}_{C_r}) \\hspace{0.02cm}\\) is computed. \\(\\hspace{0.03cm}x_i\\hspace{0.12cm}\\) is reassigned to it’s nearest cluster. Formally, \\(\\hspace{0.02cm}x_i\\hspace{0.02cm}\\) is reassigned to the cluster \\(\\hspace{0.02cm}C_{r^*}\\) where: \\[r^* \\hspace{0.12cm}=\\hspace{0.12cm} arg \\hspace{0.14cm} \\underset{r }{Min} \\hspace{0.16cm} \\delta(x_i , \\overline{x}_{C_r} ) \\hspace{0.3cm} , \\hspace{0.3cm} h \\in \\lbrace 1,..., k \\rbrace\\] Then, \\(\\hspace{0.02cm}x_i\\hspace{0.02cm}\\) es is reassigned to the cluster \\(\\hspace{0.02cm}C_{r^*}\\hspace{0.02cm}\\), which could be the same cluster that it already belonged to, or not. Once all the observations \\(\\hspace{0.02cm}x_1,...,x_n\\hspace{0.02cm}\\) have been reassigned, a new configuration of clusters \\(\\hspace{0.02cm}C_1^{1 },... ,C_k^{1 }\\hspace{0.02cm}\\) is obtained. , which, in general, will be different from the previous one. The superscript 1 indicates that this new cluster configuration was obtained in the first iteration of the algorithm. 5) Computing the centroids of the new clusters. The centroids of the new clusters are computed: \\[\\overline{x}_{C_r^1} \\hspace{0.1cm}=\\hspace{0.1cm} \\left(\\hspace{0.04cm} \\overline{X \\hspace{0.01cm}}_{1, C_r^1} ,...,\\overline{X\\hspace{0.01cm}}_{p, C_r^1} \\right) \\hspace{0.3cm} , \\hspace{0.3cm} r\\in \\lbrace 1,..., k \\rbrace\\] 6) Computing the sum of the intra-clusters variances for the new clusters. The sum of intra-clusters variances is competed for the new clusters configuration, that was obtained in the previous step: \\[V(C_1^{1 },...,C_k^{1 })\\] 7) Iterate steps 4) , 5) and 6). Steps 4) , 5) and 6) are iterate a \\(\\hspace{0.02cm}b\\hspace{0.02cm}\\) times, so are obtained \\(\\hspace{0.02cm}b\\hspace{0.02cm}\\) clusters configurations, and with it \\(\\hspace{0.02cm}b\\hspace{0.02cm}\\) values of the sum of the intra-clusters variances. 8) Selecting the optimal clusters configuration Once the previous steps are done, a total of \\(\\hspace{0.02cm}b+2\\hspace{0.02cm}\\) clusters configurations have been obtained \\(\\hspace{0.1cm}\\Rightarrow \\hspace{0.1cm} b\\hspace{0.1cm}\\) in step 5) , \\(1\\) in step 1) and \\(1\\) in step 4): \\[\\Bigl\\lbrace \\hspace{0.1cm} V(C_1^{h},....,C_k^h) \\hspace{0.15cm} : \\hspace{0.15cm} h \\in \\lbrace 1,...,b+2\\rbrace \\hspace{0.1cm} \\Bigr\\rbrace\\] The clusters configuration that minimize the sum of intra-clusters variances is selected as optimal configuration. Formally, the clusters configuration \\(\\hspace{0.1cm} C_1^{h^*},...,C_k^{h^*}\\) is selected as optimal. where: $\\(h^* \\hspace{0.15cm}=\\hspace{0.12cm} arg \\hspace{0.12cm} \\underset{h }{Min} \\hspace{0.16cm} V(C_1^{h},....,C_k^h) \\hspace{0.3cm} , \\hspace{0.3cm} h \\in \\lbrace 1,..., b+2 \\rbrace\\) Observations: K-means predictors should be quantitative Why? $ Due to the centroid definition as mean vector. The mean should only be applied to quantitative variables. What distance metrics should be used in K-means? Due to K-means is only for quantitative variables, only quantitative distance metric should be used, such as Euclidea, Minkowski, Canberra, Pearson or Mahalanobis. 3.2 Clusters interpretation The mean could be compute for the quantitative variables grouped by each optimal cluster. And the mode for the categorical ones, grouped by the same. Then we can make an interpretation of each cluster comparing these values. 3.3 Clustering validation algorithms One of the most popular clustering validation algorithm is the Silhouette. This topic will be seen in an specific article about clustering validation. 3.4 Ajuste del hiper-parametro k An optimal fit of the hyper-parameter \\(k\\) can be done using the Silhouette algorithm. As before, this question will be addressed in future articles. 3.5 K-means programmed in Python 3.6 K-means with sklearn "],["bibliographi.html", "Chapter 4 Bibliographi", " Chapter 4 Bibliographi Park and Jun paper "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
