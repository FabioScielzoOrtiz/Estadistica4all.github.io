<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 k-means | K-means</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 k-means | K-means" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 k-means | K-means" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="Fabio Scielzo Ortiz" />


<meta name="date" content="2023-03-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="unsupervised-classification-problem.html"/>
<link rel="next" href="bibliographi.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">K-Means</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="unsupervised-classification-problem.html"><a href="unsupervised-classification-problem.html"><i class="fa fa-check"></i><b>2</b> Unsupervised Classification Problem</a></li>
<li class="chapter" data-level="3" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>3</b> k-means</a>
<ul>
<li class="chapter" data-level="3.1" data-path="k-means.html"><a href="k-means.html#clusters-interpretation"><i class="fa fa-check"></i><b>3.1</b> Clusters interpretation</a></li>
<li class="chapter" data-level="3.2" data-path="k-means.html"><a href="k-means.html#clustering-validation-algorithms"><i class="fa fa-check"></i><b>3.2</b> Clustering validation algorithms</a></li>
<li class="chapter" data-level="3.3" data-path="k-means.html"><a href="k-means.html#ajuste-del-hiper-parametro-k"><i class="fa fa-check"></i><b>3.3</b> Ajuste del hiper-parametro k</a></li>
<li class="chapter" data-level="3.4" data-path="k-means.html"><a href="k-means.html#k-means-programmed-in-python"><i class="fa fa-check"></i><b>3.4</b> K-means programmed in <code>Python</code></a></li>
<li class="chapter" data-level="3.5" data-path="k-means.html"><a href="k-means.html#k-means-with-sklearn"><i class="fa fa-check"></i><b>3.5</b> K-means with <code>sklearn</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bibliographi.html"><a href="bibliographi.html"><i class="fa fa-check"></i><b>4</b> Bibliographi</a></li>
<li class="divider"></li>
<li><a href="https://estadistica4all.com" target="blank">Estadistica4all.com</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">K-means</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="k-means" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> k-means<a href="k-means.html#k-means" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>K-means is an unsupervised classification algorithm, in the sense that it allows solving unsupervised classification problems.</p>
<p>Next, a formal description of the algorithm will be made. But first, it should be noted that there are several versions of K-means, and here, the version proposed by Park and Jun in the paper referenced in the bibliography, will be taken .</p>
<p><br></p>
<p><strong>K-means</strong></p>
<div class="warning" style="background-color:#F7EBE8; color: #030000; border-left: solid #CA0B0B 7px; border-radius: 3px; size:1px ; padding:0.1em;">
<p><span></p>
<ul>
<li><p>It’s considered that the response variable <span class="math inline">\(\hspace{0.1cm}\mathcal{Y}\hspace{0.1cm}\)</span> has <span class="math inline">\(\hspace{0.1cm} k\hspace{0.1cm}\)</span> categories <span class="math inline">\(\hspace{0.1cm}g_1,...,g_k\)</span></p></li>
<li><p><strong>Initial clusters definition.</strong></p></li>
<li><p>The available observations of the predictors are randomly assigned to one of the response categories, this is how the the initial clusters <span class="math inline">\(\hspace{0.1cm} C_1,...,C_k\hspace{0.1cm}\)</span> are built.</p>
<ul>
<li>The clusters must be of similar size, all the observations in the sample must belong to some cluster, and two different clusters must not contain the same observation.</li>
</ul>
<p>Formally, the clusters will be defined as follows:</p>
<p><span class="math display">\[C_r = \Bigl( \hspace{0.12cm} x_i^t = (x_{i1},...,x_{ip}) \hspace{0.2cm}: \hspace{0.2cm} i \in I_r \subset \lbrace 1,...,n \rbrace \hspace{0.12cm} \Bigr)^t \\\]</span></p>
<p>Where:</p>
<p><span class="math display">\[\# I(C_r) \hspace{0.1cm} \approx \hspace{0.1cm} \# I(C_h) \hspace{0.25cm} , \hspace{0.25cm}  \forall r\neq h \in \lbrace 1,...,n\rbrace\]</span></p>
<p><span class="math display">\[\overset{n}{\underset{r=1}{\cup}}  I(C_r) \hspace{0.1cm}=\hspace{0.1cm} \lbrace 1,...,n\rbrace \\\]</span></p>
<p><span class="math display">\[I(C_r) \cap I(C_h) \hspace{0.1cm}=\hspace{0.1cm} \varnothing \hspace{0.25cm} , \hspace{0.25cm}  \forall r\neq h \in \lbrace 1,...,n\rbrace\]</span></p>
<pre><code>$I_r$ is randomly defined in this first stage. 



- $I(C_r)\hspace{0.1cm}$ es el conjunto de indices de las observaciones de los predictores que pertenecen al cluster $\hspace{0.1cm}C_r$. $\hspace{0.08cm}$ Por lo que puede verse como el conjunto de los individuos de la muestra que están asociados al cluster $\hspace{0.1cm} C_r \\$


-  $C_r\hspace{0.1cm}$ es un vector cuyas componentes son vectores fila, luego es una matriz. Como además estos vectores fila son observaciones de variables estadisticas, puede verse como una matriz de datos. </code></pre></li>
<li><p><strong>Computing the centroids of the initial clusters</strong></p>
<ul>
<li>The centroids of the initial clusters are computed.</li>
</ul>
<p>The <strong>centroid</strong> of the cluster <span class="math inline">\(\hspace{0.1cm} C_r\hspace{0.1cm}\)</span> is defined in K-means as the mean of the cluster observation. if the observation are <span class="math inline">\(\hspace{0.1cm}p\)</span>-dimensional, the centroid will be a mean vector.</p>
<p>The <strong>centroid</strong> of the cluster <span class="math inline">\(\hspace{0.1cm}C_r\hspace{0.1cm}\)</span> is defined as <span class="math inline">\(\hspace{0.2cm}\overline{x}_{C_r} = \left( \overline{X \hspace{0.1cm}}_{1, C_r} ,...,\overline{X\hspace{0.1cm}}_{p, C_r} \right)\)</span></p>
<p>Where:</p>
<p><span class="math display">\[X_{j , C_r } = (\hspace{0.1cm} x_{ij} \hspace{0.1cm}/\hspace{0.1cm} i\in I_r \hspace{0.1cm})  \hspace{0.2cm} , \hspace{0.2cm} \forall j\in \lbrace 1,...,p \rbrace\]</span></p>
<p>is the sample observations if the variable <span class="math inline">\(\hspace{0.1cm}\mathcal{X}_j\hspace{0.1cm}\)</span> that belong to the cluster <span class="math inline">\(\hspace{0.1cm}C_r\)</span> <span class="math inline">\(\\[0.8cm]\)</span></p>
<p><span class="math inline">\(\hspace{1cm}\)</span> Therefore, <span class="math inline">\(\hspace{0.1cm}\overline{X \hspace{0.1cm}}_{j, C_r}\hspace{0.1cm}\)</span> is the mean of the observations of <span class="math inline">\(\hspace{0.1cm}\mathcal{X}_j\hspace{0.1cm}\)</span> that belong to the cluster <span class="math inline">\(\hspace{0.1cm}C_r\)</span> .</p></li>
<li><p><strong>Computing the sum of intra-clusters variances</strong></p>
<p>Given a distance measure <span class="math inline">\(\hspace{0.1cm}\delta\hspace{0.1cm}\)</span> defined for each pair <span class="math inline">\(\hspace{0.1cm}(x_i,x_r)\hspace{0.1cm}\)</span> of observations of the predictors, the <strong>sum if intra-clusters variances</strong> for the clusters configuration <span class="math inline">\(\hspace{0.1cm}C_1,...,C_k\hspace{0.1cm}\)</span> is defined as follows:</p>
<p><span class="math display">\[V(C_1,...,C_k) = \sum_{r=1}^{k} \hspace{0.2cm} \sum_{i \in I_r} \hspace{0.2cm} \delta(x_i , \overline{x}_{C_r}) \\\]</span></p>
<p><span class="math inline">\(\hspace{0.12cm}V(C_1,...,C_k)\hspace{0.12cm}\)</span>, is the metric based on which the stopping criteria will be defined.</p>
<ul>
<li>Measures how similar are the observations that belong to the same cluster. The lower it is, the more similar they are, and vice versa.</li>
</ul></li>
<li><p><strong>Reassignment of observations to clusters</strong></p>
<ul>
<li><p><span class="math inline">\(\hspace{0.12cm} \delta(x_i , \overline{x}_{C_r}) \hspace{0.02cm}\)</span> is computed.</p></li>
<li><p><span class="math inline">\(\hspace{0.12cm}x_i\hspace{0.12cm}\)</span> is reassigned to it’s nearest cluster.</p></li>
<li><p>Formally, <span class="math inline">\(\hspace{0.12cm}x_i\hspace{0.12cm}\)</span> is reassigned to the cluster <span class="math inline">\(\hspace{0.12cm}C_{r^*}\)</span></p></li>
</ul>
<p>Where:</p>
<p><span class="math display">\[r^* \hspace{0.12cm}=\hspace{0.12cm} arg \hspace{0.12cm} \underset{r}{Min} \hspace{0.16cm} \delta(x_i , \overline{x}_{C_r} )\]</span></p>
<ul>
<li><p>Then, <span class="math inline">\(\hspace{0.12cm}x_i\hspace{0.12cm}\)</span> es is reassigned to the cluster <span class="math inline">\(\hspace{0.12cm}C_{r^*}\hspace{0.12cm}\)</span>, which could be the same cluster that it already belonged to, or not.</p></li>
<li><p>Once all the observations <span class="math inline">\(\hspace{0.12cm}x_1,...,x_n\hspace{0.12cm}\)</span> have been reassigned, a new configuration of clusters <span class="math inline">\(\hspace{0.12cm}C_1^{1 },... is obtained. ,C_k^{1 }\hspace{0.12cm}\)</span> , which, in general, will be different from the previous one.</p></li>
</ul>
<p>The superscript 1 indicates that this new cluster configuration was obtained in the first iteration of the algorithm.</p></li>
<li><p><strong>Computing the centroids of the new clusters</strong></p>
<ul>
<li>The centroids of the new clusters are computed :</li>
</ul>
<p><span class="math display">\[\overline{x}_{C_r^1} = \left( \overline{X \hspace{0.1cm}}_{1, C_r^1} ,...,\overline{X\hspace{0.1cm}}_{p, C_r^1} \right)\]</span></p>
<p>for <span class="math inline">\(\hspace{0.1cm} r\in \lbrace 1,..., k \rbrace\)</span></p></li>
<li><p><strong>Computing the sum of the intra-clusters variances for the new clusters</strong></p>
<ul>
<li>The sum of intra-clusters variances is competed for the new clusters configuration, that was obtained in the previous step:</li>
</ul>
<p><span class="math display">\[V(C_1^{1 },...,C_k^{1 })\]</span></p></li>
<li><p><strong>Iterate steps 4) , 5) and 6)</strong></p>
<ul>
<li>Steps <strong>4)</strong> , <strong>5)</strong> and <strong>6)</strong> are iterate a number <span class="math inline">\(\hspace{0.1cm}b\hspace{0.1cm}\)</span> of times, so are obtained <span class="math inline">\(\hspace{0.1cm}b\hspace{0.1cm}\)</span> clusters configurations, and with it <span class="math inline">\(\hspace{0.1cm}b\hspace{0.1cm}\)</span> values of the sum of the intra-clusters variances.</li>
</ul></li>
<li><p><strong>8) Selecting the optimal clusters configuration</strong></p>
<ul>
<li>Once the previous steps are done, a total of b+2$ clusters configurations have been obtained <span class="math inline">\(\hspace{0.1cm}\Rightarrow \hspace{0.1cm} b\hspace{0.1cm}\)</span> in step <strong>5)</strong> , <span class="math inline">\(1\)</span> in step <strong>1)</strong> and <span class="math inline">\(1\)</span> in step <strong>4)</strong>.</li>
</ul>
<p><span class="math display">\[\left\lbrace \hspace{0.15cm} V(C_1^{h},....,C_k^h) \hspace{0.12cm} / \hspace{0.12cm} h \in \lbrace 1,...,b+2\rbrace \hspace{0.15cm} \right\rbrace\]</span></p>
<ul>
<li><p>The clusters configuration that minimize the sum of intra-clusters variances is selected as optimal configuration.</p></li>
<li><p>Formally, the clusters configuration <span class="math inline">\(\hspace{0.1cm} C_1^{h^*},...,C_k^{h^*}\)</span> is selected as optimal.</p></li>
<li><p>Where:</p>
<p><span class="math display">\[h^*  \hspace{0.15cm}=\hspace{0.12cm} arg \hspace{0.12cm} \underset{h}{Min} \hspace{0.16cm} V(C_1^{h},....,C_k^h)\]</span></p></li>
</ul></li>
</ul>
</p>
</p>
<p></span></p>
</div>
<p><br></p>
<p><strong><em>Observations:</em></strong></p>
<ul>
<li><p><strong>K-means predictors should be quantitative</strong></p>
<ul>
<li>Why? $ Due to the centroid definition as mean vector. The mean should only be applied to quantitative variables.</li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p><strong>What distance metrics should be used in K-means?</strong></p>
<ul>
<li>Due to K-means is only for quantitative variables, only quantitative distance metric should be used, such as Euclidea, Minkowski, Canberra, Pearson or Mahalanobis.</li>
</ul></li>
</ul>
<p><br></p>
<div id="clusters-interpretation" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Clusters interpretation<a href="k-means.html#clusters-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The mean could be compute for the quantitative variables grouped by each optimal cluster. And the mode for the categorical ones, grouped by the same. Then we can make an interpretation of each cluster comparing these values.</p>
<p><br></p>
</div>
<div id="clustering-validation-algorithms" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Clustering validation algorithms<a href="k-means.html#clustering-validation-algorithms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the most popular clustering validation algorithm is the Silhouette.</p>
<p>This topic will be seen in an specific article about clustering validation.</p>
<p><br></p>
</div>
<div id="ajuste-del-hiper-parametro-k" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Ajuste del hiper-parametro k<a href="k-means.html#ajuste-del-hiper-parametro-k" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An optimal fit of the hyper-parameter <span class="math inline">\(k\)</span> can be done using the Silhouette algorithm.</p>
<p>As before, this question will be addressed in future articles.</p>
<p><br></p>
</div>
<div id="k-means-programmed-in-python" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> K-means programmed in <code>Python</code><a href="k-means.html#k-means-programmed-in-python" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><br></p>
</div>
<div id="k-means-with-sklearn" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> K-means with <code>sklearn</code><a href="k-means.html#k-means-with-sklearn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><br></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="unsupervised-classification-problem.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliographi.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://estadistica4all.com/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
