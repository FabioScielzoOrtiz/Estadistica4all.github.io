{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('House_Price_Regression.csv')\n",
    "\n",
    "Data = Data.loc[:, ['latitude', 'longitude', 'price', 'size_in_m_2']]\n",
    "\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_medoids_Park_Jun(Data, random_seed, k, max_n_iter):\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "# Se generan los clusters iniciales aleatoreamente\n",
    "\n",
    "    sample = resample(range(0, len(Data)), n_samples=len(Data), replace=False, random_state=random_seed)\n",
    "\n",
    "    df_sample = pd.DataFrame({'index': range(0,len(Data)) , 'sample':sample})\n",
    "\n",
    "    Q = []\n",
    "\n",
    "    for q in np.arange(0 , 1 + 1/k , 1/k):\n",
    "\n",
    "        Q.append( np.quantile( range(0, len(Data)) , q ).round(0) )\n",
    "\n",
    "\n",
    "\n",
    "    labels_clusters = np.array([0])\n",
    "\n",
    "    for j in range(0,k):\n",
    "   \n",
    "        labels_clusters = np.concatenate([labels_clusters, np.repeat( j, len( df_sample.loc[Q[j]:(math.floor(Q[j+1])-1), 'sample'] ) ) ] ) \n",
    " \n",
    "\n",
    "    df_cluster = pd.DataFrame({'observacion': df_sample['sample'] , 'cluster': labels_clusters})\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "\n",
    "# calculamos los medoids de los clusters iniciales\n",
    "\n",
    "    M1 = Matrix_Dist_Euclidea(Data)\n",
    "\n",
    "    M2 = M1 + M1.T\n",
    "\n",
    "   ######################\n",
    "\n",
    "    medoids = []\n",
    "\n",
    "    for j in range(0, k):\n",
    "\n",
    "      a = df_cluster.loc[df_cluster.cluster == j ,].observacion\n",
    "\n",
    "      M3 = M2[a, :][: ,a] # matriz distancias para el cluster j\n",
    "\n",
    "      suma_distancias = M3.sum(axis=1) # suma de la matriz por filas\n",
    "\n",
    "      h = np.where( suma_distancias == min( suma_distancias ) ) # indice en la matriz de la observacion que minimiza la suma de distancias en el cluster j --> medoid del cluster j\n",
    "\n",
    "      d = df_cluster.loc[df_cluster.cluster == j , ]    \n",
    "\n",
    "      # d.iloc[h[0],:].observacion indice en el data set original de la observacion medoid del cluster j   \n",
    "\n",
    "      medoids.append( Data[ d.iloc[h[0],:].observacion , : ]  ) # su componente j es el medoid del cluster j\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "\n",
    "   # Calculamos la suma de varianzas intra cluster para los clusters iniciales\n",
    "\n",
    "    lista_suma_varianzas_intra_cluster = []\n",
    "\n",
    "    distancias, varianzas_intra_cluster , suma_varianzas_intra_cluster = [], [], []\n",
    "\n",
    "    for j in range(0,k):\n",
    "\n",
    "        for i in df_cluster.loc[df_cluster.cluster == j, 'observacion']:\n",
    "\n",
    "            distancias.append( Dist_Euclidea(Data[i,:], medoids[j]) )\n",
    "\n",
    "        varianzas_intra_cluster.append( sum(distancias) )\n",
    "\n",
    "\n",
    "    suma_varianzas_intra_cluster.append( sum(varianzas_intra_cluster) )\n",
    "\n",
    "    lista_suma_varianzas_intra_cluster.append( suma_varianzas_intra_cluster )\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "\n",
    "# Asignar cada observacion a su cluster m치s cercano (paso 3-1 del articulo Park y Jun)\n",
    "\n",
    "# En el articulo de Park y Jun no se especifica como hacer esto.\n",
    "\n",
    "# Se me ocurren dos opciones. Una es la expuesta en mi algoritmo de k-means (pasos 4, 5 y 6). \n",
    "\n",
    "# La otra opci칩n es la que creo que est치 m치s cerca de lo que Park y Jun quieren expresar:\n",
    "# 1) Se calculan las distancias entre x_i y cada uno de los medoids de los clusters existentes, \n",
    "#    Se asigna x_i al cluster cuyo medoid minimiza esas distancias.\n",
    "# 2) Con ello se obtiene una nueva configuracion de clusters. Se calcula la suma de varianzas intra cluster de esa nueva configuracion.\n",
    "# Si la suma de varianzas en igual a la anterior, se para. Si es menor, se repite lo anterior (pasos 1) y 2))\n",
    "\n",
    "\n",
    "    n_iter = []\n",
    "\n",
    "    iter_condicion_cluster_vacio = []\n",
    "\n",
    "\n",
    "    for r in range(0 , max_n_iter):\n",
    "\n",
    "        n_iter.append(r)\n",
    "       \n",
    "\n",
    "       # Calculamos los nuevos clusters \n",
    "\n",
    "        labels_clusters = []\n",
    "\n",
    "        for i in df_sample['sample']:\n",
    "\n",
    "            distancias = []\n",
    "    \n",
    "            for j in range(0,k):\n",
    "\n",
    "                distancias.append( Dist_Euclidea(Data[i,:], medoids[j]) )\n",
    "\n",
    "                \n",
    "\n",
    "            df_distancias = pd.DataFrame({'Distancias' : distancias , 'Cluster': range(0,k)})\n",
    "\n",
    "            df_distancias_sort = df_distancias.sort_values(by='Distancias', ascending=True)\n",
    "\n",
    "            labels_clusters.append( df_distancias_sort.iloc[0]['Cluster'] )\n",
    "\n",
    "        \n",
    "        \n",
    "        df_cluster = pd.DataFrame({'observacion' : df_sample['sample'] , 'cluster': labels_clusters})\n",
    "\n",
    "\n",
    "\n",
    "        # Puede ser que se llegue a una configuracion de clusters en la que algun cluster no tiene observaciones\n",
    "        # En ese caso vamos a considerar dos respuestas. O bien se deja que el algoritmo haga una seleccion de k. \n",
    "        # O bien se fuerza a que k sea el fijado, aignando una observacion aleatoriamente a ese cluster vacio.\n",
    "        # Vamos a implementar la segunda.\n",
    "\n",
    "        #for j in range(0,k):\n",
    "\n",
    "            #if len(df_cluster.loc[df_cluster.cluster == j ,]) == 0 :\n",
    "\n",
    "                #iter_condicion_cluster_vacio.append(r)\n",
    "\n",
    "                # Si un cluster esta vacio (no se le ha asignado ningun elemento), se le asigna una observacion al azar\n",
    "                # Y se elimina esa observacion del cluster al que pertenecia.\n",
    "\n",
    "                #m = resample(range(0, len(Data)), n_samples=1 , replace=False)\n",
    "\n",
    "                # Eliminamos la observacion m del cluster al que se le habia asignado \n",
    "\n",
    "                #df_cluster = df_cluster.drop(df_cluster.loc[df_cluster.observacion == m[0] , :].index)\n",
    "\n",
    "                # Asignamos la observacion m al cluster vacio\n",
    "\n",
    "                #df_cluster = pd.concat([df_cluster , pd.Series({'observacion': df_sample.loc[m[0] , 'sample'], 'cluster': j }).to_frame().T ], ignore_index=True)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "\n",
    "       # Calculamos los medoids de los nuevos clusters\n",
    "\n",
    "        medoids = []\n",
    "        label_medoids =[]\n",
    "\n",
    "\n",
    "        for j in range(0, k):                    \n",
    "                \n",
    "                a = df_cluster.loc[df_cluster.cluster == j ,].observacion\n",
    "\n",
    "                M3 = M2[a, :][: ,a] # matriz distancias para el cluster j\n",
    "\n",
    "                suma_distancias = M3.sum(axis=1) # suma de la matriz por filas\n",
    "\n",
    "                h = np.where( suma_distancias == min( suma_distancias) ) # indice en la matriz de la observacion que minimiza la suma de distancias en el cluster j --> medoid del cluster j\n",
    "\n",
    "                d = df_cluster.loc[df_cluster.cluster == j , ]    \n",
    "\n",
    "                # d.iloc[h[0],:].observacion indice en el data set original de la observacion medoid del cluster j   \n",
    "\n",
    "                medoids.append( Data[ d.iloc[h[0],:].observacion , : ]  ) # su componente j es el medoid del cluster j\n",
    "  \n",
    "                label_medoids.append(d.iloc[h[0],:].observacion)\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "        # Calculamos la suma de varianzas intra cluster para los nuevos clusters\n",
    "\n",
    "        distancias, varianzas_intra_cluster , suma_varianzas_intra_cluster = [], [], []\n",
    "\n",
    "        for j in range(0,k):\n",
    "\n",
    "            for i in df_cluster.loc[df_cluster.cluster == j, 'observacion']:\n",
    "\n",
    "                distancias.append( Dist_Euclidea(Data[i,:], medoids[j]) )\n",
    "\n",
    "            varianzas_intra_cluster.append( sum(distancias) )\n",
    "\n",
    "\n",
    "        suma_varianzas_intra_cluster.append( sum(varianzas_intra_cluster) )\n",
    "\n",
    "        lista_suma_varianzas_intra_cluster.append( suma_varianzas_intra_cluster )\n",
    "\n",
    "\n",
    "        #####################\n",
    "\n",
    "        # if lista_suma_varianzas_intra_cluster[r+1] != lista_suma_varianzas_intra_cluster[r] :\n",
    "\n",
    "            # continue\n",
    "\n",
    "        # else :\n",
    "\n",
    "            # break\n",
    "\n",
    "\n",
    "        ###########################################\n",
    "\n",
    "    df_cluster_final = pd.DataFrame({'observaciones': df_sample['sample']  , 'cluster': labels_clusters })\n",
    "\n",
    "\n",
    "\n",
    "    return df_cluster_final, df_cluster, lista_suma_varianzas_intra_cluster, n_iter, iter_condicion_cluster_vacio, label_medoids, medoids\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c63d8c7d738c2960218a10995aedf0a7f67a49a231e71037adf0440953cdb45b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
