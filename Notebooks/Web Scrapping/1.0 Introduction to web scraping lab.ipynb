{"cells":[{"cell_type":"markdown","metadata":{"id":"bVPlvWXyR8dd"},"source":["<img src=\"https://www.uc3m.es/ss/Satellite?blobcol=urldata&blobkey=id&blobtable=MungoBlobs&blobwhere=1371573952659\">\n","\n","---\n","\n","# WEB ANALYTICS COURSE 4 - SEMESTER 2 \n","# BACHELOR IN DATA SCIENCE AND ENGINEERING \n","\n","# LAB 1 WEB SCRAPING \n","\n","---\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":188,"status":"ok","timestamp":1662547940956,"user":{"displayName":"JOSE GONZALEZ CABAÑAS","userId":"18315766743287740804"},"user_tz":-120},"id":"NyadvVmjk_gP","outputId":"b1020ce6-dc40-48c6-e92a-4b0eea19006e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["%cd /content\n","%pwd"]},{"cell_type":"markdown","metadata":{"id":"6PatXYtJE9RI"},"source":["# Introduction to Web Scraping\n","\n","In this notebook, we will learn how to use Python to retrieve information from websites. \n","\n","\n","---\n","\n","**IMPORTANT** You don't have to deliver the answers of this notebook. \n","\n","---\n","\n","\n","\n","Before we start, let's review the HTML structure, as we will navigate through it in our scrapers.\n","\n","### HTML overview\n","\n","**Hypertext Markup Language (HTML)** is the main language used to write/build web pages. HTML describes the structure of a web page and it can be used with **Cascading Style Sheets (CSS)** (to describe the presentation of web pages, including colors, layout, and fonts) and a scripting language such as **JavaScript** (to create interactive websites). \n","\n","**HTML Tags:** \n","\n","HTML is a markup language and makes use of various tags to format the content. These tags are enclosed within angle braces `<Tag Name>`. Except for few tags, HTML tags typically come in pairs like `<p>` and `</p>`. The first tag in a pair is the opening tag, and the second tag is the closing tag. The end tag is written like the start tag but with a slash inserted before the tag name.\n","\n","Some of the tags are:\n","* `<!DOCTYPE html>` defines the document type and the HTML version. Current version of HTML is 5. \n","* `<html>` element is the root element of an HTML page. \n","* `<head>` element contains meta information about the document.  \n","* `<title>` element specifies a title for the document.  \n","* `<body>` element contains the visible page content.  \n","* `<div>` tag defines a division or a section in an HTML document. It's usually a container for other elements.\n","* `<h1>` element defines a large heading.  \n","* `<p>` element defines a paragraph.  \n","* `<a>` element defines a hyperlink. \n","\n","Here you have an example of a simple HTML structure: \n","\n","```\n","<!DOCTYPE html>\n","<html>\n","\n","   <head>\n","      <title>This is document title</title>\n","   </head>\n","\t\n","   <body>\n","      <h1>This is a heading</h1>\n","      <p>This is a paragraph</p>\n","      <a href=\"https://www.uc3m.es/\">This is a link to uc3m website</a>\n","   </body>\n","\t\n","</html>\n","```\n","\n","As you can see, HTML is structured like a tree thanks to the **Document Object Model (DOM)**, a programming API that defines the logical structure of documents and the way a document is accessed and manipulated.\n","\n","For a complete overview of HTML, check out the official documentation at w3schools: [Documentation](https://www.w3schools.com/html/)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ouLRQumZJ6IQ"},"source":["### Short exercise\n","To get some experience with the HTML page structure, we will search and locate elements in [IMDB](https://www.imdb.com/). \n","\n","**Tip**: Remember to use the web browser developer tool. Click right button and select *Inspect* to access the elements panel. "]},{"cell_type":"markdown","metadata":{"id":"CxOczcyoJ22P"},"source":["\n","* Find the _Sign in_ button. Write here the corresponding HTML code.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUUG11SGJyOE"},"outputs":[],"source":["<a class=\"ipc-button ipc-button--single-padding ipc-button--center-align-content ipc-button--default-height ipc-button--core-baseAlt ipc-button--theme-baseAlt ipc-button--on-textPrimary ipc-text-button imdb-header__signin-text\" role=\"button\" tabindex=\"0\" aria-disabled=\"false\" href=\"/registration/signin?ref=nv_generic_lgin\"><div class=\"ipc-button__text\">Sign In</div></a>"]},{"cell_type":"markdown","metadata":{"id":"ATQVc_TskSa2"},"source":["* Find the document title. Write here the corresponding HTML code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXNYW77WKEro"},"outputs":[],"source":["<title>IMDb: Ratings, Reviews, and Where to Watch the Best Movies &amp; TV Shows</title>"]},{"cell_type":"markdown","metadata":{"id":"ZSyk_ZGzJ-7N"},"source":["* Find the IMDB logo. Write here the corresponding HTML code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n5FttHXfJ-Dr"},"outputs":[],"source":["<a class=\"NavLogo-e02kni-0 cXfGIs imdb-header__logo-link _3XaDsUnZG7ZfFqFF37dZPv\" id=\"home_img_holder\" href=\"/?ref_=nv_home\" aria-label=\"Home\"><svg id=\"home_img\" class=\"ipc-logo\" xmlns=\"http://www.w3.org/2000/svg\" width=\"64\" height=\"32\" viewBox=\"0 0 64 32\" version=\"1.1\"><g fill=\"#F5C518\"><rect x=\"0\" y=\"0\" width=\"100%\" height=\"100%\" rx=\"4\"></rect></g><g transform=\"translate(8.000000, 7.000000)\" fill=\"#000000\" fill-rule=\"nonzero\"><polygon points=\"0 18 5 18 5 0 0 0\"></polygon><path d=\"M15.6725178,0 L14.5534833,8.40846934 L13.8582008,3.83502426 C13.65661,2.37009263 13.4632474,1.09175121 13.278113,0 L7,0 L7,18 L11.2416347,18 L11.2580911,6.11380679 L13.0436094,18 L16.0633571,18 L17.7583653,5.8517865 L17.7707076,18 L22,18 L22,0 L15.6725178,0 Z\"></path><path d=\"M24,18 L24,0 L31.8045586,0 C33.5693522,0 35,1.41994415 35,3.17660424 L35,14.8233958 C35,16.5777858 33.5716617,18 31.8045586,18 L24,18 Z M29.8322479,3.2395236 C29.6339219,3.13233348 29.2545158,3.08072342 28.7026524,3.08072342 L28.7026524,14.8914865 C29.4312846,14.8914865 29.8796736,14.7604764 30.0478195,14.4865461 C30.2159654,14.2165858 30.3021941,13.486105 30.3021941,12.2871637 L30.3021941,5.3078959 C30.3021941,4.49404499 30.272014,3.97397442 30.2159654,3.74371416 C30.1599168,3.5134539 30.0348852,3.34671372 29.8322479,3.2395236 Z\"></path><path d=\"M44.4299079,4.50685823 L44.749518,4.50685823 C46.5447098,4.50685823 48,5.91267586 48,7.64486762 L48,14.8619906 C48,16.5950653 46.5451816,18 44.749518,18 L44.4299079,18 C43.3314617,18 42.3602746,17.4736618 41.7718697,16.6682739 L41.4838962,17.7687785 L37,17.7687785 L37,0 L41.7843263,0 L41.7843263,5.78053556 C42.4024982,5.01015739 43.3551514,4.50685823 44.4299079,4.50685823 Z M43.4055679,13.2842155 L43.4055679,9.01907814 C43.4055679,8.31433946 43.3603268,7.85185468 43.2660746,7.63896485 C43.1718224,7.42607505 42.7955881,7.2893916 42.5316822,7.2893916 C42.267776,7.2893916 41.8607934,7.40047379 41.7816216,7.58767002 L41.7816216,9.01907814 L41.7816216,13.4207851 L41.7816216,14.8074788 C41.8721037,15.0130276 42.2602358,15.1274059 42.5316822,15.1274059 C42.8031285,15.1274059 43.1982131,15.0166981 43.281155,14.8074788 C43.3640968,14.5982595 43.4055679,14.0880581 43.4055679,13.2842155 Z\"></path></g></svg></a>"]},{"cell_type":"markdown","metadata":{"id":"jRTMkIJhKE8M"},"source":["\n","* What is the _heading_ size of the titles in the main section of the page, (e.g., \"Featured today\" header)? "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgNUdJH1KGc1"},"outputs":[],"source":["h3"]},{"cell_type":"markdown","metadata":{"id":"Qe8OQU8AEbUK"},"source":["# Scrape HTML Content From a Website\n","\n","Now we will learn how to extract data from a Website, this action is called *Web scraping*. *Web scraping* has become an effective way of extractiong information from the web for decision making and analysis. It is an essential part of the data science toolkit. Data scientists shoud know how to gahter data from web pages and store that data for further analysis. \n","\n","Any web page you see on the internet can be crawled for information and anything visible on a web page can be extracted. Every web page has its own structure and web elements that because of which you need to write your web crawlers/spiders according to the web page being extracted.\n"]},{"cell_type":"markdown","metadata":{"id":"AXrKZx-lK2WL"},"source":["We will use *requests* to obtain the source code of a web page. \n","\n","Documentation: [Requests](https://docs.python-requests.org/en/master/) is an elegant and simple HTTP library for Python, built for human beings \n","\n","Here is an example to extract the source code of a website:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtQT0c2eEJXj"},"outputs":[],"source":["import requests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBk9x_OpEN_i"},"outputs":[],"source":["web_url = \"https://www.imdb.com/\"\n","\n","# Use requests to retrieve data from a given website\n","page = requests.get(web_url)\n","\n","# Print the source code\n","print(page.text)"]},{"cell_type":"markdown","metadata":{"id":"EdB2i7h4FeRw"},"source":["# Parse HTML using Beautiful Soup\n","\n","We just saw how to download the source code of a website to a Python variable, but we need to use an extra library to parse the HTML. For that purpose, we will use `BeautifulSoup`.\n","\n","BeautifulSoup will help us to extract data from the HTML response \n","\n","Documentation: [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n","\n","### Review CSS Selectors\n","\n","1. To get a tag, such as `<a></a>`, `<body></body>`, use the naked name for the tag. E.g. select_one('a') gets an anchor/link element, select_one('body') gets the body element\n","\n","2. .temp gets an element with a class of temp, E.g. to get `<a class=\"temp\"></a>` use select_one('.temp')\n","\n","3. #temp gets an element with an id of temp, E.g. to get `<a id=\"temp\"></a>` use select_one('#temp')\n","\n","4. .temp.example gets an element with both classes temp and example, E.g. to get `<a class=\"temp example\"></a>` use select_one('.temp.example')\n","\n","5. .temp a gets an anchor element nested inside of a parent element with class temp, E.g. to get `<div class=\"temp\"><a></a></div>` use select_one('.temp a'). Note the space between .temp and a.\n","\n","6. .temp .example gets an element with class example nested inside of a parent element with class temp, E.g. to get `<div class=\"temp\"><a class=\"example\"></a></div>` use select_one('.temp .example'). Again, note the space between .temp and .example. The space tells the selector that the class after the space is a child of the class before the space.\n","\n","7. ids, such as` <a id=one></a>`, are unique so you can usually use the id selector by itself to get the right element. No need to do nested selectors when using ids.\n","\n","Source text: [here](https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/) \n","\n","Complete list of CSS Selectors: [CSS Selector Reference](https://www.w3schools.com/cssref/css_selectors.asp)"]},{"cell_type":"markdown","metadata":{"id":"TsSZw_KqoFMr"},"source":["### Tasks\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7CT9SuPXptbt"},"source":["*   Get HTML object from [Wikipedia, Universidad Carlos III de Madrid](https://en.wikipedia.org/wiki/Charles_III_University_of_Madrid) using BeautifulSoup."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uy5pM4SoFXDE"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import requests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YLDb50NFmth"},"outputs":[],"source":["web_url = \"https://en.wikipedia.org/wiki/Charles_III_University_of_Madrid\"\n","page = requests.get(web_url)\n","\n","soup = BeautifulSoup(page.content, \"html.parser\")"]},{"cell_type":"markdown","metadata":{"id":"-imxIQnCp-Tp"},"source":["* Print result of `BeatuifulSoup object` . `h1`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":274,"status":"ok","timestamp":1631517948299,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14203154634187713156"},"user_tz":-120},"id":"eBmqKTArpaIm","outputId":"d2b40dfc-22d1-433b-a1fa-f7e0355fab6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["<h1 class=\"firstHeading\" id=\"firstHeading\">Charles III University of Madrid</h1>\n"]}],"source":["print(soup.h1)"]},{"cell_type":"markdown","metadata":{"id":"CKXGMIbxqNcX"},"source":["* Find the element printed above, using function `find()`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":273,"status":"ok","timestamp":1631518338158,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14203154634187713156"},"user_tz":-120},"id":"YmkTqAAXEdkg","outputId":"2f3be7ed-3451-4087-a25d-b4d6f2388a3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["<h1 class=\"firstHeading\" id=\"firstHeading\">Charles III University of Madrid</h1>\n"]}],"source":["print(soup.find(class_=\"firstHeading\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1631518302100,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14203154634187713156"},"user_tz":-120},"id":"ACJ58ZOYEXmo","outputId":"7e092bb1-a718-475e-cab2-9eba931490ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["<h1 class=\"firstHeading\" id=\"firstHeading\">Charles III University of Madrid</h1>\n"]}],"source":["print(soup.find(\"h1\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1631517981624,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14203154634187713156"},"user_tz":-120},"id":"58MehER4pil0","outputId":"54ca6417-d60b-43d7-fe7b-e85fce5583e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["<h1 class=\"firstHeading\" id=\"firstHeading\">\n"," Charles III University of Madrid\n","</h1>\n","\n"]}],"source":["print(soup.find(id=\"firstHeading\").prettify())"]},{"cell_type":"markdown","metadata":{"id":"hH8WayBwqWmQ"},"source":["* Print the text inside the `h1` tag. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":277,"status":"ok","timestamp":1631518119582,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14203154634187713156"},"user_tz":-120},"id":"XnROfZ45onlB","outputId":"05f93252-8409-4f40-ec30-4d34da27cb36"},"outputs":[{"name":"stdout","output_type":"stream","text":["Charles III University of Madrid\n"]}],"source":["result = soup.find(id=\"firstHeading\")\n","print(result.text)"]},{"cell_type":"markdown","metadata":{"id":"8VMfiI7xqciS"},"source":["* Find all `a` tags that contain the class named `external`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PrtP39fF-L1"},"outputs":[],"source":["soup.find_all(\"a\", class_=\"external\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eumPJHSpXhi"},"outputs":[],"source":["soup.select(\"a.external\")"]},{"cell_type":"markdown","metadata":{"id":"4OTwFR0zqnkR"},"source":["* Find all `span` tags that contain the class named `reference-text`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16mwAS8UpWZu"},"outputs":[],"source":["soup.find_all(\"span\", class_=\"reference-text\")"]},{"cell_type":"markdown","metadata":{"id":"fE2-24Gw9EZr"},"source":["\n","* Check https://www.amazon.es/robots.txt and see if the site can be crawled or not. Then, access to amazon, select an item to buy (e.g., Alexa), copy the URL and obtain the HTML. What happens? "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUO6EhuL9R_F"},"outputs":[],"source":["url = \"https://www.amazon.es/dp/B07PHPXHQS/ref=gw_es_desk_h1_aucc_cr_qh_mwac_0921?pf_rd_r=0CH9P0ABK013CTYCNAJ0&pf_rd_p=e46ea28e-c2ae-4b16-9330-52fc84c1cb32&pd_rd_r=975d63e5-f24f-453c-af9a-9d7998e349d2&pd_rd_w=M9HAZ&pd_rd_wg=BpKAR&ref_=pd_gw_unk\"\n","page=requests.get(url)\n","\n","soup =BeautifulSoup(page.content, \"html.parser\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GKmCbvaN9UkN"},"outputs":[],"source":["soup"]},{"cell_type":"markdown","metadata":{"id":"4kSQHqxrFnoB"},"source":["# Parse HTML using Scrapy"]},{"cell_type":"markdown","metadata":{"id":"K61gell5q01x"},"source":["So far, you know how to extract information from a website using BeautifulSoup, a parsing library.  \n","\n","Now, we will learn how to crawl with **Scrapy**. \n","\n","[Scrapy](https://scrapy.org/) is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival."]},{"cell_type":"markdown","metadata":{"id":"l3nQTK4HTWjP"},"source":["* First, let's install it:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14994,"status":"ok","timestamp":1662546839775,"user":{"displayName":"JOSE GONZALEZ CABAÑAS","userId":"18315766743287740804"},"user_tz":-120},"id":"kpDCqYKwfoXS","outputId":"77edc629-4b17-42e4-a81d-92d0193d205d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scrapy\n","  Downloading Scrapy-2.6.2-py2.py3-none-any.whl (264 kB)\n","\u001b[K     |████████████████████████████████| 264 kB 5.2 MB/s \n","\u001b[?25hCollecting pyOpenSSL>=16.2.0\n","  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 3.4 MB/s \n","\u001b[?25hCollecting w3lib>=1.17.0\n","  Downloading w3lib-2.0.1-py3-none-any.whl (20 kB)\n","Collecting parsel>=1.5.0\n","  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n","Collecting itemadapter>=0.1.0\n","  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\n","Collecting service-identity>=16.0.0\n","  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n","Collecting Twisted>=17.9.0\n","  Downloading Twisted-22.4.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 41.3 MB/s \n","\u001b[?25hCollecting itemloaders>=1.0.1\n","  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\n","Collecting queuelib>=1.4.2\n","  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n","Collecting protego>=0.1.15\n","  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy) (57.4.0)\n","Collecting PyDispatcher>=2.0.5\n","  Downloading PyDispatcher-2.0.6.tar.gz (38 kB)\n","Collecting zope.interface>=4.1.3\n","  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n","\u001b[K     |████████████████████████████████| 251 kB 47.0 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.9.1)\n","Collecting cryptography>=2.0\n","  Downloading cryptography-38.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 39.3 MB/s \n","\u001b[?25hCollecting cssselect>=0.9.1\n","  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n","Collecting tldextract\n","  Downloading tldextract-3.3.1-py3-none-any.whl (93 kB)\n","\u001b[K     |████████████████████████████████| 93 kB 2.2 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->scrapy) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.21)\n","Collecting jmespath>=0.9.5\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (22.1.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n","Collecting Automat>=0.8.0\n","  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n","Collecting hyperlink>=17.1.1\n","  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.4 MB/s \n","\u001b[?25hCollecting constantly>=15.1\n","  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n","Collecting incremental>=21.3.0\n","  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy) (4.1.1)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.10)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (2.23.0)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (3.8.0)\n","Collecting requests-file>=1.4\n","  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n","Building wheels for collected packages: PyDispatcher\n","  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.6-py3-none-any.whl size=11958 sha256=c8579e731c54675c6a329b3a165a43b4be107e281af94629602273db18be962d\n","  Stored in directory: /root/.cache/pip/wheels/c9/d6/6a/de198d890277cde60ca3dbebe7ae592d3b381c7d9bb2455f4d\n","Successfully built PyDispatcher\n","Installing collected packages: w3lib, cssselect, zope.interface, requests-file, parsel, jmespath, itemadapter, incremental, hyperlink, cryptography, constantly, Automat, Twisted, tldextract, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n","Successfully installed Automat-20.2.0 PyDispatcher-2.0.6 Twisted-22.4.0 constantly-15.1.0 cryptography-38.0.0 cssselect-1.1.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.7.0 itemloaders-1.0.6 jmespath-1.0.1 parsel-1.6.0 protego-0.2.1 pyOpenSSL-22.0.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.6.2 service-identity-21.1.0 tldextract-3.3.1 w3lib-2.0.1 zope.interface-5.4.0\n"]}],"source":["%%capture\n","!pip install scrapy "]},{"cell_type":"markdown","metadata":{"id":"g8KBSW2oT7Yp"},"source":["* Create a new folder for the lab and move there:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":516,"status":"ok","timestamp":1662546844977,"user":{"displayName":"JOSE GONZALEZ CABAÑAS","userId":"18315766743287740804"},"user_tz":-120},"id":"FTGsmvxWLa7J","outputId":"2e513131-744e-47d4-9c45-7a9fdbd1356d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/Lab_scrapy\n"]}],"source":["%mkdir Lab_scrapy\n","%cd \"Lab_scrapy\""]},{"cell_type":"markdown","metadata":{"id":"VI8B9xjDUQGW"},"source":["* Create a new scrapy project:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KUy9ImcfXCe"},"outputs":[],"source":["import scrapy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1125,"status":"ok","timestamp":1662546872484,"user":{"displayName":"JOSE GONZALEZ CABAÑAS","userId":"18315766743287740804"},"user_tz":-120},"id":"RcwVxPwRL3PE","outputId":"9604edc0-6c3e-4370-c133-3df82963a3c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["New Scrapy project 'my_scrapy_crawler', using template directory '/usr/local/lib/python3.7/dist-packages/scrapy/templates/project', created in:\n","    /content/Lab_scrapy/my_scrapy_crawler\n","\n","You can start your first spider with:\n","    cd my_scrapy_crawler\n","    scrapy genspider example example.com\n"]}],"source":["# Create first scrapy project\n","!scrapy startproject my_scrapy_crawler"]},{"cell_type":"markdown","metadata":{"id":"Bpe38c5oUoKM"},"source":["* Move to the project folder:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":198,"status":"ok","timestamp":1662546880083,"user":{"displayName":"JOSE GONZALEZ CABAÑAS","userId":"18315766743287740804"},"user_tz":-120},"id":"QNqOMRgiL-CM","outputId":"9dcfd659-92e1-41d9-ff07-1639d21672b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/Lab_scrapy/my_scrapy_crawler\n"]}],"source":["%cd \"my_scrapy_crawler\""]},{"cell_type":"markdown","metadata":{"id":"rc_IYVvSEPFM"},"source":["## Tasks"]},{"cell_type":"markdown","metadata":{"id":"ys4wymAvUsOV"},"source":["* Create automatically a spider called _wikipedia_spider_ that crawls the url [Wikipedia, Universidad Carlos III de Madrid](https://en.wikipedia.org/wiki/Charles_III_University_of_Madrid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1395,"status":"ok","timestamp":1662546890450,"user":{"displayName":"JOSE GONZALEZ CABAÑAS","userId":"18315766743287740804"},"user_tz":-120},"id":"kjTmOinvUnXN","outputId":"e25db179-8425-4e5f-e175-b59847210674"},"outputs":[{"name":"stdout","output_type":"stream","text":["Created spider 'wikipedia_spider' using template 'basic' in module:\n","  my_scrapy_crawler.spiders.wikipedia_spider\n"]}],"source":["# Command to create automatically the wikipedia_spider\n","!scrapy genspider wikipedia_spider en.wikipedia.org/wiki/Charles_III_University_of_Madrid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNXUVFjQLbjR"},"outputs":[],"source":["# Code inside wikipedia_spider.py\n","\n","import scrapy\n","\n","class WikipediaSpiderSpider(scrapy.Spider):\n","    name = 'wikipedia_spider'\n","    start_urls = ['https://en.wikipedia.org/wiki/Charles_III_University_of_Madrid']\n","\n","    def parse(self, response):\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1874,"status":"ok","timestamp":1662547744670,"user":{"displayName":"JOSE GONZALEZ CABAÑAS","userId":"18315766743287740804"},"user_tz":-120},"id":"of2_9i7JNV9_","outputId":"d3724637-53fe-487b-bbdc-292eee075d3f"},"outputs":[{"name":"stdout","output_type":"stream","text":["2022-09-07 10:49:03 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: my_scrapy_crawler)\n","2022-09-07 10:49:03 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.4.0, Python 3.7.13 (default, Apr 24 2022, 01:04:09) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.0, Platform Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\n","2022-09-07 10:49:03 [scrapy.crawler] INFO: Overridden settings:\n","{'BOT_NAME': 'my_scrapy_crawler',\n"," 'NEWSPIDER_MODULE': 'my_scrapy_crawler.spiders',\n"," 'ROBOTSTXT_OBEY': True,\n"," 'SPIDER_MODULES': ['my_scrapy_crawler.spiders']}\n","2022-09-07 10:49:03 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n","2022-09-07 10:49:03 [scrapy.extensions.telnet] INFO: Telnet Password: cac8489e4916490c\n","2022-09-07 10:49:03 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","2022-09-07 10:49:03 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2022-09-07 10:49:03 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2022-09-07 10:49:03 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2022-09-07 10:49:03 [scrapy.core.engine] INFO: Spider opened\n","2022-09-07 10:49:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2022-09-07 10:49:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2022-09-07 10:49:04 [filelock] DEBUG: Attempting to acquire lock 139902495613904 on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-09-07 10:49:04 [filelock] DEBUG: Lock 139902495613904 acquired on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-09-07 10:49:04 [filelock] DEBUG: Attempting to release lock 139902495613904 on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-09-07 10:49:04 [filelock] DEBUG: Lock 139902495613904 released on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-09-07 10:49:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/robots.txt> (referer: None)\n","2022-09-07 10:49:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Charles_III_University_of_Madrid> (referer: None)\n","['Charles III University of Madrid']\n","2022-09-07 10:49:04 [scrapy.core.engine] INFO: Closing spider (finished)\n","2022-09-07 10:49:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 596,\n"," 'downloader/request_count': 2,\n"," 'downloader/request_method_count/GET': 2,\n"," 'downloader/response_bytes': 36640,\n"," 'downloader/response_count': 2,\n"," 'downloader/response_status_count/200': 2,\n"," 'elapsed_time_seconds': 0.280058,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2022, 9, 7, 10, 49, 4, 203971),\n"," 'httpcompression/response_bytes': 162995,\n"," 'httpcompression/response_count': 2,\n"," 'log_count/DEBUG': 7,\n"," 'log_count/INFO': 10,\n"," 'memusage/max': 92921856,\n"," 'memusage/startup': 92921856,\n"," 'response_received_count': 2,\n"," 'robotstxt/request_count': 1,\n"," 'robotstxt/response_count': 1,\n"," 'robotstxt/response_status_count/200': 1,\n"," 'scheduler/dequeued': 1,\n"," 'scheduler/dequeued/memory': 1,\n"," 'scheduler/enqueued': 1,\n"," 'scheduler/enqueued/memory': 1,\n"," 'start_time': datetime.datetime(2022, 9, 7, 10, 49, 3, 923913)}\n","2022-09-07 10:49:04 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}],"source":["# Command to run the crawler\n","!scrapy crawl wikipedia_spider"]},{"cell_type":"markdown","metadata":{"id":"_Z0oB7zPVaKU"},"source":["Now, you should have the following contents:\n","\n","Lab_scrapy/\n","> my_scrapy_crawler/\n","\n",">> scrapy.cfg\n","\n",">> my_scrapy_crawler/\n","\n",">>> __init__.py\n","\n",">>> items.py \n","\n",">>> middlewares.py   \n","\n",">>> pipelines.py     \n","\n",">>> settings.py      \n","\n",">>> spiders/ \n","\n",">>>> __init__.py\n","\n",">>>> wikipedia_spider.py\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ls-nnGjXXDOB"},"source":["* Modify the crawler you just created to extract the information inside the tag with `id=firstHeading`. \n","\n","**Tip:** You have to modify the function `parse` inside the file wikipedia_spider.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3k7ph5Pwf10E"},"outputs":[],"source":["# Code to add inside function parse\n","def parse(self, response):\n","    title=response.css(\".firstHeading ::text\").getall()\n","    print(title)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1744,"status":"ok","timestamp":1662547566650,"user":{"displayName":"JOSE GONZALEZ CABAÑAS","userId":"18315766743287740804"},"user_tz":-120},"id":"3sJGhWh8YCoA","outputId":"09cf1d4b-5ff7-4fc5-a633-eca32db5d3b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["2022-09-07 10:46:05 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: my_scrapy_crawler)\n","2022-09-07 10:46:05 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.4.0, Python 3.7.13 (default, Apr 24 2022, 01:04:09) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.0, Platform Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\n","2022-09-07 10:46:05 [scrapy.crawler] INFO: Overridden settings:\n","{'BOT_NAME': 'my_scrapy_crawler',\n"," 'NEWSPIDER_MODULE': 'my_scrapy_crawler.spiders',\n"," 'ROBOTSTXT_OBEY': True,\n"," 'SPIDER_MODULES': ['my_scrapy_crawler.spiders']}\n","2022-09-07 10:46:05 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n","2022-09-07 10:46:05 [scrapy.extensions.telnet] INFO: Telnet Password: 3a754b1b82c2acae\n","2022-09-07 10:46:05 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","2022-09-07 10:46:05 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2022-09-07 10:46:05 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2022-09-07 10:46:05 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2022-09-07 10:46:05 [scrapy.core.engine] INFO: Spider opened\n","2022-09-07 10:46:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2022-09-07 10:46:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2022-09-07 10:46:06 [filelock] DEBUG: Attempting to acquire lock 140457185297808 on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-09-07 10:46:06 [filelock] DEBUG: Lock 140457185297808 acquired on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-09-07 10:46:06 [filelock] DEBUG: Attempting to release lock 140457185297808 on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-09-07 10:46:06 [filelock] DEBUG: Lock 140457185297808 released on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-09-07 10:46:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/robots.txt> (referer: None)\n","2022-09-07 10:46:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Charles_III_University_of_Madrid> (referer: None)\n","['Charles III University of Madrid']\n","2022-09-07 10:46:06 [scrapy.core.engine] INFO: Closing spider (finished)\n","2022-09-07 10:46:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 596,\n"," 'downloader/request_count': 2,\n"," 'downloader/request_method_count/GET': 2,\n"," 'downloader/response_bytes': 36639,\n"," 'downloader/response_count': 2,\n"," 'downloader/response_status_count/200': 2,\n"," 'elapsed_time_seconds': 0.305057,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2022, 9, 7, 10, 46, 6, 266096),\n"," 'httpcompression/response_bytes': 162995,\n"," 'httpcompression/response_count': 2,\n"," 'log_count/DEBUG': 7,\n"," 'log_count/INFO': 10,\n"," 'memusage/max': 92852224,\n"," 'memusage/startup': 92852224,\n"," 'response_received_count': 2,\n"," 'robotstxt/request_count': 1,\n"," 'robotstxt/response_count': 1,\n"," 'robotstxt/response_status_count/200': 1,\n"," 'scheduler/dequeued': 1,\n"," 'scheduler/dequeued/memory': 1,\n"," 'scheduler/enqueued': 1,\n"," 'scheduler/enqueued/memory': 1,\n"," 'start_time': datetime.datetime(2022, 9, 7, 10, 46, 5, 961039)}\n","2022-09-07 10:46:06 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}],"source":["!scrapy crawl wikipedia_spider"]},{"cell_type":"markdown","metadata":{"id":"HAWzxkgjdPGw"},"source":["**EXTRA**: One of the advantages of Scrapy is that you can repeat the same crawler to multiple URLs at once. Try to add more URLs from Wikipedia in the list `start_urls` and check how easy it is! "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMqkHzcvbNrh"},"outputs":[],"source":["# Modify \n","start_urls = ['https://en.wikipedia.org/wiki/Charles_III_University_of_Madrid', 'https://en.wikipedia.org/wiki/Complutense_University_of_Madrid']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9epsCYjPPCtf"},"outputs":[],"source":["!scrapy crawl wikipedia_spider"]},{"cell_type":"markdown","metadata":{"id":"vQppiIbUdAzo"},"source":["* Modify the crawler you just created to extract all the `a` tags wiht class name `external`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8ZOKw4Ceu5K"},"outputs":[],"source":["# Modify\n","start_urls = ['https://en.wikipedia.org/wiki/Charles_III_University_of_Madrid']\n","\n","def parse(self, response):\n","    title=response.css(\".firstHeading ::text\").getall()\n","    print(title)\n","\n","    links = response.css(\"a.external\").getall()\n","    print(links)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_MGkNm25fZny"},"outputs":[],"source":["!scrapy crawl wikipedia_spider"]},{"cell_type":"markdown","metadata":{"id":"qZ9gwiFSV9_L"},"source":["* Access to Facebook using Scrapy (that implies repeating the previous process and create a new spider for Facebook). Can we use Scrapy to extract information from this website? Explain."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2092,"status":"ok","timestamp":1631521605479,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14203154634187713156"},"user_tz":-120},"id":"XNC9jeX2WKQ8","outputId":"47e0b92e-5e9c-4dce-f33d-ac16abaf0456"},"outputs":[{"name":"stdout","output_type":"stream","text":["New Scrapy project 'facebook_test', using template directory '/usr/local/lib/python3.7/dist-packages/scrapy/templates/project', created in:\n","    /content/Lab_scrapy/my_scrapy_crawler/facebook_test\n","\n","You can start your first spider with:\n","    cd facebook_test\n","    scrapy genspider example example.com\n","/content/Lab_scrapy/my_scrapy_crawler/facebook_test\n","Created spider 'facebook_spider' using template 'basic' in module:\n","  facebook_test.spiders.facebook_spider\n"]}],"source":["!scrapy startproject facebook_test\n","%cd facebook_test\n","!scrapy genspider facebook_spider www.facebook.com"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTbxgxjgWKFW"},"outputs":[],"source":["!scrapy crawl facebook_spider"]},{"cell_type":"markdown","metadata":{"id":"3lePvVrpGZiA"},"source":["# Parse HTML using Selenium"]},{"cell_type":"markdown","metadata":{"id":"vBvl-Bm8gAjp"},"source":["Last but not least, we will learn [Selenium](https://www.selenium.dev/), a powerful web scraping tool. Initially, it was created for website testing purposes. However, nowadays, it is also used for Web Scraping.  Selenium it is useful to scrape dynamic websites, that contains cookies, JavaScript functions or any other dynamic or that require human actions. \n","\n","Selenium requieres integraition with third-party browsers in order to work. Let's see how to install it and how we can build our own first scraper with Selenium"]},{"cell_type":"markdown","metadata":{"id":"As4cpySaiIu0"},"source":["* Install Chrome and Selenium"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ko2TyM7HGdHt"},"outputs":[],"source":["!apt update\n","!apt install chromium-chromedriver\n","!pip install selenium"]},{"cell_type":"markdown","metadata":{"id":"uUt-sIM3iRfF"},"source":["* Import selenium webdriver and create our headless browser (a headless browser is a web browser without a graphical user interface)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MIuuOoO2UTPC"},"outputs":[],"source":["# set options to be headless\n","from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","\n","options = webdriver.ChromeOptions()\n","options.add_argument('--headless')\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')\n","# open it, go to a website, and get results\n","driver = webdriver.Chrome('chromedriver',options=options)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0hPNpPP_UXnT"},"outputs":[],"source":["url = \"https://en.wikipedia.org/wiki/Charles_III_University_of_Madrid\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xR74xeCBVTu5"},"outputs":[],"source":["driver.get(url)"]},{"cell_type":"markdown","metadata":{"id":"hBCBUwrGEVri"},"source":["## Tasks"]},{"cell_type":"markdown","metadata":{"id":"F5jZ7K7Bj9XP"},"source":["* Find element by `id=firstHeading` and print the text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1631522223210,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14203154634187713156"},"user_tz":-120},"id":"Hx9cqXdXEWiU","outputId":"bc61a1e6-7fd9-4a0d-adc3-350bd5864b94"},"outputs":[{"name":"stdout","output_type":"stream","text":["Charles III University of Madrid\n"]}],"source":["results = driver.find_element(By.ID, \"firstHeading\")\n","print(results.text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":296,"status":"ok","timestamp":1631522245148,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14203154634187713156"},"user_tz":-120},"id":"zZGVtNZlTYve","outputId":"ee1f92d8-f483-46eb-d0ba-d472de67fe03"},"outputs":[{"name":"stdout","output_type":"stream","text":["Charles III University of Madrid\n"]}],"source":["results = driver.find_element(By.CSS,\"#firstHeading\")\n","print(results.text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":264,"status":"ok","timestamp":1631522253432,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14203154634187713156"},"user_tz":-120},"id":"hxW_DqpWTduF","outputId":"bbc0cd86-d86a-4a95-862d-9b2f7772f9bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Charles III University of Madrid\n"]}],"source":["results = driver.find_element(By.CSS,\".firstHeading\")\n","print(results.text)"]},{"cell_type":"markdown","metadata":{"id":"YoVZtFnxkQTs"},"source":["* Find all elements by class name `external` and print the result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oG69HkD-ExzO"},"outputs":[],"source":["links = driver.find_elements(By.CLASS_NAME, \"external\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Djiuady5UbO-"},"outputs":[],"source":["for link in links:\n","  print(link.text) \n","  print(link.get_attribute(\"href\"))"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.0 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.0"},"vscode":{"interpreter":{"hash":"c63d8c7d738c2960218a10995aedf0a7f67a49a231e71037adf0440953cdb45b"}}},"nbformat":4,"nbformat_minor":0}
