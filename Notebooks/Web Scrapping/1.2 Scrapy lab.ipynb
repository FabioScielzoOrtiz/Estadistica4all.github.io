{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XjqGtEqMUB2O"},"source":["<img src=\"https://www.uc3m.es/ss/Satellite?blobcol=urldata&blobkey=id&blobtable=MungoBlobs&blobwhere=1371573952659\">\n","\n","---\n","\n","# WEB ANALYTICS COURSE 4 - SEMESTER 2 \n","# BACHELOR IN DATA SCIENCE AND ENGINEERING \n","\n","# LAB 1.2 WEB SCRAPING WITH SCRAPY\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"vS1O4BW82LV6"},"source":["# 0. Lab Preparation\n","\n","1.  Study and have clear the concepts explained in the theoretical class and the introductory lab. \n","\n","2.   Gain experience with the use of the [Scrapy](https://scrapy.org/). The exercises of this lab will be mainly based on the utilization of functions offered by this library. \n","\n","3. It is assumed students have experience in using Python notebooks. Either a local installation (e.g., local python installation + Jupyter) or a cloud-based solution (e.g., Google Colab). *We recommend the second option*."]},{"cell_type":"markdown","metadata":{"id":"nwlGXDUG2db2"},"source":["# 1. Lab Introduction"]},{"cell_type":"markdown","metadata":{"id":"ypNv8Fpi2YS7"},"source":["* In this lab, we will implement a web scraper using [Scrapy](https://scrapy.org/). One of the tools explained in the theoretical class. \n","\n","* The lab will be done in groups of 2 people.\n","\n","* The lab defines a set of milestones the students must complete. Upon completing every milestone, students should call the professor, who will check the correctness of the solution (*If the professor is busy, do not wait for them, move to the next milestone*).\n","\n","* **The final mark will be computed as a function of the number of milestones successfully completed.**\n","\n","* **Each group should also share their lab notebook with the professor upon the finalization of the lab.**\n","\n","* In this lab we will use the [Scrapy](https://scrapy.org/) library for the creation of a web scraper, to extract information from the web. As indicated in the *Lab Preparation* section above, it is expected that students have gained experience in the use of the library before starting the first session of the lab.\n","\n","- It is recommended to use [Google Colab](https://colab.research.google.com/) to produce the Python notebook with the solution of the lab. Of course, if any student prefers using its local programming environment (e.g., jupyter) and python installation, they are welcome to do so. "]},{"cell_type":"code","metadata":{"id":"lnIUv5WsKSfJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631082368982,"user_tz":-120,"elapsed":15725,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"","userId":"14203154634187713156"}},"outputId":"ce89ead8-cdd9-448e-bb13-e6f8d9ee24bb"},"source":["!pip install scrapy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scrapy\n","  Downloading Scrapy-2.5.0-py2.py3-none-any.whl (254 kB)\n","\u001b[K     |████████████████████████████████| 254 kB 2.7 MB/s \n","\u001b[?25hCollecting itemloaders>=1.0.1\n","  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n","Collecting cssselect>=0.9.1\n","  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n","Collecting w3lib>=1.17.0\n","  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n","Collecting Twisted[http2]>=17.9.0\n","  Downloading Twisted-21.7.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 48.7 MB/s \n","\u001b[?25hCollecting service-identity>=16.0.0\n","  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n","Collecting h2<4.0,>=3.0\n","  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[K     |████████████████████████████████| 65 kB 1.6 MB/s \n","\u001b[?25hCollecting itemadapter>=0.1.0\n","  Downloading itemadapter-0.4.0-py3-none-any.whl (10 kB)\n","Collecting cryptography>=2.0\n","  Downloading cryptography-3.4.8-cp36-abi3-manylinux_2_24_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 42.6 MB/s \n","\u001b[?25hCollecting PyDispatcher>=2.0.5\n","  Downloading PyDispatcher-2.0.5.zip (47 kB)\n","\u001b[K     |████████████████████████████████| 47 kB 4.3 MB/s \n","\u001b[?25hCollecting protego>=0.1.15\n","  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n","\u001b[K     |████████████████████████████████| 3.2 MB 42.1 MB/s \n","\u001b[?25hCollecting queuelib>=1.4.2\n","  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n","Collecting zope.interface>=4.1.3\n","  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n","\u001b[K     |████████████████████████████████| 251 kB 71.6 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.2.6)\n","Collecting pyOpenSSL>=16.2.0\n","  Downloading pyOpenSSL-20.0.1-py2.py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 2.2 MB/s \n","\u001b[?25hCollecting parsel>=1.5.0\n","  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->scrapy) (1.14.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.20)\n","Collecting hyperframe<6,>=5.2.0\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Collecting hpack<4,>=3.0\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Collecting jmespath>=0.9.5\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (21.2.0)\n","Collecting constantly>=15.1\n","  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted[http2]>=17.9.0->scrapy) (3.7.4.3)\n","Collecting incremental>=21.3.0\n","  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n","Collecting Automat>=0.8.0\n","  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n","Collecting hyperlink>=17.1.1\n","  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.2 MB/s \n","\u001b[?25hCollecting priority<2.0,>=1.1.0\n","  Downloading priority-1.3.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->scrapy) (2.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=4.1.3->scrapy) (57.4.0)\n","Building wheels for collected packages: protego, PyDispatcher\n","  Building wheel for protego (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7782 sha256=f26d9e5af026f531cdae86eb61fb7d649ea46bbef9e2e813dfa579095ca4cc59\n","  Stored in directory: /root/.cache/pip/wheels/ca/44/01/3592ccfbcfaee4ab297c4097e6e9dbe1c7697e3531a39877ab\n","  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=1ff45e7eb224dec9749bc37462b98d2874caa3ea840927b8ead6fe9270136014\n","  Stored in directory: /root/.cache/pip/wheels/2d/18/21/3c6a732eaa69a339198e08bb63b7da2c45933a3428b29ec454\n","Successfully built protego PyDispatcher\n","Installing collected packages: zope.interface, w3lib, incremental, hyperlink, hyperframe, hpack, cssselect, constantly, Automat, Twisted, priority, parsel, jmespath, itemadapter, h2, cryptography, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n","Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-21.7.0 constantly-15.1.0 cryptography-3.4.8 cssselect-1.1.0 h2-3.2.0 hpack-3.0.0 hyperframe-5.2.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.4.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 priority-1.3.0 protego-0.1.16 pyOpenSSL-20.0.1 queuelib-1.6.2 scrapy-2.5.0 service-identity-21.1.0 w3lib-1.22.0 zope.interface-5.4.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"jhPF9V3_Jyjv"},"source":["# MILESTONE 1\n","\n","a) Create a _Scrapy_ project for this lab (**HINT:** Remember you can use the command `startproject`).\n","\n","b) Create a crawler/spider to the website [BACHELOR IN DATA SCIENCE AND ENGINEERING\n","](https://www.uc3m.es/bachelor-degree/data-science).\n","\n","c) Add the code to the crawler to get PROGRAM header. **TIP:** Find the element tag with `id=\"program\"` and print the result.\n","\n","d) Add the code to the crawler to find the table inside PROGRAM for Course 1 - Semester 1 and print the result."]},{"cell_type":"code","metadata":{"id":"QR58vq8AwZuQ"},"source":["import scrapy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bRsxq87GKL2b","executionInfo":{"status":"ok","timestamp":1631082378857,"user_tz":-120,"elapsed":1175,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"","userId":"14203154634187713156"}},"outputId":"dd2300dc-bee4-406b-ce6d-dd4b7937791a"},"source":["## Connect Drive with this notebook first. \n","%cd \"/content/drive/MyDrive/Web Analytics/Tema_Scrapers/Labs_solutions/\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1O273XR2abNwMMZ4GDzEtwA-i5oEey4dm/Web Analytics/Tema_Scrapers/Labs_solutions\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_m1T_uvKlec","executionInfo":{"status":"ok","timestamp":1631082382632,"user_tz":-120,"elapsed":1333,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"","userId":"14203154634187713156"}},"outputId":"cabedc5b-2c72-4903-f5f0-835cf25e027b"},"source":["# a) Create scrapy project\n","!scrapy startproject lab2_scrapy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["New Scrapy project 'lab2_scrapy', using template directory '/usr/local/lib/python3.7/dist-packages/scrapy/templates/project', created in:\n","    /content/drive/.shortcut-targets-by-id/1O273XR2abNwMMZ4GDzEtwA-i5oEey4dm/Web Analytics/Tema_Scrapers/Labs_solutions/lab2_scrapy\n","\n","You can start your first spider with:\n","    cd lab2_scrapy\n","    scrapy genspider example example.com\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CviVORXnNfRw","executionInfo":{"status":"ok","timestamp":1631082384558,"user_tz":-120,"elapsed":7,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"","userId":"14203154634187713156"}},"outputId":"f78046bd-952a-478e-99b6-305a3d8cefe9"},"source":["%cd \"lab2_scrapy\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1O273XR2abNwMMZ4GDzEtwA-i5oEey4dm/Web Analytics/Tema_Scrapers/Labs_solutions/lab2_scrapy\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T5XaDuIiObY1","executionInfo":{"status":"ok","timestamp":1631082387681,"user_tz":-120,"elapsed":343,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"","userId":"14203154634187713156"}},"outputId":"9f87f133-9cb6-4386-d416-1eba76fda744"},"source":["%ls -R"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".:\n","\u001b[0m\u001b[01;34mlab2_scrapy\u001b[0m/  scrapy.cfg\n","\n","./lab2_scrapy:\n","__init__.py  items.py  middlewares.py  pipelines.py  settings.py  \u001b[01;34mspiders\u001b[0m/\n","\n","./lab2_scrapy/spiders:\n","__init__.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQOMzmviFo3A","executionInfo":{"status":"ok","timestamp":1631082429959,"user_tz":-120,"elapsed":1323,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"","userId":"14203154634187713156"}},"outputId":"83539ab7-6ef8-49bf-bc44-d8dc526b35a0"},"source":["# b) Create crawler\n","\n","# Automatically\n","!scrapy genspider lab2_crawler_milestone1 www.uc3m.es/bachelor-degree/data-science"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Created spider 'lab2_crawler' using template 'basic' in module:\n","  lab2_scrapy.spiders.lab2_crawler\n"]}]},{"cell_type":"code","metadata":{"id":"l47nRVzysgFt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630397090718,"user_tz":-120,"elapsed":1381,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"","userId":"14203154634187713156"}},"outputId":"f630b311-00a0-4f8b-82e5-e3310731173b"},"source":["# Resulting code in file lab2_scrapy/spiders/lab2_crawler.py\n","import scrapy\n","\n","class Lab2CrawlerSpider(scrapy.Spider):\n","    name = 'lab2_crawler_milestone1'\n","    start_urls = ['http://www.uc3m.es/bachelor-degree/data-science']\n","\n","    def parse(self, response):\n","        pass"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Created spider 'lab2_crawler' using template 'basic' in module:\n","  lab2_scrapy.spiders.lab2_crawler\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zIcMGECnKrOr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631082476341,"user_tz":-120,"elapsed":3525,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"","userId":"14203154634187713156"}},"outputId":"c15d790c-0d36-41b5-9873-c019d43a3f5e"},"source":["!scrapy crawl lab2_crawler"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-09-08 06:27:53 [scrapy.utils.log] INFO: Scrapy 2.5.0 started (bot: lab2_scrapy)\n","2021-09-08 06:27:53 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.7.11 (default, Jul  3 2021, 18:01:19) - [GCC 7.5.0], pyOpenSSL 20.0.1 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\n","2021-09-08 06:27:53 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n","2021-09-08 06:27:53 [scrapy.crawler] INFO: Overridden settings:\n","{'BOT_NAME': 'lab2_scrapy',\n"," 'NEWSPIDER_MODULE': 'lab2_scrapy.spiders',\n"," 'ROBOTSTXT_OBEY': True,\n"," 'SPIDER_MODULES': ['lab2_scrapy.spiders']}\n","2021-09-08 06:27:53 [scrapy.extensions.telnet] INFO: Telnet Password: c532688eed5d6585\n","2021-09-08 06:27:53 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","2021-09-08 06:27:53 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2021-09-08 06:27:53 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2021-09-08 06:27:53 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2021-09-08 06:27:53 [scrapy.core.engine] INFO: Spider opened\n","2021-09-08 06:27:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2021-09-08 06:27:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2021-09-08 06:27:53 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.uc3m.es/robots.txt> from <GET http://www.uc3m.es/robots.txt>\n","2021-09-08 06:27:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n","2021-09-08 06:27:54 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.uc3m.es/bachelor-degree/data-science/> from <GET http://www.uc3m.es/bachelor-degree/data-science/>\n","2021-09-08 06:27:55 [scrapy.core.engine] DEBUG: Crawled (410) <GET https://www.uc3m.es/bachelor-degree/data-science/> (referer: None)\n","2021-09-08 06:27:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <410 https://www.uc3m.es/bachelor-degree/data-science/>: HTTP status code is not handled or not allowed\n","2021-09-08 06:27:55 [scrapy.core.engine] INFO: Closing spider (finished)\n","2021-09-08 06:27:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 1183,\n"," 'downloader/request_count': 4,\n"," 'downloader/request_method_count/GET': 4,\n"," 'downloader/response_bytes': 7790,\n"," 'downloader/response_count': 4,\n"," 'downloader/response_status_count/200': 1,\n"," 'downloader/response_status_count/301': 2,\n"," 'downloader/response_status_count/410': 1,\n"," 'elapsed_time_seconds': 2.074167,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2021, 9, 8, 6, 27, 55, 756101),\n"," 'httpcompression/response_bytes': 3341,\n"," 'httpcompression/response_count': 1,\n"," 'httperror/response_ignored_count': 1,\n"," 'httperror/response_ignored_status_count/410': 1,\n"," 'log_count/DEBUG': 4,\n"," 'log_count/INFO': 11,\n"," 'memusage/max': 107073536,\n"," 'memusage/startup': 107073536,\n"," 'response_received_count': 2,\n"," 'robotstxt/request_count': 1,\n"," 'robotstxt/response_count': 1,\n"," 'robotstxt/response_status_count/200': 1,\n"," 'scheduler/dequeued': 2,\n"," 'scheduler/dequeued/memory': 2,\n"," 'scheduler/enqueued': 2,\n"," 'scheduler/enqueued/memory': 2,\n"," 'start_time': datetime.datetime(2021, 9, 8, 6, 27, 53, 681934)}\n","2021-09-08 06:27:55 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}]},{"cell_type":"code","metadata":{"id":"DVsGGiuaQwv2"},"source":["# c) Find the element tag with id=\"program\"\n","# d) Add the code to the crawler to find the table inside PROGRAM for Course 1 - Semester 1 and print the result.\n","def parse(self, response):\n","      program=response.css('#program').css('h2::text').extract()\n","      #program=response.css('div.marcoParrafo').getall()[1] \n","      print(\"***Result***: \")\n","      print(program)\n","\n","      course=response.xpath(\"//h2[contains(text(),'Course 1 - Semester 1')]/text()\").extract()\n","      #course=response.xpath('//*[@id=\"content\"]/div[2]/div[3]/div[2]/div/div[1]/div[1]/div[1]').extract()\n","      print(\"***Result course***: \")\n","      print(course)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_3w_XlwQwpd","executionInfo":{"status":"ok","timestamp":1631083451746,"user_tz":-120,"elapsed":2249,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"","userId":"14203154634187713156"}},"outputId":"578245be-a215-44cf-b70e-b90e11d8e7c3"},"source":["!scrapy crawl lab2_crawler"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-09-08 06:44:10 [scrapy.utils.log] INFO: Scrapy 2.5.0 started (bot: lab2_scrapy)\n","2021-09-08 06:44:10 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.7.11 (default, Jul  3 2021, 18:01:19) - [GCC 7.5.0], pyOpenSSL 20.0.1 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\n","2021-09-08 06:44:10 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n","2021-09-08 06:44:10 [scrapy.crawler] INFO: Overridden settings:\n","{'BOT_NAME': 'lab2_scrapy',\n"," 'NEWSPIDER_MODULE': 'lab2_scrapy.spiders',\n"," 'ROBOTSTXT_OBEY': True,\n"," 'SPIDER_MODULES': ['lab2_scrapy.spiders']}\n","2021-09-08 06:44:10 [scrapy.extensions.telnet] INFO: Telnet Password: c5298edd0a863c86\n","2021-09-08 06:44:10 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","2021-09-08 06:44:10 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2021-09-08 06:44:10 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2021-09-08 06:44:10 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2021-09-08 06:44:10 [scrapy.core.engine] INFO: Spider opened\n","2021-09-08 06:44:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2021-09-08 06:44:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2021-09-08 06:44:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n","2021-09-08 06:44:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n","***Result***: \n","<div class=\"marcoParrafo\" id=\"program\">\n","\t\t\t\t\t\t\t<h2>Program</h2>\n","\t\t\t\t\t\t</div>\n","***Result course***: \n","['<h2>Course 1 - Semester 1</h2>']\n","2021-09-08 06:44:11 [scrapy.core.engine] INFO: Closing spider (finished)\n","2021-09-08 06:44:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 460,\n"," 'downloader/request_count': 2,\n"," 'downloader/request_method_count/GET': 2,\n"," 'downloader/response_bytes': 40997,\n"," 'downloader/response_count': 2,\n"," 'downloader/response_status_count/200': 2,\n"," 'elapsed_time_seconds': 1.070932,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2021, 9, 8, 6, 44, 11, 358207),\n"," 'httpcompression/response_bytes': 71524,\n"," 'httpcompression/response_count': 1,\n"," 'log_count/DEBUG': 2,\n"," 'log_count/INFO': 10,\n"," 'memusage/max': 107728896,\n"," 'memusage/startup': 107728896,\n"," 'response_received_count': 2,\n"," 'robotstxt/request_count': 1,\n"," 'robotstxt/response_count': 1,\n"," 'robotstxt/response_status_count/200': 1,\n"," 'scheduler/dequeued': 1,\n"," 'scheduler/dequeued/memory': 1,\n"," 'scheduler/enqueued': 1,\n"," 'scheduler/enqueued/memory': 1,\n"," 'start_time': datetime.datetime(2021, 9, 8, 6, 44, 10, 287275)}\n","2021-09-08 06:44:11 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}]},{"cell_type":"markdown","metadata":{"id":"oHKtqTS95RSx"},"source":["# MILESTONE 2\n","\n","a) Obtain the link to Web Analytics course by finding the corresponding href. \n","\n","b) Create a new spider _class_ and access to this URL. \n","\n","**TIP**: For this milestone, you need to create a new crawler and give it a different name.\n","\n","c) Print the text inside the _Description of contents: programme_ section.\n","\n"]},{"cell_type":"code","metadata":{"id":"wVrENkpNJ95X"},"source":["url=\"https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"arJtKiJmLPhz"},"source":["class Lab2CrawlerSpider_milestone2(scrapy.Spider):\n","    name = 'lab2_crawler_milestone2'\n","    start_urls = ['https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2']\n","\n","    def parse(self, response):\n","      description=response.xpath(\"//body/div[1]/div[5]/div[2]\").css(\".tarea::text\").extract()\n","      print(\"***Result description***: \")\n","      print(description)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OPwMwXlYKf_L"},"source":["!scrapy crawl lab2_crawler_milestone2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QN4L9SxRP3Hj"},"source":["# MILESTONE 3\n","\n","a) Modify your code from previous milestones for running both crawlers in the same command line process. \n","\n","\n","b) Instead of printing the results (from Milestone 1 and 2), save them in a file."]},{"cell_type":"code","metadata":{"id":"iz5O6q5yOhL1"},"source":["#### Example code for running multiple spiders in the same process. ####\n","# Check https://docs.scrapy.org/en/latest/topics/practices.html\n","import scrapy\n","from scrapy.crawler import CrawlerProcess\n","\n","class MySpider1(scrapy.Spider):\n","    # Your first spider definition\n","    ...\n","\n","class MySpider2(scrapy.Spider):\n","    # Your second spider definition\n","    ...\n","\n","process = CrawlerProcess()\n","process.crawl(MySpider1)\n","process.crawl(MySpider2)\n","process.start() # the script will block here until all crawling jobs are finished\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSjvsVVPSTDx"},"source":["# Modify your code from previous milestones for running both crawlers in the same process. \n","process = CrawlerProcess()\n","process.crawl(Lab2CrawlerSpider_milestone1)\n","process.crawl(Lab2CrawlerSpider_milestone2)\n","process.start() # the script will block here until all crawling jobs are finished"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-M_VoRIdTtn6"},"source":["# d) Save all the results (from Mileston 1 and 2) in a file.\n","\n","# Inside parse of lab2_crawler_milestone1\n","filename = \"lab2_results.txt\"\n","with open(filename, 'wb') as f:\n","    f.write(\"Text program: \"+program[0]+\"\\n\")\n","    f.write(\"Text course: \"+course[0]+\"\\n\")\n","\n","# Inside parse of lab2_crawler_milestone2\n","\n","filename = \"lab2_results.txt\"\n","with open(filename, 'wb') as f:\n","    f.write(\"Text description: \"+description[0]+\"\\n\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qt-Wq68PTaIT","executionInfo":{"status":"ok","timestamp":1631086043670,"user_tz":-120,"elapsed":234,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"","userId":"14203154634187713156"}},"outputId":"18bec3db-c2aa-4775-cfe5-08e34dacde99"},"source":["%ls -R"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".:\n","lab2_results.txt  \u001b[0m\u001b[01;34mlab2_scrapy\u001b[0m/  scrapy.cfg\n","\n","./lab2_scrapy:\n","__init__.py  middlewares.py  \u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34mspiders\u001b[0m/\n","items.py     pipelines.py    settings.py\n","\n","./lab2_scrapy/__pycache__:\n","__init__.cpython-37.pyc  settings.cpython-37.pyc\n","\n","./lab2_scrapy/spiders:\n","__init__.py  lab2_crawler.py  lab_unified_crawler.py  \u001b[01;34m__pycache__\u001b[0m/\n","\n","./lab2_scrapy/spiders/__pycache__:\n","__init__.cpython-37.pyc      lab_unified_crawler.cpython-37.pyc\n","lab2_crawler.cpython-37.pyc\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g5B4VprfLgC0","executionInfo":{"status":"ok","timestamp":1631086085322,"user_tz":-120,"elapsed":2665,"user":{"displayName":"PATRICIA CALLEJO PINARDO","photoUrl":"","userId":"14203154634187713156"}},"outputId":"0ee92c46-ea3d-4b80-87cb-fcb9956b088e"},"source":["#!scrapy crawl\n","!python lab2_scrapy/spiders/lab2_crawler.py"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-09-08 07:28:03 [scrapy.utils.log] INFO: Scrapy 2.5.0 started (bot: scrapybot)\n","2021-09-08 07:28:03 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.7.11 (default, Jul  3 2021, 18:01:19) - [GCC 7.5.0], pyOpenSSL 20.0.1 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\n","2021-09-08 07:28:03 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n","2021-09-08 07:28:03 [scrapy.crawler] INFO: Overridden settings:\n","{}\n","2021-09-08 07:28:03 [scrapy.extensions.telnet] INFO: Telnet Password: 2f36240141c00f92\n","2021-09-08 07:28:03 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","2021-09-08 07:28:03 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2021-09-08 07:28:03 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2021-09-08 07:28:03 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2021-09-08 07:28:03 [scrapy.core.engine] INFO: Spider opened\n","2021-09-08 07:28:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2021-09-08 07:28:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2021-09-08 07:28:03 [scrapy.crawler] INFO: Overridden settings:\n","{}\n","2021-09-08 07:28:03 [scrapy.extensions.telnet] INFO: Telnet Password: 1af983ab6d759678\n","2021-09-08 07:28:03 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","2021-09-08 07:28:03 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2021-09-08 07:28:03 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2021-09-08 07:28:03 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2021-09-08 07:28:03 [scrapy.core.engine] INFO: Spider opened\n","2021-09-08 07:28:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2021-09-08 07:28:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n","2021-09-08 07:28:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n","***Result program***: \n","['Program']\n","***Result course***: \n","['Course 1 - Semester 1']\n","2021-09-08 07:28:04 [scrapy.core.engine] INFO: Closing spider (finished)\n","2021-09-08 07:28:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 239,\n"," 'downloader/request_count': 1,\n"," 'downloader/request_method_count/GET': 1,\n"," 'downloader/response_bytes': 35244,\n"," 'downloader/response_count': 1,\n"," 'downloader/response_status_count/200': 1,\n"," 'elapsed_time_seconds': 1.136755,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2021, 9, 8, 7, 28, 4, 458125),\n"," 'httpcompression/response_bytes': 71524,\n"," 'httpcompression/response_count': 1,\n"," 'log_count/DEBUG': 1,\n"," 'log_count/INFO': 19,\n"," 'memusage/max': 123748352,\n"," 'memusage/startup': 123748352,\n"," 'response_received_count': 1,\n"," 'scheduler/dequeued': 1,\n"," 'scheduler/dequeued/memory': 1,\n"," 'scheduler/enqueued': 1,\n"," 'scheduler/enqueued/memory': 1,\n"," 'start_time': datetime.datetime(2021, 9, 8, 7, 28, 3, 321370)}\n","2021-09-08 07:28:04 [scrapy.core.engine] INFO: Spider closed (finished)\n","2021-09-08 07:28:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2> (referer: None)\n","***Result description***: \n","['1.\\tData Collection in the Web ecosystem: \\n        1.1  Scrapers, Crawlers\\n        1.2\\tAPIs\\n2.\\tData Analytics in the web\\n        2.1  Graph Analysis: Centrality and Influence metrics\\n        2.2  Network structure: \\n               2.2.1 Type of networks (bipartite graph, small world, scale free)\\n               2.2.2 Clustering, Community Detection, K-core decomposition\\n\\n3.\\tWeb data visualization\\n        3.1\\tRepresentation of web information.\\n        3.2\\tVisualization tools.\\n\\n4.\\tFinal Web Analytics Project\\n        4.1\\tThe project needs to include the three components presented above (Data Collection, Data Analytics and Data Visualization\\n']\n","2021-09-08 07:28:04 [scrapy.core.engine] INFO: Closing spider (finished)\n","2021-09-08 07:28:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 273,\n"," 'downloader/request_count': 1,\n"," 'downloader/request_method_count/GET': 1,\n"," 'downloader/response_bytes': 7412,\n"," 'downloader/response_count': 1,\n"," 'downloader/response_status_count/200': 1,\n"," 'elapsed_time_seconds': 1.595427,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2021, 9, 8, 7, 28, 4, 927025),\n"," 'httpcompression/response_bytes': 17292,\n"," 'httpcompression/response_count': 1,\n"," 'log_count/DEBUG': 2,\n"," 'log_count/INFO': 13,\n"," 'memusage/max': 123748352,\n"," 'memusage/startup': 123748352,\n"," 'response_received_count': 1,\n"," 'scheduler/dequeued': 1,\n"," 'scheduler/dequeued/memory': 1,\n"," 'scheduler/enqueued': 1,\n"," 'scheduler/enqueued/memory': 1,\n"," 'start_time': datetime.datetime(2021, 9, 8, 7, 28, 3, 331598)}\n","2021-09-08 07:28:04 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}]},{"cell_type":"markdown","metadata":{"id":"qAP-IAG8JMGJ"},"source":["# MILESTONE 4\n","\n","\n","\n","* Now, let's take advantage of the Scrapy library capabilities. For that purpose, we are going to create a crawler that scrapes multiple urls at once. "]},{"cell_type":"code","metadata":{"id":"iFfF2iZ6WHBi"},"source":[],"execution_count":null,"outputs":[]}]}