{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing PAI with resampling methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# Quantitative Response\n",
    "\n",
    "Y_old = np.random.normal(loc=50, scale=10, size=500)\n",
    "\n",
    "# Quantitative variables \n",
    "\n",
    "X1_old = np.random.normal(loc=30, scale=25, size=500)\n",
    " \n",
    "# Binary variables \n",
    "\n",
    "X2_old = np.random.uniform(low=0.0, high=1.0, size=500).round()\n",
    " \n",
    "# Multiclass categorical variables\n",
    "\n",
    "X3_old = np.random.uniform(low=0, high=4, size=500).round()   # categories: 0,1,2,3,4\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Data (with a big change in X1 distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "\n",
    "# Quantitative Response\n",
    "\n",
    "Y_new = np.random.normal(loc=50, scale=10, size=500)\n",
    "\n",
    "# Quantitative variables \n",
    "\n",
    "X1_new = np.random.normal(loc=15, scale=60, size=500)\n",
    " \n",
    "# Binary variables \n",
    "\n",
    "X2_new = np.random.uniform(low=0.0, high=1.0, size=500).round()\n",
    " \n",
    "# Multiclass categorical variables\n",
    "\n",
    "X3_new = np.random.uniform(low=0, high=4, size=500).round()   # categories: 0,1,2,3,4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Old = pd.DataFrame( {\"Y\":Y_old , \"X1\": X1_old , \"X2\": X2_old , \"X3\": X3_old} ) \n",
    "\n",
    "df_New = pd.DataFrame( {\"Y\":Y_new , \"X1\": X1_new , \"X2\": X2_new , \"X3\": X3_new} ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_line, geom_point, geom_histogram, geom_bar, geom_boxplot, scale_y_continuous, scale_x_continuous, labs, after_stat,  geom_vline, scale_color_manual, theme_gray, theme_xkcd, scale_color_identity, geom_hline, facet_wrap, scale_fill_discrete, scale_fill_manual,  scale_fill_hue, guides, guide_legend, ggtitle\n",
    "from mizani.formatters import percent_format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array as arr\n",
    "\n",
    "df_Old_New = pd.concat([df_Old , df_New])\n",
    "\n",
    "repeat_Old = ['Old Data']*len(Old_Data_Set)\n",
    "\n",
    "repeat_New = ['New Dta']*len(New_Data_Set)\n",
    "\n",
    "df_repeat_New = pd.DataFrame( {\"group\": repeat_New} ) \n",
    "\n",
    "df_repeat_Old = pd.DataFrame( {\"group\": repeat_Old} ) \n",
    "\n",
    "groups = pd.concat([df_repeat_Old , df_repeat_New])\n",
    "\n",
    "df_Old_New_groups = pd.concat([df_Old_New , groups], axis=1 ) \n",
    "\n",
    "df_Old_New_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "ggplot( df_Old_New_groups )\n",
    "+  aes(x='X1' , y =  after_stat('width*density'))\n",
    "+ geom_histogram(fill=\"plum\", color=\"black\", bins = 25)\n",
    "+  labs(x = \"X1\", y = \"Frecuencia Relativa\")\n",
    "+ scale_x_continuous( breaks = range(int(df_Old_New_groups['X1'].min()) , int(df_Old_New_groups['X1'].max()) , 50) ) \n",
    "+ scale_y_continuous( breaks = np.arange(0, 0.5, 0.02) )\n",
    "+ facet_wrap('group')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to consider the following definition of PAI (instead of use the variance, we will use the standard deviation. if we would consider the PAI definition with the variance, the process to compute it would have been very similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "The numerator is computing using the model trained with the Old Data (for the response and the predictors) but predicting the response variable using the New Data for the predictors.\n",
    "\n",
    "The denominator is computing using the model trained Old Data (for the response and the predictors) and also predicting the response variable using the Old Data for the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\n",
    "PAI = \\dfrac{\\dfrac{1}{N} \\sum_{i \\in NewData} \\widehat{Var}(\\hat{y}_i)}{\\dfrac{1}{n} \\sum_{i \\in OldData} \\widehat{Var}(\\hat{y}_i)}  \n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varcharProcessing(X, varchar_process = \"dummy_dropfirst\"):\n",
    "    \n",
    "    dtypes = X.dtypes\n",
    "\n",
    "    if varchar_process == \"drop\":   \n",
    "        X = X.drop(columns = dtypes[dtypes == np.object].index.tolist())\n",
    "\n",
    "    elif varchar_process == \"dummy\":\n",
    "        X = pd.get_dummies(X,drop_first=False)\n",
    "\n",
    "    elif varchar_process == \"dummy_dropfirst\":\n",
    "        X = pd.get_dummies(X,drop_first=True)\n",
    "\n",
    "    else: \n",
    "        X = pd.get_dummies(X,drop_first=True)\n",
    "    \n",
    "    X[\"intercept\"] = 1\n",
    "    cols = X.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    X = X[cols]\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Old['X2'] = df_Old['X2'].astype('category')\n",
    "df_Old['X3'] = df_Old['X3'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=100\n",
    "\n",
    "y_predictions_Old_Data = np.zeros((B , len(df_Old)))\n",
    "\n",
    "for i in range(0, B):\n",
    "\n",
    "    df_Old_BOOT_SAMPLE = df_Old.sample( n=len(df_Old) , random_state=i , replace=True )    # i-th boot sample\n",
    "\n",
    "    X_old = df_Old_BOOT_SAMPLE[['X1', 'X2', 'X3']]\n",
    "\n",
    "    y_old = df_Old_BOOT_SAMPLE['Y']\n",
    "\n",
    "    X_old = varcharProcessing(X_old, varchar_process = \"dummy_dropfirst\")\n",
    "\n",
    "    # We train the model with i-th boot sample of the Old Data:\n",
    "\n",
    "    Model_train_Old_data = LinearRegression().fit(X_old, y_old)\n",
    "\n",
    "    # y predictions using Model_train_Old_data with the i-th boot sample of the Old Data for the predictors \n",
    "\n",
    "    y_predictions_Old_Data[i, :] = Model_train_Old_data.predict(X_old)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $(k , r)$ element of the $nxB$ matrix `y_predictions_Old_Data` is $\\widehat{y_k}$ (the $Y$ estimation for the $k$-th individual of the sample) when the model is trained with the $r$-th boot sample of  `Old_Data_Set` \n",
    "\n",
    "Where: \n",
    "\n",
    "$n =$ len(Old\\_Data\\_Set)\n",
    "\n",
    "$B=$ nÂº of boot samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions_Old_Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the standard deviation of each column of the matrix , and we get an estimation of $Var(\\hat{y_i})$ for $i=1,...,n$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the i-th value of the following vector is $$\\widehat{Var}(\\hat{y_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # compute the variance by cols in an array\n",
    " \n",
    "y_predictions_Old_Data.var(axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_predictions_Old_Data.var(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the mean of the previous vector:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\dfrac{1}{n} \\cdot \\sum_{i=1,...,n} \\widehat{Var}(\\hat{y_i})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions_Old_Data.var(axis=0).mean()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAI_denominator = y_predictions_Old_Data.var(axis=0).mean()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the previous process but now we get the response predictions for de predictors of the New_Data_Set (this is so important, taking into a count the PAI definitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_New['X2'] = df_New['X2'].astype('category')\n",
    "df_New['X3'] = df_New['X3'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=100\n",
    "\n",
    "y_predictions_New_Data = np.zeros((B , len(New_Data_Set)))\n",
    "\n",
    "for i in range(0, B):\n",
    "\n",
    "    # i-th boot sample of the Old Data\n",
    "\n",
    "    df_Old_BOOT_SAMPLE = df_Old.sample( n=len(df_Old) , random_state=i , replace=True ) \n",
    " \n",
    "\n",
    "    \n",
    "    X_old = df_Old_BOOT_SAMPLE[['X1', 'X2', 'X3']]  \n",
    "\n",
    "    y_old =  df_Old_BOOT_SAMPLE['Y']\n",
    "\n",
    "    X_old = varcharProcessing(X_old, varchar_process = \"dummy_dropfirst\")\n",
    "\n",
    "    \n",
    "\n",
    "    X_new = df_New[['X1', 'X2', 'X3']]\n",
    "\n",
    "    y_new = df_New['Y']\n",
    "\n",
    "    X_new = varcharProcessing(X_new, varchar_process = \"dummy_dropfirst\") \n",
    "\n",
    "\n",
    "    Model_Old_Boot_Sample = LinearRegression().fit(X_old, y_old)\n",
    "\n",
    "    \n",
    "    # y predictions for the New Data using the model trained with the Old Data Boot Sample\n",
    "    # For this step with sk-learn is necessary X_new (test_set) columns have the same name as X_old (train set) columns\n",
    "\n",
    "    y_predictions_New_Data[i, :] = Model_Old_Boot_Sample.predict(X_new)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAI_numerator = y_predictions_New_Data.var(axis=0).mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAI = PAI_numerator / PAI_denominator\n",
    "PAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, in mean, the variance of the response predictions using the New Data Set is 1.17 times greater than the variance of the response predictions using the Old Data Set.\n",
    "\n",
    "Following the interpretation \"values less than 1.1 indicate no significant deterioration; values from 1.1 to 1.5 indicate a deterioration requiring further investigation, values exceeding 1.5 indicate the predictive\n",
    "accuracy of the model has deteriorated significantly\" exposed in the paper `The Population Accuracy Index: A New Measure of\n",
    "Population Stability for Model Monitoring` , so, the PAI value that we have got indicates a deterioration of  the model predictive\n",
    "accuracy , so could be recommendable to train again the model using the New Data Set instead the Old."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c63d8c7d738c2960218a10995aedf0a7f67a49a231e71037adf0440953cdb45b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
