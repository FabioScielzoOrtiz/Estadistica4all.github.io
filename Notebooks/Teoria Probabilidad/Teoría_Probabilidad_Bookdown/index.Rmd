--- 
title: "Introducción Teoría de la Probabilidad"
author: "Fabio Scielzo Ortiz"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  Este artículo es una introducción, más o menos formal, a la teoría de la probabilidad.
link-citations: yes
github-repo: rstudio/bookdown-demo
---



# Introducción


<div class="warning" style='background-color:#FCF2EC; color: #000000; border-left: solid #FE9554 7px; border-radius: 3px; size:1px ; padding:0.1em;'>
<span>
 
<p style='margin-left:10em;'>


- **Más artículos:    $\hspace{0.1cm}$ [Estadistica4all](https://estadistica4all.com/)**

- **Autor:** $\hspace{0.1cm}$ 
 [Fabio Scielzo Ortiz.](http://estadistica4all.com/autores/autores.html)

- **Si utilizas este artículo, por favor, cítalo:** 

$\hspace{1cm}$ Scielzo Ortiz, Fabio. (2023). Introducción a la teoría de la probabilidad. Estadistica4all.

- ***Se recomienda abrir el artículo en un ordenador o en una tablet.***


</p>
 
</p></span>
</div>


 <br> 
 
Este artículo es una introducción, más o menos formal, a la teoría de la probabilidad. 
 
 <br>


# Conceptos sobre Sucesos




## Experimento Aleatorio 

Un experimento aleatorio $\hspace{0.01cm}\varepsilon\hspace{0.01cm}$ es un  experimento que si se repite varias veces en las mismas condiciones puede ofrecer diferentes resultados.

<br>

## Espacio Muestral

El espacio muestal de un experimento aleatorio $\hspace{0.01cm}\varepsilon\hspace{0.01cm}$ es el conjunto de todos los resultados posibles de una realización del esperimento, y se denota por $\hspace{0.01cm}\Omega(\varepsilon)\hspace{0.01cm}$.


<br>

## Sucesos

Sea $\hspace{0.01cm}\varepsilon\hspace{0.01cm}$  un experimento aleatorio con espacio muestral $\hspace{0.01cm}\Omega(\varepsilon)\hspace{0.01cm}$ , 

- $A\hspace{0.01cm}$ es un suceso del experimento aleatorio $\hspace{0.01cm}\varepsilon\hspace{0.01cm}$ $\hspace{0.15cm}\Leftrightarrow\hspace{0.15cm}$ $\hspace{0.01cm}A\subset \Omega(\varepsilon)\hspace{0.01cm}$.

<br>

## Ocurrencia de sucesos 

Se realiza un experimento aleatorio $\hspace{0.01cm}\varepsilon\hspace{0.01cm}$ un número $\hspace{0.01cm}n\hspace{0.01cm}$ determinado de veces.

Sea $\hspace{0.01cm}w_i \in \Omega(\varepsilon)\hspace{0.01cm}$  el resultado obtenido en la $i$-esima realizacion del exprerimento aleatorio $\hspace{0.01cm}\varepsilon\hspace{0.01cm}$ , 


- El suceso $\hspace{0.01cm}A \subset \Omega(\varepsilon)\hspace{0.01cm}$ ha ocurrido en ese conjunto de realizaciones  $\hspace{0.15cm}\Leftrightarrow\hspace{0.15cm}$ $\hspace{0.01cm}w_i \in A \hspace{0.15cm},\hspace{0.15cm} \exists \hspace{0.08cm}  i=1,2,...,n$


<br>

### Suceso Imposible

Sea $\hspace{0.01cm}\emptyset\hspace{0.01cm}$ el conjunto vacio,  


- $\emptyset\hspace{0.01cm}$ es el suceso imposible, ya que nunca ocurre, en ningún conjunto de realizaciones de un experimento aleatorio.


<br>

### Suceso Seguro


- $\Omega(\varepsilon)\hspace{0.01cm}$ es el suceso seguro, ya que siempre ocurre, en cualquier conjunto de realizaciones de un experimento aleatorio.


<br>

## Suceso Simple


- $A\subset \Omega(\varepsilon)\hspace{0.01cm}$ es un suceso simple $\hspace{0.15cm}\Leftrightarrow\hspace{0.15cm}$ $\# A = 1$

<br>

## Suceso Compuesto


- $A\subset \Omega(\varepsilon)\hspace{0.01cm}$ es un suceso compuesto $\hspace{0.15cm}\Leftrightarrow\hspace{0.15cm}$ $\# A > 1$.

<br>

## Operaciones con sucesos


### Unión de sucesos

Sean $\hspace{0.01cm}A \hspace{0.05cm},\hspace{0.05cm} B \subset \Omega(\varepsilon)\hspace{0.01cm}$,

La unión de $\hspace{0.01cm}A\hspace{0.01cm}$ y $\hspace{0.01cm}B\hspace{0.01cm}$ se define como:  

$$A\cup B \hspace{0.01cm}=\hspace{0.01cm} \lbrace \hspace{0.03cm} w \in \Omega(\varepsilon)\hspace{0.1cm} : \hspace{0.1cm} w \in A \hspace{0.1cm} o\hspace{0.2cm} w \in B  \hspace{0.03cm}  \rbrace \\$$


**Observación:**

Sea $\hspace{0.01cm}w \in \Omega(\varepsilon)\hspace{0.01cm}$ el resultado de una realizacion del experimento aleatorio $\hspace{0.01cm}\varepsilon\hspace{0.01cm}$.

- Si $\hspace{0.01cm}w \in A\cup B \hspace{0.15cm} \Rightarrow\hspace{0.15cm}$  ocurre $\hspace{0.01cm}A\hspace{0.01cm}$ o $\hspace{0.01cm}B\hspace{0.01cm}$.

<br>

#### Propiedades de la unión de sucesos

- $A\cup B \hspace{0.02cm}=\hspace{0.02cm} B\cup A$  

- $A \cup A \hspace{0.02cm}=\hspace{0.02cm} A$ 

- $A \cup \emptyset  \hspace{0.02cm}=\hspace{0.02cm} A$  

- $A \cup \Omega(\varepsilon) \hspace{0.02cm}=\hspace{0.02cm} \Omega(\varepsilon)$  

- $A \cup A^c \hspace{0.02cm}=\hspace{0.02cm} \Omega(\varepsilon)$  

- Si $\hspace{0.01cm} A \cup B \hspace{0.02cm}=\hspace{0.02cm} \emptyset \hspace{0.15cm}\Rightarrow\hspace{0.15cm} \# A \cup B \hspace{0.02cm}=\hspace{0.02cm} \#A + \#B$  

- $\# A \cup B \hspace{0.02cm}=\hspace{0.02cm} \# A \hspace{0.02cm}+\hspace{0.02cm} \# B \hspace{0.02cm}-\hspace{0.02cm} \# A\cap B$


<br>

### Intersección de sucesos

Sean $\hspace{0.08cm}A \hspace{0.05cm},\hspace{0.05cm} B \subset \Omega(\varepsilon)\hspace{0.08cm}$,

La intersección de $\hspace{0.08cm}A\hspace{0.08cm}$ y $\hspace{0.08cm}B\hspace{0.08cm}$ se define como:  

$$A\cap B \hspace{0.08cm}=\hspace{0.08cm} \lbrace \hspace{0.08cm} w \in \Omega(\varepsilon) \hspace{0.15cm} : \hspace{0.15cm} w \in A \hspace{0.2cm} y\hspace{0.2cm} w \in B   \hspace{0.08cm} \rbrace \\$$

 
**Observación:**

Sea $\hspace{0.02cm}w \in \Omega(\varepsilon)\hspace{0.02cm}$ el resultado de una realizacion del experimento aleatorio $\hspace{0.01cm}\varepsilon\hspace{0.01cm}$.

- Si $\hspace{0.08cm}w \in A\cap B \hspace{0.15cm}\Rightarrow\hspace{0.15cm}$  ocurren $\hspace{0.02cm}A\hspace{0.02cm}$ y $\hspace{0.02cm}B\hspace{0.02cm}$.

<br>

#### Propiedades de la intersección de sucesos

- $A\cap B \hspace{0.02cm}=\hspace{0.02cm} B \cap A$  

- $A\cap A \hspace{0.02cm}=\hspace{0.02cm} A$ 

- $A \cap \emptyset \hspace{0.02cm}=\hspace{0.02cm} \emptyset$ 

- $A \cap \Omega(\varepsilon) \hspace{0.02cm}=\hspace{0.08cm} A$ 

- $A \cap A^c \hspace{0.02cm}=\hspace{0.02cm} \emptyset$


<br>

### Diferencia de sucesos

Sean $\hspace{0.01cm}A \hspace{0.05cm},\hspace{0.05cm} B \subset \Omega(\varepsilon)\hspace{0.02cm}$,

La diferencia de $\hspace{0.01cm}A\hspace{0.01cm}$ y $\hspace{0.01cm}B\hspace{0.01cm}$ se define como:   

$$A - B \hspace{0.08cm}=\hspace{0.08cm} \lbrace \hspace{0.08cm} w \in \Omega(\varepsilon) \hspace{0.15cm} : \hspace{0.15cm} w \in A \hspace{0.2cm} y \hspace{0.2cm} w \notin B   \hspace{0.08cm} \rbrace \\$$


 
**Observación:**

Sea $\hspace{0.08cm}w \in \Omega(\varepsilon)\hspace{0.08cm}$ el resultado de una realizacion del experimento aleatorio $\hspace{0.08cm}\varepsilon\hspace{0.08cm}$ .

- Si $\hspace{0.08cm}w \in A - B$ $\hspace{0.15cm}\Rightarrow\hspace{0.15cm}$  ocurre $\hspace{0.08cm}A\hspace{0.08cm}$ pero no $\hspace{0.08cm}B\hspace{0.08cm}$.


<br>

### Propiedades de la diferencia de sucesos

- $A-B \hspace{0.08cm}\neq\hspace{0.08cm} B - A$  , en general. 

- $A-A \hspace{0.08cm}=\hspace{0.08cm} \emptyset$  

- $A-\emptyset \hspace{0.08cm}=\hspace{0.08cm} A$  

- $A - \Omega(\varepsilon) \hspace{0.08cm}=\hspace{0.08cm} \emptyset$  

- $A- B \hspace{0.08cm}=\hspace{0.08cm} A - (A\cap B)$  

- $A-B \hspace{0.08cm}=\hspace{0.08cm} A\cap B^c$



<br>


### Suceso Contrario

Sea $\hspace{0.08cm}A   \subset \Omega(\varepsilon)\hspace{0.08cm}$,

El suceso contrario de $\hspace{0.08cm}A\hspace{0.08cm}$ y $\hspace{0.08cm}B\hspace{0.08cm}$ se define como:  

$$A^c  \hspace{0.08cm}=\hspace{0.08cm} \lbrace \hspace{0.08cm} w \in \Omega(\varepsilon) \hspace{0.15cm} : \hspace{0.15cm} w \notin A  \hspace{0.08cm} \rbrace \\$$

#### Propiedades del suceso contrario

- $\emptyset^c \hspace{0.08cm}=\hspace{0.08cm} \Omega(\varepsilon)$  

- $\Omega(\varepsilon)^c \hspace{0.08cm}=\hspace{0.08cm} \emptyset$  

- $(A^c)^c \hspace{0.08cm}=\hspace{0.08cm} A$




<br>

## Leyes de teoria de conjuntos útiles para sucesos


### Leyes Asociativas

- $A \cup (B \cup C) \hspace{0.08cm}=\hspace{0.08cm} (A \cup B ) \cup C$  

- $A \cap (B \cap C) \hspace{0.08cm}=\hspace{0.08cm} (A \cap B ) \cap C$  

### Leyes Distributivas

- $A \cap (B \cup C) \hspace{0.08cm}=\hspace{0.08cm} (A \cap B) \cup (A \cap C)$ 

- $A \cup ( B \cap C) \hspace{0.08cm}=\hspace{0.08cm} (A \cup B) \cap (A \cup C)$  

### Leyes de Morgan

- $(A \cup B)^c \hspace{0.08cm}=\hspace{0.08cm} A^c \cap B^c$  

- $(A \cap B)^c \hspace{0.08cm} =\hspace{0.08cm} A^c \cup B^c$  


 

### Conjunto potencia del espacio muestral

El conjunto potencia del espacio muestral $\hspace{0.08cm}\Omega\hspace{0.08cm}$ es:

$$2^\Omega \hspace{0.08cm}=\hspace{0.08cm} \lbrace\hspace{0.08cm} A \hspace{0.15cm}:\hspace{0.15cm} A \subset \Omega  \hspace{0.08cm}\rbrace\\$$

Se cumple la siguiente propiedad:

$$\# 2 ^\Omega \hspace{0.08cm}=\hspace{0.08cm} 2 ^{\# \Omega}$$ 


<br>


## Sucesos Disjuntos

- $A\hspace{0.08cm}$ y $\hspace{0.08cm}B\hspace{0.08cm}$ son sucesos disjuntos $\hspace{0.15cm}\Leftrightarrow\hspace{0.15cm} A\cap B \hspace{0.08cm}=\hspace{0.08cm} \emptyset$

- $A_1\hspace{0.08cm},\hspace{0.08cm} A_2\hspace{0.08cm} ,..., \hspace{0.08cm}A_n\hspace{0.08cm}$ son sucesos disjuntos $\hspace{0.15cm}\Leftrightarrow \hspace{0.15cm} A\hspace{0.08cm}\cap\hspace{0.08cm} A_2 \hspace{0.08cm}\cap \dots \cap\hspace{0.08cm} A_n \hspace{0.08cm}=\hspace{0.08cm} \emptyset$


<br>


## Sucesos Disjuntos dos a dos  

- $A_1 \hspace{0.08cm},\hspace{0.08cm} A_2 \hspace{0.08cm},..., \hspace{0.08cm} A_n\hspace{0.08cm}$ son sucesos disjuntos dos a dos $\hspace{0.15cm}\Leftrightarrow\hspace{0.15cm} A_i \cap A_j \hspace{0.08cm}=\hspace{0.08cm} \emptyset \hspace{0.25cm} , \hspace{0.25cm}\forall \hspace{0.08cm}i \neq j = 1,...,n$


<br>

# Concepto de probabilidad 

## Probabilidad de Laplace

Sea $\hspace{0.02cm}\varepsilon\hspace{0.02cm}$ un experimento aleatorio con espacio muestral $\hspace{0.02cm}\Omega\hspace{0.02cm}$ , tal que $\hspace{0.02cm}\# \Omega < \infty\hspace{0.02cm}$  y $\hspace{0.02cm}\# \Omega \neq 0\hspace{0.02cm}$.

La probabilidad, en el sentido de Laplace, de un suceso $\hspace{0.01cm}A \subset \Omega\hspace{0.01cm}$ se define como: 

$$P(A) \hspace{0.08cm} =\hspace{0.08cm} \dfrac{\# A}{\# \Omega} \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\text{casos favorables}}{\text{casos posibles}}\\$$

### Propiedades de la probabilidad clásica:

- $P(\Omega) \hspace{0.08cm}=\hspace{0.08cm} 1$  

- $P(A) \hspace{0.08cm}\geq\hspace{0.08cm} 0$  

- $P(A \cup B) \hspace{0.08cm}=\hspace{0.08cm} P(A) + P(B) + P(A\cap B)$  

- Si $\hspace{0.08cm}A\hspace{0.08cm}$ y $\hspace{0.08cm}B\hspace{0.08cm}$ son sucesos disjuntos $\hspace{0.08cm}(A\cap B = \emptyset)\hspace{0.08cm}$ , entonces:

$$P(A\cup B)\hspace{0.08cm}=\hspace{0.08cm} P(A) + P(B)$$


<br>


## Probabilidad Frecuentista

Sea $\hspace{0.02cm}\varepsilon\hspace{0.02cm}$ un experimento aleatorio con espacio muestral $\hspace{0.02cm}\Omega\hspace{0.02cm}$ y sea $\hspace{0.02cm}A\subset \Omega\hspace{0.02cm}$.

Si se realiza $\hspace{0.02cm}n\hspace{0.02cm}$ veces el experimento $\hspace{0.02cm}\varepsilon\hspace{0.02cm}$ y $\hspace{0.02cm}n(A)\hspace{0.02cm}$ es el número de veces que ocurre el suceso $\hspace{0.02cm}A\hspace{0.02cm}$ en las $\hspace{0.02cm}n\hspace{0.02cm}$ realizaciones del experimento, la probabilidad en sentido frecuentista de $\hspace{0.02cm}A\hspace{0.02cm}$ se define como:  



$$P(A) \hspace{0.08cm}=\hspace{0.08cm} \underset{n \rightarrow \infty}{lim}\left( \dfrac{n(A)}{n} \right)$$



Es decir, la probabilidad de un suceso de un experimento aleatorio es la proporción de veces que ocurre cuando el experimento se repite muchas veces. 


### Propiedades

- $P(\Omega) \hspace{0.08cm} =\hspace{0.08cm} 1$  
 
- $P(\emptyset)\hspace{0.08cm}=\hspace{0.08cm}0$  

- $P(A) \hspace{0.08cm}\geq\hspace{0.08cm} 0$  

- $P(A\cup B) \hspace{0.08cm}=\hspace{0.08cm} P(A) \hspace{0.08cm}+\hspace{0.08cm} P(B) \hspace{0.08cm}-\hspace{0.08cm} P(A\cap B)$   

-  Si $\hspace{0.02cm}A\hspace{0.02cm}$ y $\hspace{0.02cm}B\hspace{0.02cm}$ son sucesos disjuntos $\hspace{0.02cm}(A\cap B = \emptyset)\hspace{0.02cm}$ , entonces:

$$P(A\cup B) \hspace{0.08cm}=\hspace{0.08cm} P(A) \hspace{0.08cm}+\hspace{0.08cm} P(B)$$


<br>

## Probabilidad Axiomatica de Kolmogorov

Sea $\hspace{0.02cm}\varepsilon\hspace{0.02cm}$ un experimento aleatorio con espacio muestral $\hspace{0.02cm}\Omega\hspace{0.02cm}$.


$P\hspace{0.02cm}$ es una medida de probabilidad definida sobre $\hspace{0.08cm}\Omega\hspace{0.1cm}$ si y solo si :

- $P \hspace{0.08cm}:\hspace{0.08cm} 2^\Omega \hspace{0.08cm}\rightarrow\hspace{0.08cm} \mathbb{R}$  

- $P(A) \hspace{0.08cm}\geq\hspace{0.08cm} 0$  

- $P(\Omega) \hspace{0.08cm}=\hspace{0.08cm} 1$  

- Si $\hspace{0.02cm}A_1,A_2,A_3,... \subset \Omega\hspace{0.02cm}$  , entonces:

$$P(A_1 \cup A_2 \cup A_3 \cup ...) \hspace{0.08cm}=\hspace{0.08cm} P(A_1)\hspace{0.08cm}+\hspace{0.08cm}P(A_2)\hspace{0.08cm}+\hspace{0.08cm}P(A_3)\hspace{0.08cm}+ \hspace{0.05cm}\dots \\$$


### Propiedades


- $P(\emptyset) \hspace{0.08cm}=\hspace{0.08cm} 0$  

- Si $\hspace{0.08cm}A_1,A_2,...,A_n \subset \Omega\hspace{0.08cm}$  , entonces:

$$P(A_1 \cup A_2 \cup ...\cup A_n)\hspace{0.08cm} =\hspace{0.08cm} P(A_1)+P(A_2)+...+P(A_n) \hspace{0.08cm}=\hspace{0.08cm} \sum_{i=1}^n P(A_i)$$  

- $P(A^c) \hspace{0.08cm}=\hspace{0.08cm} 1 - P(A)$  


- Si $\hspace{0.08cm}A \subseteq B \hspace{0.15cm}\Rightarrow\hspace{0.15cm} P(A) \hspace{0.08cm}\geq\hspace{0.08cm} P(B)$ 

- Si $\hspace{0.08cm}A \subseteq B \hspace{0.15cm}\Rightarrow\hspace{0.15cm} P(B-A)\hspace{0.08cm} =\hspace{0.08cm} P(B)-P(A)$ 

- $0 \hspace{0.08cm}\geq\hspace{0.08cm} P(A) \hspace{0.08cm}\geq\hspace{0.08cm} 1$  

- $P(A-B) \hspace{0.08cm}=\hspace{0.08cm} P(A) - P(A\cap B)$  

- $P(A \cup B)\hspace{0.08cm}=\hspace{0.08cm} P(A) \hspace{0.08cm}+\hspace{0.08cm} P(B) \hspace{0.08cm}-\hspace{0.08cm} P(A\cap B)$



<br>

# Probabilidad Condicionada

La probabilidad de $\hspace{0.02cm}A\hspace{0.02cm}$ condicionada a $\hspace{0.02cm}B\hspace{0.02cm}$ se define como:  

$$P(A \hspace{0.08cm}|\hspace{0.08cm} B) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\hspace{0.08cm}P(A\cap B)\hspace{0.08cm}}{P(B)} \\$$



## Propiedades

- $P(\emptyset \hspace{0.03cm} | \hspace{0.03cm} B) \hspace{0.03cm}=\hspace{0.03cm} 0$  

- Si $\hspace{0.03cm}A_1 , A_2,...,A_n\hspace{0.03cm}$ son disjuntos dos a dos , entonces:

$$P(A_1 \cup A_2 \cup ... \cup A_n \hspace{0.03cm} | \hspace{0.03cm} B) \hspace{0.03cm}=\hspace{0.03cm} P(A_1 \hspace{0.03cm}|\hspace{0.03cm} B) \hspace{0.03cm}+\hspace{0.03cm} P(A_2 \hspace{0.03cm}|\hspace{0.03cm} B)  \hspace{0.03cm}+ ... +\hspace{0.03cm} P(A_n \hspace{0.03cm}|\hspace{0.03cm} B) \hspace{0.03cm}=\hspace{0.03cm} \sum_{i=1}^n \hspace{0.03cm} P(A_i \hspace{0.03cm}|\hspace{0.03cm} B) \\$$  

- $P(A^c \hspace{0.03cm}|\hspace{0.03cm} B) \hspace{0.03cm}=\hspace{0.03cm} 1 - P(A \hspace{0.03cm}|\hspace{0.03cm} B)$  

- Si $A \subseteq D \hspace{0.1cm}\Rightarrow\hspace{0.1cm} P(A \hspace{0.03cm}|\hspace{0.03cm} B) \geq P( D \hspace{0.03cm}|\hspace{0.03cm} B)$  

- Si $\hspace{0.03cm}A \subseteq D \hspace{0.1cm}\Rightarrow\hspace{0.1cm} P(D-A \hspace{0.03cm}|\hspace{0.03cm} B) \hspace{0.03cm}=\hspace{0.03cm} P( D \hspace{0.03cm}|\hspace{0.03cm} B) - P(A \hspace{0.03cm}|\hspace{0.03cm} B)$  

- $0 \geq P(A \hspace{0.03cm}|\hspace{0.03cm} B)   \geq 1$ 

- $P(A-D \hspace{0.03cm}|\hspace{0.03cm} B) \hspace{0.03cm}=\hspace{0.03cm} P(A \hspace{0.03cm}|\hspace{0.03cm} B) - P(A\cap D \hspace{0.03cm}|\hspace{0.03cm} B)$  

- $P(A \cup B) \hspace{0.03cm}=\hspace{0.03cm} P(A) + P(B) - P(A\cap B)$

<br>

# Partición del espacio muestral

Sea $\hspace{0.01cm}\varepsilon\hspace{0.01cm}$ un experimento aleatorio con espacio muestral $\hspace{0.02cm}\Omega\hspace{0.02cm}$.

Sean $\hspace{0.02cm}A_1,A_2, ..., A_n \subseteq \Omega\hspace{0.02cm}$.

$A_1,A_2, ..., A_n\hspace{0.02cm}$ es una partición de $\hspace{0.02cm}\Omega\hspace{0.2cm}$  si y solo si:

-  $B_i \hspace{0.08cm}\neq\hspace{0.08cm} \Omega \hspace{0.18cm},\hspace{0.16cm} \forall\hspace{0.08cm} i = 1,...,n$  


- $B_i \cap B_j \hspace{0.08cm}=\hspace{0.08cm} \emptyset \hspace{0.18cm},\hspace{0.18cm} \forall\hspace{0.08cm} i \neq j = 1,...,n$  

- $B_1 \hspace{0.05cm}\cup ... \cup\hspace{0.05cm} B_n \hspace{0.08cm}=\hspace{0.08cm} \Omega$


<br>


# Teorema de la Probabilidad Total

Sea $\hspace{0.02cm}A \subseteq \Omega\hspace{0.02cm}$.

Si $\hspace{0.02cm}B_1,...,B_n\hspace{0.02cm}$ una partición de $\hspace{0.02cm}\Omega\hspace{0.02cm}$ , entonces:

$$P(A)\hspace{0.08cm}=\hspace{0.08cm}P(A\cap B_1) \hspace{0.08cm}+\hspace{0.08cm} P(A\cap B_2) \hspace{0.08cm}+\dots +\hspace{0.08cm} P(A \cap B_n) \hspace{0.08cm}=\hspace{0.08cm} \sum_{i=1}^n P(A \cap B_i)$$

<br>

## Teorema de la probabilidad total con probabilidad condicionada

Sea $\hspace{0.02cm}A \subseteq \Omega\hspace{0.02cm}$,

Si $\hspace{0.02cm}B_1,...,B_n\hspace{0.02cm}$ una partición de $\hspace{0.02cm}\Omega\hspace{0.02cm}$, con $\hspace{0.02cm}P(B_i)>0\hspace{0.02cm}$ , entonces:


$$P(A) \hspace{0.08cm}=\hspace{0.08cm} P(A | B_1)\cdot P(B_1) \hspace{0.08cm}+ \dots + \hspace{0.08cm}P(A | B_n)\cdot P(B_n)\hspace{0.08cm}=\hspace{0.08cm} \sum_{i=1}^n P(A |  B_i)\cdot P(B_i)$$

<br>

# Teorema de Bayes

Sean $\hspace{0.08cm}A,B \subset \Omega\hspace{0.08cm}$ , con $\hspace{0.08cm}P(B)\neq 0\hspace{0.08cm}$.  

$$P(A |B)\hspace{0.08cm} =\hspace{0.08cm} \dfrac{\hspace{0.1cm}P(B|A)\cdot P(A)\hspace{0.1cm}}{P(B)}$$


<br>

# Independencia de sucesos


## Independencia de un par de sucesos

- $A\hspace{0.02cm}$ y $\hspace{0.02cm}B\hspace{0.02cm}$ son sucesos independientes $\hspace{0.15cm}\Leftrightarrow\hspace{0.15cm}$ $P(A\cap B) \hspace{0.08cm}=\hspace{0.08cm} P(A)\cdot P(B)$ $\\$ 

### Propiedades

- $A\hspace{0.02cm}$ y $\hspace{0.02cm}B\hspace{0.02cm}$ son sucesos independientes $\hspace{0.15cm}\Leftrightarrow\hspace{0.15cm}$ $\hspace{0.08cm}P(A | B) \hspace{0.08cm}=\hspace{0.08cm} P(A)$ $\hspace{0.15cm}\Leftrightarrow\hspace{0.15cm}$ $P(B | A)\hspace{0.08cm} =\hspace{0.08cm} P(B)$  

- Si $\hspace{0.02cm}A\hspace{0.02cm}$ y $\hspace{0.02cm}B\hspace{0.02cm}$ son sucesos independientes $\hspace{0.15cm}\Rightarrow\hspace{0.15cm}$ $A^c\hspace{0.02cm}$ y $\hspace{0.02cm}B^c\hspace{0.02cm}$ también lo son. 


- Sucesos disjuntos no implica independientes. 

- Sucesos independientes no implica disjuntos.


<br>

## Independencia de múltiples sucesos 


$A_1 , A_2, ..., A_n\hspace{0.02cm}$ son sucesos independientes, si y solo si:

- Son independientes $\hspace{0.02cm}2\hspace{0.02cm}$ a $\hspace{0.02cm}2\hspace{0.02cm}$.  

- Son independientes $\hspace{0.02cm}3\hspace{0.02cm}$ a $\hspace{0.02cm}3\hspace{0.02cm}$.  

$\hspace{1cm} \dots$  

- Son independientes $\hspace{0.02cm}n\hspace{0.02cm}$ a $\hspace{0.02cm}n\hspace{0.02cm}$. $\\$  


Lo que equivale a que se cumplan las siguientes condiciones:


- $P(A_i \cap A_j) \hspace{0.08cm}=\hspace{0.08cm} P(A_i)\cdot P(A_j) \hspace{0.2cm},\hspace{0.2cm} \forall\hspace{0.08cm} i \neq j = 1,...,n$ 

- $P(A_i \cap A_j \cap A_r) \hspace{0.08cm}= \hspace{0.08cm} P(A_i)\cdot P(A_j)\cdot P(A_r) \hspace{0.2cm},\hspace{0.2cm} \forall\hspace{0.08cm}  i \neq j \neq r = 1,...,n$  

$\hspace{1cm}\dots$  

- $P(A_1 \cap A_2 \cap ... \cap A_n)\hspace{0.08cm}=\hspace{0.08cm}P(A_1)\cdot P(A_2)\cdot \dots \cdot P(A_n)$


<br>

# Sigma Algrebra

$\sigma (\Omega)\hspace{0.08cm}$ es un sigma-algebra de $\hspace{0.08cm}\Omega\hspace{0.08cm}$, si y solo si:

- $\sigma (\Omega) \hspace{0.08cm}=\hspace{0.08cm} \lbrace \hspace{0.08cm} A \hspace{0.08cm}:\hspace{0.08cm} A\subset \Omega \hspace{0.08cm}\rbrace$  


- $\Omega \in \sigma (\Omega)$  

- Si $A \in \sigma(\Omega) \hspace{0.08cm}\Rightarrow\hspace{0.08cm} A^c \in \sigma (\Omega)$  

- Si $A_1, A_2 , A_3 ,\dots \hspace{0.08cm} \in\hspace{0.08cm} \sigma (\Omega)  \hspace{0.15cm}\Rightarrow\hspace{0.15cm} A_1 \cup A2 \cup A_3 \cup \dots \hspace{0.08cm}\in\hspace{0.08cm} \sigma (\Omega) \\$  


**Observación:**

La condición primera significa que   $\hspace{0.08cm}\sigma (\Omega)\hspace{0.08cm}$ es un conjunto de subconjuntos de $\hspace{0.08cm}\Omega\hspace{0.08cm}$.

Otra forma equivalente de expresar esta condición es: $\hspace{0.08cm}\sigma (\Omega) \subseteq 2^\Omega$ $\\$


## Propiedades

- $\emptyset \in \sigma (\Omega)$  

- Si $\hspace{0.02cm}A_1, A_2, A_3,\dots \hspace{0.03cm} \in \hspace{0.03cm}\sigma (\Omega) \hspace{0.15cm} \Rightarrow\hspace{0.15cm} A_1 \cap A_2 \cap A_3 \cap \dots \hspace{0.03cm} \in \hspace{0.03cm} \sigma (\Omega)$  

- Si $\hspace{0.02cm}A,B \in \sigma (\Omega)\hspace{0.15cm} \Rightarrow \hspace{0.15cm} A-B \hspace{0.02cm}\in\hspace{0.02cm} \sigma (\Omega)$


<br>

# Variables aleatorias


$X\hspace{0.02cm}$ es una variable aleatoria (v.a.) definida sobre $\hspace{0.02cm}(\Omega , \sigma (\Omega))\hspace{0.02cm}$, si y solo si:

- $X \hspace{0.08cm}:\hspace{0.08cm} \Omega \hspace{0.08cm}\rightarrow\hspace{0.08cm} R$ 
   

- $\lbrace \hspace{0.08cm} w \in \Omega \hspace{0.15cm}:\hspace{0.15cm} X(w)=x \hspace{0.08cm} \rbrace \hspace{0.08cm} \in \hspace{0.08cm} \sigma(\Omega) \hspace{0.2cm},\hspace{0.2cm} \forall \hspace{0.08cm} x \in \mathbb{R}$ $\\$ 





**Observaciones:**

- Formalmente una v.a. ni es una variable ni es aleatoria, es una funcion que asigna números a los elementos de un espacio muestral.  

- Definiciones adicionales:

$$\mathcal{X}=x \hspace{0.13cm} := \hspace{0.13cm} \lbrace \hspace{0.08cm} w\in \Omega \hspace{0.15cm} : \hspace{0.15cm} \mathcal{X}(w) = x \hspace{0.08cm} \rbrace$$

$$ \mathcal{X}\in A  \hspace{0.13cm} := \hspace{0.13cm} \lbrace \hspace{0.08cm} w\in \Omega \hspace{0.15cm} : \hspace{0.15cm} \mathcal{X}(w) \in A \hspace{0.08cm} \rbrace$$

<br>

## Variables Aleatorias y Probabilidad

Dada una medida de probabilidad $\hspace{0.04cm}P \hspace{0.08cm}:\hspace{0.08cm} \sigma(\Omega) \hspace{0.08cm}\rightarrow\hspace{0.08cm} [0,1]\hspace{0.08cm}$ .


Se cumple que $\hspace{0.03cm}P\hspace{0.02cm}$ está definida en $\hspace{0.02cm}\mathcal{X}=x\hspace{0.08cm}$ , para todo $\hspace{0.02cm}x\in \mathbb{R}$

Es decir, $\hspace{0.02cm}P(\mathcal{X}=x) \in [0,1] \hspace{0.15cm},\hspace{0.15cm} \forall \hspace{0.08cm} x \in \mathbb{R}$ y se cumplen todas las propiedades de la probabilidad en sentido axiomatico.

<br>

## Variables aleatorias discretas y continuas 

Sea $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ una variable aleatoria,


- $\mathcal{X}\hspace{0.02cm}$ es **discreta** $\hspace{0.15cm}\Leftrightarrow \hspace{0.15cm} Im(\mathcal{X})\hspace{0.02cm}$ es un conjunto **contable**.  

- $\mathcal{X}\hspace{0.02cm}$ es **continua** $\hspace{0.15cm}\Leftrightarrow\hspace{0.15cm} Im(X)$ es un conjunto **no contable**.




Donde:

$$Im(\mathcal{X}) \hspace{0.08cm}=\hspace{0.08cm} \left\lbrace \hspace{0.03cm} x \in \mathbb{R} \hspace{0.15cm} : \hspace{0.15cm} \exists \hspace{0.08cm} w \in \Omega \hspace{0.08cm}, \hspace{0.08cm} \mathcal{X}(w) = x \hspace{0.03cm}\right\rbrace \\$$ 

Es decir, $\hspace{0.03cm}Im(\mathcal{X})\hspace{0.03cm}$ es la imagen de la v.a. $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$. Notese que $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$ es una función, luego tiene dominio e imagen.  


**Observación:**

Un conjunto $\hspace{0.03cm}A\hspace{0.03cm}$ es **contable** $\hspace{0.03cm}\Leftrightarrow\hspace{0.08cm}$ Existe alguna función $\hspace{0.03cm}f \hspace{0.08cm}:\hspace{0.08cm} \mathbb{N} \hspace{0.08cm}\rightarrow\hspace{0.08cm} A\hspace{0.08cm}$ que sea **biyectiva** .

Es decir, que para todo $\hspace{0.03cm}y\in Im(f)=A\hspace{0.03cm}$ , existe un único número   $\hspace{0.03cm}x \in Dom(f)=\mathbb{N}\hspace{0.03cm}$ tal que $\hspace{0.03cm}f(x)=y\hspace{0.08cm}$.


<br>

# Función de probabilidad

Sea $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ una variable aleatoria **discreta**,

$P_\mathcal{X}\hspace{0.02cm}$ es la función de probabilidad de la v.a. $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$, si y solo si:  $\\$


$$P_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} P(\mathcal{X}=x) \cdot \mathbb{I}\hspace{0.08cm}[\hspace{0.08cm} x\in Im(\mathcal{X})\hspace{0.08cm} ]$$


Donde:


 $\mathbb{I}\hspace{0.08cm}[\hspace{0.02cm} x\in Im(\mathcal{X})\hspace{0.02cm} ]\hspace{0.08cm}$  es igual a $\hspace{0.08cm}1\hspace{0.08cm}$ cuando $\hspace{0.03cm}x\in Im(\mathcal{X})\hspace{0.03cm}$   y $\hspace{0.03cm}0\hspace{0.03cm}$ cuando $x\notin Im(\mathcal{X})$.  
 
 
<br> 
 
## Propiedades

- $P_\mathcal{X} (x) \hspace{0.08cm} \geq \hspace{0.08cm} 0  \hspace{0.15cm} , \hspace{0.15cm} \forall x \in \mathbb{R}$  
 
- $\sum_{x\in\mathbb{R}}\hspace{0.08cm}  P_\mathcal{X}(x) \hspace{0.08cm}= \hspace{0.08cm} 1$  
 
- Si  $P(\mathcal{X}\in A)\hspace{0.08cm} =\hspace{0.08cm} \sum_{x\in A} \hspace{0.08cm} P_\mathcal{X}(x) \hspace{0.2cm} , \hspace{0.2cm} \forall \hspace{0.02cm} A \subset \mathbb{R}$ 
 
<br> 
 
# Función de densidad

Sea $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$ una variable aleatoria **continua**,


 
$f_\mathcal{X}\hspace{0.02cm}$ es la funcion de densidad de la v.a. $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$, si y solo si:  $\\$

- $f_\mathcal{X}(x) \hspace{0.08cm}\geq\hspace{0.08cm} 0 \hspace{0.2cm} ,  \hspace{0.2cm} \forall \hspace{0.08cm} x \in Im(\mathcal{X})$  

- $f\hspace{0.03cm}$ es integrable. 

- $P(\mathcal{X}\in A) \hspace{0.08cm}=\hspace{0.08cm} \int_{x\in A} f(x)\cdot dx \hspace{0.2cm} , \hspace{0.2cm} \forall \hspace{0.02cm}  A \subset \mathbb{R}$  
 
 
 <br>
 
## Propiedades

- $\int_{-\infty}^{\infty} f(x)\cdot dx \hspace{0.08cm}=\hspace{0.08cm} 1$  

- Toda función no negativa, integrable y cuya integral entre $\hspace{0.08cm}-\infty\hspace{0.08cm}$ y $\hspace{0.08cm}\infty\hspace{0.08cm}$ sea $\hspace{0.02cm}1\hspace{0.02cm}$ es una función de densidad.

- $P(\hspace{0.03cm}\mathcal{X} \in (a,b) \hspace{0.03cm}) \hspace{0.08cm}=\hspace{0.08cm} \int_{a}^b f_{\mathcal{X}}(x) \cdot dx$

- $P(\hspace{0.03cm}\mathcal{X} \in [a,b] \hspace{0.03cm}) \hspace{0.08cm}=\hspace{0.08cm} \int_{a}^b f_{\mathcal{X}}(x) \cdot dx$

- $P(\hspace{0.03cm}\mathcal{X} = x \hspace{0.03cm}) \hspace{0.08cm}=\hspace{0.08cm}P(\hspace{0.03cm}\mathcal{X} \in [x,x] \hspace{0.03cm}) \hspace{0.08cm}=\hspace{0.08cm} \int_{x}^x f_{\mathcal{X}}(x) \cdot dx \hspace{0.08cm}=\hspace{0.08cm} 0 \hspace{0.2cm} , \hspace{0.2cm} \forall \hspace{0.08cm} x \in \mathbb{R}$
 

 

<br>
 
# Función de distribución


$F_\mathcal{X}\hspace{0.03cm}$ es la función de distribución de la v.a.  $\hspace{0.02cm}\mathcal{X} \hspace{0.2cm}\Leftrightarrow\hspace{0.2cm}$ $F_\mathcal{X}(x) \hspace{0.05cm}=\hspace{0.05cm} P(\mathcal{X} \leq x)$  


## Propiedades  


- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ es una v.a. **continua**:

$$F_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} P(\mathcal{X} \leq x) \hspace{0.08cm}=\hspace{0.08cm} \int_{-\infty}^{x} f_X(z)\cdot dz$$



- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ es una v.a. **discreta** : 


$$F_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} P(\mathcal{X} \leq x) \hspace{0.08cm}=\hspace{0.08cm} \sum_{z \leq x} P(\mathcal{X}=z)$$
 
 
- $\underset{x\rightarrow \infty}{lim} F_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} 1$  
 
- $\underset{x\rightarrow -\infty}{lim} F_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.02cm} 0$  

- $F_\mathcal{X}\hspace{0.04cm}$ es una función creciente.




<br>

# Esperanza de una variable aleatoria

La esperanza de una v.a. $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$  se define del siguiente modo:  

- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ es una v.a. **discreta** :


    $$E[\mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} \sum_{x\in \mathbb{R}} x \cdot P(\mathcal{X}=x)$$


- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ es una v.a. **continua** : 

    $$E[\mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} \int_{-\infty}^{\infty} x \cdot f_\mathcal{X}(x) \cdot dx\\$$ 


## Propiedades

- $E[a]\hspace{0.08cm}=\hspace{0.08cm}a$  

- $E[b\cdot \mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} b \cdot E[\mathcal{X}]$  

- $E[a + b\cdot \mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} a + b \cdot E[\mathcal{X}]$  

- $E[\mathcal{X}_1 + \mathcal{X}_2 + ...+ \mathcal{X}_n] \hspace{0.08cm}=\hspace{0.08cm} E[\mathcal{X}_1]+E[\mathcal{X}_2] + ... + E[\mathcal{X}_n]$  

- $E[g_1(\mathcal{X}_1) + ... + g_n(\mathcal{X}_n)]\hspace{0.08cm}=\hspace{0.08cm}E[g_1(\mathcal{X}_1)]+...+E[g_n(\mathcal{X}_n)]$  

- Si $\hspace{0.08cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.08cm}$ son independientes, entonces:

$$E[\mathcal{X}_1 \cdot \mathcal{X}_2 \cdot ...\cdot \mathcal{X}_n]= E[\mathcal{X}_1]\cdot E[\mathcal{X}_2] \cdot ... \cdot E[\mathcal{X}_n]\\$$ 

- Si $\hspace{0.08cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.08cm}$ son independientes, entonces:

$$E[g_1(\mathcal{X}_1) \cdot ... \cdot g_n(\mathcal{X}_n)]=E[g_1(\mathcal{X}_1)]\cdot ...\cdot E[g_n(\mathcal{X}_n)]\\$$  

- Fórmulas de transferencia:

    Si $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$ es **discreta**:


    $$E[g(\mathcal{X})] \hspace{0.08cm}=\hspace{0.08cm} \sum_{x\in \mathbb{R}} x \cdot P(g(\mathcal{X})=x) \hspace{0.08cm}=\hspace{0.08cm} \sum_{x\in \mathbb{R}} g(x) \cdot P(\mathcal{X}=x)\\$$


    Si $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$ es **continua**:

    $$E[g(\mathcal{X})] = \int_{-\infty}^{\infty} x \cdot f_{g(\mathcal{X})}(x) \cdot dx =  \int_{-\infty}^{\infty} g(x) \cdot f_\mathcal{X}(x) \cdot dx\\$$



    Donde: $\hspace{0.08cm}g(\mathcal{X})\hspace{0.08cm}$ es una transformación de $\hspace{0.02cm}\mathcal{X}$.


<br>

# Varianza de una variable aleatoria

Dada una variable aleatoria $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$, 

La varianza de $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$  se define como: 

$$Var(\mathcal{X}) \hspace{0.08cm}=\hspace{0.08cm} E\Bigl[ \hspace{0.08cm} (\mathcal{X} - E[\mathcal{X}])^2 \hspace{0.08cm} \Bigr]\\$$ 
 
 
 
## Propiedades 
 
- $Var(\mathcal{X}) \hspace{0.08cm}\geq\hspace{0.08cm} 0$  

- $Var(a) \hspace{0.08cm}=\hspace{0.08cm} 0$  

- $Var(b\cdot \mathcal{X}) \hspace{0.08cm}=\hspace{0.08cm} b^2 \cdot Var(\mathcal{X})$  

- $Var(a + b\cdot \mathcal{X})\hspace{0.08cm} =\hspace{0.08cm} b^2 \cdot Var(\mathcal{X})$ 

- $Var(\mathcal{X})\hspace{0.08cm}=\hspace{0.08cm}E\left[\mathcal{X}^2\right] - E\left[\mathcal{X}\right]^2$ 

- $Var(a\cdot \mathcal{X} + b\cdot \mathcal{Y}) \hspace{0.08cm}=\hspace{0.08cm} a^2 \cdot Var(\mathcal{X}) + b^2 \cdot Var(\mathcal{Y}) + 2\cdot Cov(\mathcal{X},\mathcal{Y})$  

- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ e $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$ son independientes, entonces:

$$Var(a\cdot \mathcal{X} , b\cdot \mathcal{Y}) \hspace{0.08cm}=\hspace{0.08cm} a^2\cdot Var(\mathcal{X}) + b^2 \cdot Var(\mathcal{Y})\\$$ 


- $Var(g_1(\mathcal{X})\hspace{0.08cm}+\hspace{0.08cm}g_2(\mathcal{Y})) \hspace{0.08cm}=\hspace{0.08cm} Var(g_1(\mathcal{X})) + Var(g_2(\mathcal{Y})) + 2\cdot Cov(g_1(\mathcal{X}),g_2(\mathcal{Y}))$ $\\$  




- Si $\hspace{0.02cm}\mathcal{X}_1,\dots ,\mathcal{X}_k \hspace{0.02cm}$  no son independientes:

$$Var(a_1\cdot \mathcal{X}_1 + a_2\cdot \mathcal{X}_2 + \dots + a_k\cdot \mathcal{X}_k) \hspace{0.08cm}= \\[0.4cm] = \hspace{0.08cm} a_1^2 \cdot Var(\mathcal{X}_1) \hspace{0.03cm}+\hspace{0.03cm} a_2^2 \cdot Var(\mathcal{X}_2) \hspace{0.03cm}+\hspace{0.03cm} \dots \hspace{0.03cm}+\hspace{0.03cm}   a_k^2 \cdot Var(\mathcal{X}_k)  \hspace{0.05cm}+\hspace{0.05cm} 2\cdot   \sum_{i\neq j =1,...,k}   a_i \cdot a_j \cdot Cov(\mathcal{X}_i,\mathcal{X}_j)$$  


- Si $\hspace{0.03cm}\mathcal{X}_1,\dots ,\mathcal{X}_k \hspace{0.03cm}$   son independientes, entonces:

   
    $$Var(a_1\cdot \mathcal{X}_1 + a_2\cdot \mathcal{X}_2 + \dots + a_k\cdot \mathcal{X}_k) \hspace{0.08cm}=\hspace{0.08cm} a_1^2 \cdot Var(\mathcal{X}_1) + a_2^2 \cdot Var(\mathcal{X}_2) + \dots +   a_k^2 \cdot Var(\mathcal{X}_k)$$ 



<br>

# Covarianza 

La covarianza entre las variables $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$ y $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ se define como :  

$$Cov(\mathcal{X},\mathcal{Y}) \hspace{ 0.08cm}=\hspace{0.08cm} E\Bigl[\hspace{0.08cm} (\hspace{0.05cm}\mathcal{X}- E[\mathcal{X}]\hspace{0.05cm})\cdot (\hspace{0.05cm}\mathcal{Y}-E[\mathcal{Y}]\hspace{0.05cm})\hspace{0.08cm}\Bigr]\\$$ 

## Propiedades  

- $Cov(\mathcal{X},\mathcal{Y}) \hspace{0.08cm}=\hspace{0.08cm} E[\mathcal{X}\cdot \mathcal{Y}] \hspace{0.08cm} - \hspace{0.08cm} E[\mathcal{X}]\cdot E[\mathcal{Y}]$  

- Si $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$ e $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ son independientes $\hspace{0.2cm}\Rightarrow\hspace{0.2cm}$ $Cov(\mathcal{X},\mathcal{Y}) \hspace{0.05cm}=\hspace{0.05cm}0$ 

- Si $\hspace{0.03cm}Cov(\mathcal{X},\mathcal{Y}) \neq 0$ $\hspace{0.2cm}\Rightarrow\hspace{0.2cm}$ $\mathcal{X}\hspace{0.03cm}$ e $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ son independientes.  

- Que $\hspace{0.03cm}Cov(\mathcal{X},\mathcal{Y})=0\hspace{0.03cm}$ no implica que $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$ e $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ sean independientes.    


- $Cov(a,\mathcal{X}) \hspace{0.08cm}=\hspace{0.08cm} 0$    

- $Cov(a,b) \hspace{0.08cm}=\hspace{0.08cm} 0$  

-  $Cov(\mathcal{X},\mathcal{X}) \hspace{0.08cm}=\hspace{0.08cm} Var(\mathcal{X})$   


-  $Cov( a\cdot \mathcal{X}\hspace{0.08cm} ,\hspace{0.08cm}   b \cdot \mathcal{Y}) \hspace{0.08cm}=\hspace{0.08cm} a\cdot b\cdot Cov(\mathcal{X},\mathcal{Y})$   

- $Cov(a + b\cdot\mathcal{X}\hspace{0.1cm} ,\hspace{0.1cm}  \mathcal{Y})\hspace{0.08cm} =\hspace{0.08cm}  Cov(a,\mathcal{Y}) + Cov(b\cdot \mathcal{X}\hspace{0.08cm},\hspace{0.08cm}\mathcal{Y})\hspace{0.08cm}  = \hspace{0.08cm} b\cdot Cov(\mathcal{X},\mathcal{Y})$   

- $Cov(c_1 + a_1\cdot \mathcal{X}_1 + a_2\cdot \mathcal{X}_2\hspace{0.13cm} ,\hspace{0.13cm}   c_2 + b_1 \cdot \mathcal{Y}_1 + b_2 \cdot \mathcal{Y}_2) \hspace{0.1cm}=\hspace{0.1cm}  
Cov(c_1, c_2) \hspace{0.1cm}+\hspace{0.1cm} Cov(c_1\hspace{0.1cm} ,\hspace{0.1cm}  b_1 \cdot \mathcal{Y}_1)\hspace{0.1cm}  + \\[0.6cm] + \hspace{0.1cm} Cov(c_1, b_2 \cdot \mathcal{Y}_2) \hspace{0.1cm}  +\hspace{0.1cm} Cov(a_1\cdot \mathcal{X}_1\hspace{0.1cm} ,\hspace{0.1cm}  c_2) \hspace{0.1cm} \hspace{0.4cm} + \hspace{0.1cm} Cov(a_1\cdot \mathcal{X}_1\hspace{0.1cm} ,\hspace{0.1cm}   b_1 \cdot \mathcal{Y}_1) \hspace{0.1cm} +\hspace{0.1cm} Cov(a_1\cdot \mathcal{X}_1\hspace{0.1cm} ,\hspace{0.1cm}  b_2 \cdot \mathcal{Y}_2) \hspace{0.1cm} + \\[0.6cm]  +\hspace{0.1cm}  Cov(a_2\cdot \mathcal{X}_2\hspace{0.1cm} ,\hspace{0.1cm}  c_2) \hspace{0.1cm}+\hspace{0.1cm} Cov(a_2\cdot \mathcal{X}_2\hspace{0.1cm} , \hspace{0.1cm}  b_1 \cdot \mathcal{Y}_1) \hspace{0.1cm}+\hspace{0.1cm} Cov(a_2\cdot \mathcal{X}_2\hspace{0.1cm} ,\hspace{0.1cm}  b_2 \cdot \mathcal{Y}_2)$

<br>




# Distribuciones de probabilidad discretas


## Distribución Uniforme Discreta

 Sea $\hspace{0.03cm}X=(x_1,...,x_n)^t\hspace{0.03cm}$,

$$\mathcal{X} \sim U(X) \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm}\mathcal{X} \sim U(x_1,....,x_n) \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} P(\mathcal{X}=x) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{1}{n}\cdot \mathbb{I}\hspace{0.03cm} \left[ \hspace{0.03cm} x\in \lbrace x_1,...,x_n \rbrace \hspace{0.03cm} \right]$$ 


### Propiedades 



- $E[\mathcal{X}]\hspace{0.08cm} = \hspace{0.08cm}\dfrac{1}{n} \sum_{i=1}^{n} x_i \hspace{0.08cm}=\hspace{0.08cm} \overline{X}$  
 
- $Var(\mathcal{X})\hspace{0.08cm} =\hspace{0.08cm} \dfrac{1}{n} \sum_{i=1}^{n} (x_i - E[\mathcal{X}])^2 \hspace{0.08cm}=\hspace{0.08cm} \sigma(X)^2$  
 
- Sea $\hspace{0.03cm}(x_{(1)},...,x_{(n)})^t\hspace{0.03cm}$ el vector $\hspace{0.03cm}X=(x_1,...,x_n)^t\hspace{0.03cm}$ ordenado de menor a mayor: $\\$

$$F_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} \left\lbrace\begin{array}{c} 1 \hspace{0.3cm},\hspace{0.3cm} \text{si} \hspace{0.2cm} x \hspace{0.08cm}> \hspace{0.08cm} x_{(n)} \\
0 \hspace{0.3cm},\hspace{0.3cm}  \text{si} \hspace{0.2cm} x \hspace{0.08cm}<\hspace{0.08cm} x_{(1)} \\ \dfrac{i}{n} \hspace{0.3cm},\hspace{0.3cm} \text{si} \hspace{0.2cm} x=x_{(i)} \end{array}\right.$$
 
 <br>

## Distribución de Bernoulli

$$\mathcal{X} \sim Bernoulli(p) \hspace{0.3cm} \Leftrightarrow\hspace{0.3cm} P(\mathcal{X}=x) \hspace{0.08cm}=\hspace{0.08cm} p^x \cdot (1-p)^x \cdot \mathbb{I}\hspace{0.05cm} \left[ \hspace{0.05cm}  x \in \lbrace 0,1 \rbrace \hspace{0.05cm}  \right]$$  


**Observaciones:**

- Las v.a. $\hspace{0.02cm}Bernoulli(p)\hspace{0.02cm}$ se usan para modelar experimentos aleatorios con dos únicos resultados contrarios (éxito-fracaso)
 tales que la probabilidad del exito es $\hspace{0.02cm}p\hspace{0.02cm}$ y la del fracaso $\hspace{0.02cm}1-p\hspace{0.02cm}$.  
 
     A este tipo de experimentos aleatorios se les llama experimentos tipo Bernoulli.  
 
- Si $\hspace{0.02cm}\mathcal{X} \sim Bernoulli(p)\hspace{0.02cm}$ , entonces:

    $\mathcal{X}\hspace{0.02cm}$ puede interpretarse como el nº de exitos obtenidos tras realizar una vez un experimento tipo Bernoulli (notese que este nº solo puede ser 0 o 1),  con probabilidad de exito $\hspace{0.08cm}p\hspace{0.08cm}$.  




### Propiedades

- $P(\mathcal{X}=1)\hspace{0.08cm}=\hspace{0.08cm}p$ 

- $P(\mathcal{X}=0)\hspace{0.08cm}=\hspace{0.08cm}1-p$  

- $E[\mathcal{X}]\hspace{0.08cm}=\hspace{0.08cm}p$  

- $Var(\mathcal{X})\hspace{0.08cm}=\hspace{0.08cm} p\cdot (1-p)$  

- $F_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} \left\lbrace\begin{array}{l} 1 \hspace{0.3cm},\hspace{0.3cm} \text{si} \hspace{0.2cm} x\geq 1 \\ 0 \hspace{0.3cm},\hspace{0.3cm} \text{si} \hspace{0.2cm} x < 0 \\ 1-p \hspace{0.3cm},\hspace{0.3cm} \text{si} \hspace{0.2cm} x\in [0 , 1) \end{array}\right.$ 

<br>


## Distribución Binomial


$$\mathcal{X} \sim Binomial(n,p) \hspace{0.3cm} \Leftrightarrow \hspace{0.3cm} P(\mathcal{X}=x) \hspace{0.08cm}=\hspace{0.08cm}\binom{n}{x} \cdot p^x (1-p)^x \cdot \mathbb{I}\hspace{0.08cm}\bigl[ x\in \lbrace 0,1,..,n \rbrace \bigr] \\$$ 


**Observación:**


- Si $\hspace{0.03cm}\mathcal{X} \sim Binomial(p)\hspace{0.02cm}$ , entonces:

    $\mathcal{X}$ puede interpretarse como el nº de exitos obtenidos tras realizar $n$ veces un experimento tipo Bernoulli (notese que este nº solo puede ser 0,1,...,n),  con probabilidad de exito $p$ $\\$


### Propiedades 

- Si $\hspace{0.03cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.03cm}$ son v.a. independientes tales que $\hspace{0.03cm}\mathcal{X}_i \sim Bernoulli(p)\hspace{0.03cm} , \hspace{0.03cm} i =1,...,n$ , entonces:

$$\mathcal{X}_1 + ... + \mathcal{X}_n \hspace{0.03cm} \sim \hspace{0.03cm} Binomial (n, p)$$  



- $E[\mathcal{X}]\hspace{0.08cm}=\hspace{0.08cm}n\cdot p$  


- $Var(\mathcal{X}) \hspace{0.08cm}=\hspace{0.08cm} n \cdot p \cdot (1-p)$  


- Para $\hspace{0.08cm}x\geq n\hspace{0.08cm}$ :

$$F_\mathcal{X}(x) \hspace{0.25cm}=\sum_{z\in \lbrace 0,1,...,x\rbrace} \binom{n}{z} \cdot p^z \cdot (1-p)^{n-z}$$


<br>

## Distribución Geométrica (de fracasos)


$$\mathcal{X} \sim GeoFrac(p) \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} P(\mathcal{X}=x) \hspace{0.08cm}=\hspace{0.08cm} (1-p)^x \cdot p \cdot \mathbb{I}\hspace{0.08cm}\bigl[ x\in \lbrace 0,1,..,n \rbrace \bigr] \\$$ 


**Observación:**

- Si $\hspace{0.08cm}\mathcal{X} \sim GeoFrac(p)\hspace{0.08cm}$ , entonces:

    $\mathcal{X}\hspace{0.08cm}$ puede interpretarse como el nº de  fracasos obtenidos hasta el primer exito tras realizar multiples veces un experimento tipo Bernoulli (notese que este nº solo puede ser 0,1,2,...), con probabilidad de exito $\hspace{0.08cm}p\hspace{0.08cm}$.  


### Propiedades

- $E[ \mathcal{X} ] \hspace{0.08cm} = \hspace{0.08cm} \dfrac{1-p}{p}$ 

- $Var(\mathcal{X})\hspace{0.08cm} =\hspace{0.08cm} \dfrac{1-p}{p^2}$


<br>

## Distribucion Geométrica (de intentos)


$$\mathcal{X} \sim GeoInt(p) \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} P(\mathcal{X}=x) \hspace{0.08cm}=\hspace{0.08cm} (1-p)^{x-1}\cdot p \cdot \mathbb{I}\hspace{0.08cm}\bigl[ x\in \lbrace 0,1,..,n \rbrace \bigr] \\$$


**Observación:**

$\mathcal{X}\hspace{0.08cm}$ se puede interpretar como el nº de intentos hasta la obtencion del primer exito tras realizar múltiples veces un experimento tipo Bernoulli (notese que este nº solo puede ser 0,1,2,...), con probabilidad de exito $\hspace{0.08cm}p\hspace{0.08cm}$.  



### Propiedades

- $E[\mathcal{X}]\hspace{0.08cm}=\hspace{0.08cm} \dfrac{1}{p}$  

- $Var(\mathcal{X})\hspace{0.08cm}=\hspace{0.08cm} \dfrac{1-p}{p^2}$


<br>

## Distribución de Poisson


$$\mathcal{X}\sim Poisson(\lambda) \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} P(\mathcal{X}=x) \hspace{0.08cm}=\hspace{0.08cm} e^{-\lambda} \cdot \dfrac{\lambda^x}{x!} \cdot \mathbb{I}\hspace{0.08cm}\bigl[ x\in \lbrace 0,1,..,n \rbrace \bigr]$$


Donde:  $\hspace{0.08cm}\lambda > 0$. $\\$


### Propiedad

- $E[\mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} \lambda$  

- $Var[\mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} \lambda$   

- Si $\hspace{0.03cm}\mathcal{X}_i \sim Poisson(\lambda_i)\hspace{0.03cm}$  y son independientes , para $\hspace{0.03cm}i =1,...,n\hspace{0.03cm}$ , entonces:

$$\mathcal{X}_1 + ... + \mathcal{X}_n \hspace{0.08cm}\sim\hspace{0.08cm} Poisson(\lambda_1 + ... + \lambda_n)$$

<br>

## Distribución Hipergeommetrica 


$$\mathcal{X} \sim HiperGeo(N, k, n) \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} P(\mathcal{X}=x) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\hspace{0.08cm}\binom{k}{n} \cdot \binom{N-k}{n-x}\hspace{0.08cm}}{\binom{N}{n}} \cdot \mathbb{I}\hspace{0.08cm}\bigl[ x\in \lbrace 0,1,..,n \rbrace \bigr]\\$$


**Observación:**

- Las v.a. hipergeométrica se usan para modelar experimentos aleatorios que consisten en extraer sin reemplazamiento una muestra de $\hspace{0.03cm}n\hspace{0.03cm}$ elementos de un conjunto con $\hspace{0.03cm}N\hspace{0.03cm}$ elementos, $\hspace{0.03cm}k\hspace{0.03cm}$ de una clase llamada clase-k y $\hspace{0.03cm}N-k\hspace{0.03cm}$ de otra clase.  

- Si  $\hspace{0.03cm}\mathcal{X} \sim HiperGeo(N, k, n)\hspace{0.03cm}$ , entonces:

    $\mathcal{X}\hspace{0.03cm}$ puede interpretarse como el nº de elementos de la clase-k extraidos del conjunto tras haber extraido una muestra de $\hspace{0.08cm}n\hspace{0.08cm}$ elementos.  $\\$


### Propiedades  

- $E[\mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} n \cdot \dfrac{k}{N}$  

- $Var(\mathcal{X}) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{n \cdot k\cdot (N-k)(N-n)}{N^2 \cdot (N-1)}$


<br>

# Distribuciones de probabilidad continuas



## Distribución Uniforme Continua

$$\mathcal{X} \sim U(a,b) \hspace{0.3cm}\Leftrightarrow \hspace{0.3cm} f_\mathcal{X}(x) \hspace{0.08cm} =\hspace{0.08cm} \dfrac{1}{b-a} \cdot \mathbb{I}\hspace{0.08cm}\bigl[ x\in (a,b) \bigr] \\$$



### Propiedades

- $E[\mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} \dfrac{1}{2} \cdot (a + b)$  

- $Var(\mathcal{X}) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{1}{12} \cdot (b-a)^2$  

- $F_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} P(\mathcal{X} \geq x) \hspace{0.1cm}=\hspace{0.1cm} \int_{-\infty}^{x} \hspace{0.08cm} f_X(z) \cdot dz \hspace{0.1cm}=\hspace{0.1cm} \left\lbrace\begin{array}{l} 1 \hspace{0.3cm} , \hspace{0.3cm} \text{si} \hspace{0.2cm} x\geq b \\ 0  \hspace{0.3cm} , \hspace{0.3cm} \text{si} \hspace{0.2cm} x \leq a \\ \dfrac{x-a}{b-a}  \hspace{0.3cm} , \hspace{0.3cm} \text{si} \hspace{0.2cm} x\in (a,b) \end{array}\right.$


<br>

## Distribución Exponencial


$$\mathcal{X} \sim Exponencial(\lambda) \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm}  f_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} \lambda \cdot e^{-\lambda \cdot x} \cdot \mathbb{I}\bigl[ x>0 \bigr]$$

Donde: $\hspace{0.08cm}\lambda > 0\hspace{0.08cm}$. $\\$


### Propiedades

- $E[\mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} \dfrac{1}{\lambda}$  

- $Var(\mathcal{X}) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{1}{\lambda^2}$  

- $F_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm}  P(\mathcal{X} \geq x) \hspace{0.1cm}=\hspace{0.1cm} \int_{-\infty}^{x} f_\mathcal{X}(z) \cdot dz \hspace{0.1cm}=\hspace{0.1cm} \left\lbrace\begin{array}{l} 0  \hspace{0.3cm} , \hspace{0.3cm} \text{si} \hspace{0.2cm} x \leq 0 \\ 1- e^{-\lambda \cdot x}  \hspace{0.3cm} , \hspace{0.3cm} \text{si} \hspace{0.21cm} x>0 \end{array}\right.$

<br>

## Función Gamma

$\Gamma\hspace{0.08cm}$ es la función Gamma $\hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} \Gamma(\alpha) \hspace{0.08cm}=\hspace{0.08cm} \int_{0}^{\infty} x^{\alpha - 1} \cdot e^{-x} \cdot dx$

Donde: $\hspace{0.08cm}\alpha >0\hspace{0.08cm}$. $\\$

### Propiedades

- $\Gamma(1) \hspace{0.08cm}=\hspace{0.08cm} \Gamma(2) \hspace{0.08cm}=\hspace{0.08cm} 1\hspace{0.08cm}$.  

- $\Gamma(\alpha + 1) \hspace{0.08cm}=\hspace{0.08cm} \alpha \cdot \Gamma(\alpha)\hspace{0.08cm}$.  

- $\Gamma(\alpha + 1) \hspace{0.08cm}=\hspace{0.08cm} \alpha !\hspace{0.08cm}$ , si $\hspace{0.08cm}\alpha \in \mathbb{N}\hspace{0.08cm}$.  

- $\Gamma(\alpha) \hspace{0.08cm}=\hspace{0.08cm} (\alpha -1)!\hspace{0.08cm}$ , si $\hspace{0.08cm}\alpha \in \mathbb{N}\hspace{0.08cm}$.  

- $\Gamma(1/2) \hspace{0.08cm}=\hspace{0.08cm} \sqrt{\pi}\hspace{0.08cm}$.


<br>

## Distribucion Gamma

$$\mathcal{X}\sim Gamma(\alpha , \lambda ) \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} f_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\lambda^\alpha}{\Gamma(\alpha )} \cdot x^{\alpha -1} \cdot e^{-\lambda \cdot x} \cdot \mathbb{I}\bigl[ x>0 \bigr]$$

Donde: $\hspace{0.08cm}\alpha, \lambda > 0$. $\\$


### Propiedades

- Si $\hspace{0.02cm}\alpha \in \mathbb{N}\hspace{0.02cm}$ , entonces:


    $$f_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\lambda^\alpha}{(\alpha -1)!} \cdot x^{\alpha -1} \cdot e^{-\lambda \cdot x} \cdot \mathbb{I}( x>0 )\\$$ 

- Si $\hspace{0.02cm}\alpha \hspace{0.08cm}=\hspace{0.08cm} 1\hspace{0.02cm}$ , entonces:

    $$f_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} \lambda \cdot e^{-\lambda \cdot x} \cdot \mathbb{I}(  x>0 )$$

    Por tanto:

    $$Gamma(\alpha = 1 , \lambda) \hspace{0.08cm}=\hspace{0.08cm} Exponencial(\lambda) \\$$



- $E[\mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\alpha}{\lambda}$  

- $Var(\mathcal{X}) \hspace{0.08cm}= \hspace{0.08cm} \dfrac{\alpha}{\lambda^2}$  

 
- Si $\hspace{0.03cm}\mathcal{X}_1,...,\mathcal{X}_n \sim Exponencial(\lambda)\hspace{0.03cm}$ y son independientes, entonces:

    $$\mathcal{X}_1+...+\mathcal{X}_n \hspace{0.05cm}\sim\hspace{0.05cm} Gamma(n , \lambda)\\$$

- Si $\hspace{0.08cm}\mathcal{X} \hspace{0.05cm}\sim\hspace{0.05cm} Gamma(\alpha , \lambda)\hspace{0.08cm}$ y $\hspace{0.08cm}c>0\hspace{0.08cm}$ , entonces:

    $$c\cdot \mathcal{X}\hspace{0.05cm}\sim\hspace{0.05cm} Gamma(\alpha , \lambda / c)\\$$

- Si $\hspace{0.08cm}\mathcal{X}_1 , ..., \mathcal{X}_n\hspace{0.08cm}$ son independientes y $\hspace{0.08cm}\mathcal{X}_i\hspace{0.05cm}\sim\hspace{0.05cm} Gamma(\alpha_i , \lambda)\hspace{0.08cm}$, entonces:

    $$\mathcal{X}_1 +...+ \mathcal{X}_n \hspace{0.05cm}\sim\hspace{0.05cm} Gamma(\alpha_1 +...+ \alpha_n , \lambda)$$


<br>


## Distribución Normal


$$\mathcal{X} \sim N(\mu, \sigma^2) \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} f_\mathcal{X}(x) = \dfrac{1}{\sigma \sqrt{2\pi}} \cdot exp \left(  \dfrac{-(x- \mu)^2}{2\cdot \sigma^2}  \right) \hspace{0.3cm} , \hspace{0.3cm} x\in \mathbb{R}$$


Donde: $\hspace{0.08cm}\mu \in \mathbb{R}\hspace{0.03cm}$ y $\hspace{0.03cm}\sigma^2 > 0$. $\\$



### Propiedades

- $E[\mathcal{X}]\hspace{0.08cm}=\hspace{0.08cm}\mu$  

- $Var(\mathcal{X})\hspace{0.08cm}=\hspace{0.08cm}\sigma^2$  

- $\mathcal{X}\cdot a \sim N(\mu \cdot a , a^2 \cdot \sigma^2)$  

- $\mathcal{X}\cdot b + a \sim N( b\cdot \mu + a , b^2 \cdot \sigma^2)$  

- Si $\hspace{0.08cm}\mathcal{X}_i \hspace{0.05cm}\sim\hspace{0.05cm} N(\mu_i , \sigma_i^2)\hspace{0.08cm}$ , para $\hspace{0.08cm}i =1,...,n\hspace{0.08cm}$ , entonces:

    $$\mathcal{X}_1 + ...+ \mathcal{X}_n \hspace{0.05cm}\sim\hspace{0.05cm} N(\mu_1 +...+ \mu_n \hspace{0.1cm},\hspace{0.1cm} \sigma_1^2 + ...+ \sigma_n^2)\\$$

- Si $\hspace{0.08cm}\mathcal{X}_i \hspace{0.05cm}\sim\hspace{0.05cm} N(\mu_i , \sigma_i^2)\hspace{0.08cm}$ , para $\hspace{0.08cm}i =1,...,n\hspace{0.08cm}$ , entonces:

    $$a_1\cdot \mathcal{X}_1 + ...+ a_n \cdot \mathcal{X}_n \hspace{0.05cm}\sim\hspace{0.05cm} N(a_1 \cdot \mu_1 +...+ a_n \cdot \mu_n \hspace{0.1cm},\hspace{0.1cm} a_1^2 \cdot \sigma_1^2 + ...+ a_n^2 \cdot \sigma_n^2)$$


<br>

## Funcion Beta

$$B(a,b) \hspace{0.08cm}=\hspace{0.08cm} \int_0^1 x^{a-1} \cdot (1-x)^{b-1} \cdot dx$$

Donde: $\hspace{0.08cm}a, b >0\hspace{0.05cm}$.


### Propiedades

- $B(a,b) \hspace{0.08cm}=\hspace{0.08cm} B(b,a)$  

- $B(a,1) \hspace{0.08cm}=\hspace{0.08cm} 1/a$  

- $B(a+1, b) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{a}{a+b} \cdot B(a,b)$  

- $B(a,b) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\Gamma(a) \cdot \Gamma(b)}{\Gamma(a+b)}$  


<br>


## Distribucion Beta

$$\mathcal{X}\sim Beta(a,b) \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} f_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{1}{B(a,b)} \cdot x^{a-1} \cdot (1-x)^{b-1} \cdot \mathbb{I}\left[ \hspace{0.05cm} x \in (0,1) \hspace{0.05cm} \right]$$


Donde: $\hspace{0.08cm}a, b >0$. $\\$


### Propiedades

- $E[\mathcal{X}]\hspace{0.08cm}=\hspace{0.08cm}\dfrac{a}{a+b}$  

- $Var(\mathcal{X})\hspace{0.08cm} =\hspace{0.08cm} \dfrac{a\cdot b}{(a + b +1)(a+b)^2}$  

- Si $\hspace{0.08cm}\mathcal{X}\sim Gamma(a, \lambda)\hspace{0.08cm}$ y $\hspace{0.08cm}\mathcal{Y}\sim Gamma(b,\lambda)\hspace{0.08cm}$ , entonces:

    $$\dfrac{\mathcal{X}}{\mathcal{X}+\mathcal{Y}} \sim Beta(a,b)$$


<br>


## Distribucion Weibull


$$\mathcal{X}\sim Weibull(\alpha , \lambda) \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} f_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} \lambda\cdot \alpha (\lambda \cdot x)^{\alpha - 1} \cdot e^{-(\lambda \cdot x)^\alpha} \cdot \mathbb{I}\bigl[ x > 0 \bigr]$$

Donde: $\hspace{0.08cm}\alpha \hspace{0.08cm},\hspace{0.08cm} \lambda\hspace{0.08cm} > \hspace{0.08cm}0.$ $\\$




### Propiedades

- $E[\mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} \dfrac{1}{\lambda} \Gamma\left(1 + \dfrac{1}{\alpha}\right)$  

- $Var(\mathcal{X}) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{1}{\lambda^2}\cdot \left( \Gamma\left(1 + \dfrac{2}{\alpha}\right) - \Gamma\left(1+ \dfrac{1}{\alpha}\right) \right)$  

- $F_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} \left(\hspace{0.08cm} 1 - e^{-(\lambda \cdot x)^\alpha} \hspace{0.08cm} \right) \cdot \mathbb{I}\left[ x>0 \right]$


<br>

## Distribucion Chi-cuadrado

$$\mathcal{X} \sim \chi_n ^2 \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} f_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{1}{\Gamma(n/2)} \cdot \dfrac{1}{2}^{n/2} \cdot x ^{n/2 -1} \cdot e^{-x/2} \hspace{0.3cm} , \hspace{0.3cm} x\in \mathbb{R}\\$$

 

### Propiedades

- $E[\chi_n ^2] \hspace{0.08cm}=\hspace{0.08cm} n$  

- $Var(\chi_n ^2) \hspace{0.08cm}=\hspace{0.08cm} 2n$  

- $\chi_n ^2 + \chi_m ^2 \hspace{0.08cm}=\hspace{0.08cm} \chi_{n+m} ^2$  

- $\chi_n ^2 - \chi_m ^2 \hspace{0.08cm}=\hspace{0.08cm} \chi_{n-m} ^2$ 

- $\chi_n ^2 \hspace{0.08cm}=\hspace{0.08cm} Gamma( \alpha= n/2 , \lambda=1/2)$  

- Si $\hspace{0.08cm}\mathcal{X}_1 ,..., \mathcal{X}_n \sim N(0,1)\hspace{0.08cm}$ y son independientes , entonces:

    $$\mathcal{X}_1^2 + ... + \mathcal{X}_n^2 \hspace{0.05cm}\sim\hspace{0.05cm} \chi_n^2$$

<br>

## Distribucion t-student

$$\mathcal{X}\sim t_n \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} f_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\Gamma\left(\dfrac{n+1}{2}\right)}{\sqrt{n\cdot \pi} \cdot \Gamma(n/2)} \cdot \left(1 + \dfrac{x^2}{n}\right)^{-1/2 \cdot (n+1)}  \hspace{0.4cm} , \hspace{0.4cm} x\in \mathbb{R} \\$$


### Propiedades 

- $E[\mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} 0\hspace{0.08cm}$.  

- $Var[\mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} \dfrac{n}{n-2}\hspace{0.08cm}$ , si $\hspace{0.08cm}n>2\hspace{0.08cm}$.  

- Si $\hspace{0.03cm}Z \sim N(0,1)\hspace{0.1cm}$ y $\hspace{0.1cm}X\sim \chi_n^2\hspace{0.1cm}$ y son **independientes** , entonces:

    $$\dfrac{Z}{\sqrt{X/n}} \hspace{0.05cm}\sim\hspace{0.05cm} t_n$$


<br>


## Distribucion F-Fisher  

$$\mathcal{X}\sim F(a,b) \hspace{0.3cm}\Leftrightarrow\hspace{0.3cm} f_\mathcal{X}(x) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\Gamma\left(\dfrac{a+b}{2}\right)}{\Gamma(a/2)\cdot \Gamma(b/2)} \cdot (a/b)^{a/2} \cdot x^{a/2-1} \cdot (1 + (a/b)\cdot x)^{-(a+b)/2} \cdot \mathbb{I}\left[ x>0 \right] \\$$


### Propiedades

- $E[\mathcal{X}] \hspace{0.08cm}=\hspace{0.08cm} \dfrac{b}{b-2}\hspace{0.08cm}$.  

- $Var(\mathcal{X}) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{2b^2 \cdot (a + b - 2)}{a \cdot (b-2)^2 \cdot (b-4)}\hspace{0.1cm} ,\hspace{0.1cm}$ si $\hspace{0.12cm}b>4\hspace{0.08cm}$. 

- Si $\hspace{0.08cm}\mathcal{X} \sim \chi^2_a\hspace{0.15cm}$ y $\hspace{0.15cm}\mathcal{Y} \sim \chi^2_b\hspace{0.12cm}$ , entonces:

    $$\dfrac{\hspace{0.08cm}\mathcal{X}/a\hspace{0.08cm}}{\mathcal{Y}/b} \hspace{0.05cm}\sim\hspace{0.05cm} F(a,b)$$


<br>



# Probabilidad conjunta de variables aleatorias 

  

Sean $\hspace{0.03cm}\mathcal{X}\hspace{0.05cm}$ e $\hspace{0.05cm}\mathcal{Y}\hspace{0.03cm}$ variables aleatorias, $\\$

$$P(\mathcal{X}=x \hspace{0.07cm},\hspace{0.07cm} \mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} P(\mathcal{X}= x \hspace{0.05cm}\cap\hspace{0.05cm} \mathcal{Y}=y)\\$$
$$P(\mathcal{X}\in A \hspace{0.07cm},\hspace{0.07cm} \mathcal{Y}\in B) \hspace{0.08cm}=\hspace{0.08cm} P(\mathcal{X} \in A \hspace{0.07cm}\cap\hspace{0.07cm} \mathcal{Y} \in B)$$
 

 


<br>

## Densidad conjunta de variables aleatorias continuas

- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ e $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$  son v.a's **continuas**:  

   
    - $f_{\mathcal{X},\mathcal{Y}}\hspace{0.03cm}$ es la función de densidad conjunta de las v.a's  $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ e $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$ , si y solo si:  


       - $f_{\mathcal{X},\mathcal{Y}} \hspace{0.12cm} :\hspace{0.12cm} \mathbb{R}^2 \hspace{0.08cm}\rightarrow\hspace{0.08cm} (0 , \infty)$  

       - $P( \mathcal{X} \in A \hspace{0.08cm} , \hspace{0.08cm} \mathcal{Y} \in B) \hspace{0.1cm}=\hspace{0.1cm} \int_{x\in A} \int_{y\in B} \hspace{0.08cm} f_{\mathcal{X},\mathcal{Y}}(x,y) \cdot dy \cdot dx$  

       - $\int_{x\in \mathbb{R}} \int_{y\in \mathbb{R}} \hspace{0.08cm}  f_{\mathcal{X},\mathcal{Y}}(x,y) \cdot dy \cdot dx \hspace{0.1cm}=\hspace{0.1cm} 1$  
<br>
       
### Propiedades:


- $f_{\mathcal{Y}}(y) \hspace{0.08cm}=\hspace{0.08cm} \int_{x \in \mathbb{R}} f_{\mathcal{X},\mathcal{Y}}(x,y)\cdot dx$

- $f_{\mathcal{X}}(x) \hspace{0.08cm}=\hspace{0.08cm} \int_{y \in \mathbb{R}} f_{\mathcal{X},\mathcal{Y}}(x,y)\cdot dy$

       

<br>

## Densidad conjunta de variables aleatorias discretas-continuas

- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ es **discreta** e $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$  **continua**:

   
    - $f_{\mathcal{X},\mathcal{Y}}\hspace{0.03cm}$ es la función de densidad conjunta de las v.a's  $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ e $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$ , si y solo si:  


       - $f_{\mathcal{X},\mathcal{Y}} \hspace{0.12cm} :\hspace{0.12cm} \mathbb{R}^2 \hspace{0.08cm}\rightarrow\hspace{0.08cm} (0 , \infty)$  

       - $P( \mathcal{X} \in A , \mathcal{Y} \in B) \hspace{0.08cm}=\hspace{0.08cm} \sum_{x\in A} \int_{y\in B} \hspace{0.08cm} f_{\mathcal{X},\mathcal{Y}}(x,y) \cdot dy \cdot dx$  


       - $\sum_{x\in \mathbb{R}} \int_{y\in \mathbb{R}} \hspace{0.08cm}  f_{\mathcal{X},\mathcal{Y}}(x,y) \cdot dy \cdot dx \hspace{0.08cm}=\hspace{0.08cm} 1$  


<br>

### Propiedades:


- $f_{\mathcal{Y}}(y)  \hspace{0.08cm}= \hspace{0.08cm} \sum_{x \in \mathbb{R}} f_{\mathcal{X},\mathcal{Y}}(x,y)$

- $P(\mathcal{X}=x)  \hspace{0.08cm}= \hspace{0.08cm} \int_{y \in \mathbb{R}} f_{\mathcal{X},\mathcal{Y}}(x,y)\cdot dy$


<br>

## Propiedades de la probabilidad conjunta

- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ e $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$ son v.a's **discretas** , entonces:  


    - $\sum_{x\in \mathbb{R}}\sum_{y\in \mathbb{R}} P(\mathcal{X}=x , \mathcal{Y}=y) \hspace{0.1cm}=\hspace{0.1cm} 1$  

    - Si $\hspace{0.08cm}x\notin Im(\mathcal{X})\hspace{0.1cm}$ ó $\hspace{0.1cm}y \notin Im(\mathcal{Y})$  $\hspace{0.25cm}\Rightarrow\hspace{0.25cm}$ $P(\mathcal{X}=x \hspace{0.05cm},\hspace{0.05cm} \mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} 0$ 

    - $P(\mathcal{X}\in A \hspace{0.07cm},\hspace{0.07cm} \mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} \sum_{x\in A} \hspace{0.07cm} P(\mathcal{X}=x , \mathcal{Y}=y)$  
    
    - $P(\mathcal{X}=x \hspace{0.07cm},\hspace{0.07cm} \mathcal{Y}\in B) \hspace{0.08cm}=\hspace{0.08cm} \sum_{y\in B} \hspace{0.07cm} P(\mathcal{X}=x , \mathcal{Y}=y)$

    - $P(\mathcal{X}\in A \hspace{0.07cm},\hspace{0.07cm} \mathcal{Y}\in B) \hspace{0.08cm}=\hspace{0.08cm} \sum_{x\in A} \hspace{0.07cm}\sum_{y\in B}\hspace{0.07cm} P(\mathcal{X}=x , \mathcal{Y}=y)$ 



<br>


- Si $\hspace{0.05cm}\mathcal{X}\hspace{0.05cm}$ e $\hspace{0.05cm}\mathcal{Y}\hspace{0.05cm}$ son v.a's **continuas** , entonces: 


    - $P(\mathcal{X} = x \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} 0 \hspace{0.2cm}, \hspace{0.2cm} \forall x, y \in \mathbb{R}$

    - $P(\mathcal{X}\in A \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} 0 \hspace{0.2cm}, \hspace{0.2cm} \forall x, y \in \mathbb{R}$  

    - $P(\mathcal{X}=x \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}\in B) \hspace{0.08cm}=\hspace{0.08cm} 0 \hspace{0.2cm}, \hspace{0.2cm} \forall x, x \in \mathbb{R}$  

    - $P(\mathcal{X}\in A \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}\in B) \hspace{0.08cm}=\hspace{0.08cm} \int_{x\in A} \int_{y\in B} \hspace{0.08cm} f_{\mathcal{X},\mathcal{Y}}(x , y) \cdot dy\cdot dx$  




<br>


- Si $\hspace{0.05cm}\mathcal{X}\hspace{0.05cm}$ es **discreta**, pero $\hspace{0.05cm}\mathcal{Y}\hspace{0.05cm}$ es **continua** , entonces:



    - $P(\mathcal{X} = x \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} 0 \hspace{0.2cm}, \hspace{0.2cm} \forall\hspace{0.08cm} x,y \in \mathbb{R}$

    - $P(\mathcal{X}\in A \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} 0 \hspace{0.2cm}, \hspace{0.2cm} \forall \hspace{0.08cm} y \in \mathbb{R}$  

    - $P(\mathcal{X}=x \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}\in B) \hspace{0.08cm}=\hspace{0.08cm} \int_{y\in B} \hspace{0.08cm} f_{\mathcal{X},\mathcal{Y}}(x , y) \cdot dy$  

    - $P(\mathcal{X}\in A \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}\in B) \hspace{0.08cm}=\hspace{0.08cm} \sum_{x\in A} \int_{y\in B} \hspace{0.08cm} f_{\mathcal{X},\mathcal{Y}}(x , y) \cdot dy\cdot dx$  




<br>


# Teorema de la probabilidad total


## Teorema de la probabilidad total para variables discretas

- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ e $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$ son v.a's **discretas** , entonces: $\\$


    - $P(\mathcal{X} = x) \hspace{0.08cm}= \hspace{0.08cm} \sum_{y\in \mathbb{R}} \hspace{0.08cm} P(\mathcal{X} = x \hspace{0.05cm} ,\hspace{0.05cm} \mathcal{Y}=y) \hspace{0.2cm} , \hspace{0.2cm} \forall \hspace{0.03cm} x \in \mathbb{R}$
    
    - $P(\mathcal{X} \in A) \hspace{0.08cm}= \hspace{0.08cm} \sum_{y\in \mathbb{R}} \hspace{0.08cm} P(\mathcal{X} \in A \hspace{0.05cm},\hspace{0.05cm} \mathcal{Y}=y)\hspace{0.08cm}= \hspace{0.08cm} \sum_{y\in \mathbb{R}} \hspace{0.08cm} \sum_{x\in A} \hspace{0.08cm}  P(\mathcal{X} = x \hspace{0.05cm},\hspace{0.05cm} \mathcal{Y}=y) \hspace{0.2cm} , \hspace{0.2cm} \forall \hspace{0.03cm} A \subset \mathbb{R}$


<br>

## Teorema de la probabilidad total para variables continuas

- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ e $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$ son v.a's **continuas** , entonces: $\\$


    - $f_{\mathcal{X}}(x) \hspace{0.08cm}=\hspace{0.08cm} \int_{y \in \mathbb{R}} \hspace{0.08cm} f_{\mathcal{X},\mathcal{Y}}(x,y) \cdot dy$


    - $P(\mathcal{X} = x) \hspace{0.08cm}= \hspace{0.08cm} 0 \hspace{0.2cm} , \hspace{0.2cm} \forall \hspace{0.03cm} x \in \mathbb{R}$
    
    
    - $P(\mathcal{X} \in A) \hspace{0.08cm}= \hspace{0.08cm} \int_{x\in A} \hspace{0.08cm} f_{\mathcal{X}}(x)\cdot dx \hspace{0.08cm}= \hspace{0.08cm} \int_{x\in A} \int{y \in \mathbb{R}} f_{\mathcal{X},\mathcal{Y}}(x,y) \cdot dy \cdot dx \hspace{0.2cm} , \hspace{0.2cm} \forall \hspace{0.03cm} A \subset \mathbb{R}$
    
  
<br>

## Teorema de la probabilidad total para variables discretas-continuas

- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ es **discreta** e $\hspace{0.02cm}\mathcal{Y}\hspace{0.02cm}$ es **continua** , entonces: $\\$


    - $f_{\mathcal{Y}}(y) \hspace{0.08cm}=\hspace{0.08cm} \sum_{x \in \mathbb{R}} \hspace{0.08cm} f_{\mathcal{X},\mathcal{Y}}(x,y)$
    
    - $P(\mathcal{Y} = y) \hspace{0.08cm}= \hspace{0.08cm} 0 \hspace{0.2cm} , \hspace{0.2cm} \forall \hspace{0.03cm} y \in \mathbb{R}$
    

    - $P(\mathcal{X} = x) \hspace{0.08cm}=\hspace{0.08cm} \int_{y \in \mathbb{R}} \hspace{0.08cm} f_{\mathcal{X},\mathcal{Y}}(x,y) \cdot dy$

 
    - $P(\mathcal{Y} \in A) \hspace{0.08cm}= \hspace{0.08cm} \int_{y\in A} \hspace{0.08cm} f_{\mathcal{Y}}(y)\cdot dy \hspace{0.08cm}= \hspace{0.08cm} \int_{y\in A} \sum_{x \in \mathbb{R}} f_{\mathcal{X},\mathcal{Y}}(x,y) \cdot dy \hspace{0.2cm} , \hspace{0.2cm} \forall \hspace{0.03cm} A \subset \mathbb{R}$
    
    - $P(\mathcal{X} \in A) \hspace{0.08cm}= \hspace{0.08cm} \sum_{x\in A} \hspace{0.08cm} P(\mathcal{X}=x) \hspace{0.08cm}= \hspace{0.08cm} \sum_{x\in A} \hspace{0.08cm}   \int_{y \in \mathbb{R}} \hspace{0.08cm} f_{\mathcal{X},\mathcal{Y}}(x,y) \cdot dy
    \hspace{0.2cm} , \hspace{0.2cm} \forall \hspace{0.03cm} A \subset \mathbb{R}$

<br>

# Probabilidad condicionada de variables aleatorias 

## Probabilidad condicionada de variables aleatorias discretas

Sean $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$ e $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ v.a's **discretas**.

La probabilidad de $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$ condicionada a $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$  se define como sigue: $\\$

$$P(\mathcal{X}=x \hspace{0.1cm}|\hspace{0.1cm} \mathcal{Y}=y) \hspace{0.1cm}=\hspace{0.1cm} \dfrac{\hspace{0.08cm}P(\mathcal{X}=x \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}=y)\hspace{0.08cm}}{P(\mathcal{Y}=y)} \hspace{0.3cm},\hspace{0.3cm} \forall \hspace{0.05cm} x,y \in \mathbb{R}\\$$


### Propiedades

- $P(\mathcal{X}\in A \hspace{0.08cm}|\hspace{0.08cm} \mathcal{Y}=y) \hspace{0.1cm} = \hspace{0.1cm} \sum_{x\in A} P(\mathcal{X}=x \hspace{0.08cm}|\hspace{0.08cm} \mathcal{Y}=y)\hspace{0.3cm},\hspace{0.3cm} \forall\hspace{0.05cm} y \in \mathbb{R}$  

- $P(\mathcal{X}\in A \hspace{0.08cm}|\hspace{0.08cm} \mathcal{Y}\in B) \hspace{0.1cm} = \hspace{0.1cm} \dfrac{\hspace{0.08cm}P(\mathcal{X}\in A \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}\in B)\hspace{0.08cm}}{P(\mathcal{Y}\in B)}\hspace{0.1cm} =\hspace{0.1cm} \dfrac{\hspace{0.08cm} \sum_{x\in A} \sum_{y\in B} P(\mathcal{X}=x \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}=y)\hspace{0.08cm}}{ \sum_{y\in B} P(\mathcal{Y}=y)}\hspace{0.3cm},\hspace{0.3cm} \forall\hspace{0.05cm} A, B \subset \mathbb{R}$


<br>

## Densidad condicionada de variables aleatorias continuas

Sean $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$ e $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ v.a's **continuas**.

La densidad de $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$ condicionada a $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$  se define como sigue: $\\$

 

$$f_{\mathcal{X}|\mathcal{Y}}(x,y) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\hspace{0.12cm}f_{\mathcal{X},\mathcal{Y}\hspace{0.12cm}}(x,y)}{f_Y(y)} \\$$


### Propiedades 

-  Al ser $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ continua, $\hspace{0.04cm}P(\mathcal{X}\in A \hspace{0.08cm}|\hspace{0.08cm} \mathcal{Y}=y)\hspace{0.1cm}$ no está definida, ya que:

$$P(\mathcal{X}\in A \hspace{0.08cm}|\hspace{0.08cm} \mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\hspace{0.08cm}P(\mathcal{X}=x , \mathcal{Y}=y)\hspace{0.08cm}}{P(\mathcal{Y}=y)} \hspace{0.08cm}=\hspace{0.08cm} \dfrac{P(\mathcal{X}=x , \mathcal{Y}=y)}{ 0} \hspace{0.3cm} , \hspace{0.3cm} \forall y \in \mathbb{R} \\$$

- Si $\hspace{0.08cm}\mathcal{Y}\hspace{0.08cm}$ es una v.a. continua , entonces: $\hspace{0.14cm}P(\mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} 0\hspace{0.1cm}$ , para todo $\hspace{0.08cm}y\in \mathbb{R}.$

    Por lo que $\hspace{0.08cm}P(\mathcal{X}\in A \hspace{0.08cm}|\hspace{0.08cm} \mathcal{Y}=y)\hspace{0.1cm}$ no está definido, ya que:

    $$P(\mathcal{X}\in A \hspace{0.08cm}|\hspace{0.08cm} \mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\hspace{0.08cm}P(\mathcal{X}=x , \mathcal{Y}=y)\hspace{0.08cm}}{P(\mathcal{Y}=y)} \hspace{0.08cm}=\hspace{0.08cm} \dfrac{P(\mathcal{X}=x , \mathcal{Y}=y)}{ 0}\\$$


- El problema anterior se esquiva del siguiente modo:

    Sea $\hspace{0.08cm}\delta \rightarrow 0^+\hspace{0.08cm}$,

    $$P(\mathcal{X}\in A \hspace{0.08cm}|\hspace{0.08cm} \mathcal{Y}=y) \hspace{0.08cm}\approx \hspace{0.08cm}P(\hspace{0.1cm}\mathcal{X}\in A \hspace{0.1cm}|\hspace{0.1cm} \mathcal{Y} \in [y , y+ \delta] \hspace{0.1cm}) \hspace{0.1cm}=\hspace{0.1cm} \dfrac{\hspace{0.1cm}P(\mathcal{X}\in A \hspace{0.1cm} , \hspace{0.1cm} \mathcal{Y}\in [y , y+ \delta]\hspace{0.1cm})\hspace{0.08cm}}{P(\hspace{0.08cm}\mathcal{Y}\in [y , y+ \delta]\hspace{0.08cm})}\\$$


    Donde ahora $\hspace{0.05cm}P(\mathcal{Y}\in [y , y+ \delta]) \hspace{0.08cm}>\hspace{0.08cm} 0\hspace{0.08cm}$. $\\$ 


    - Por lo que se tiene la siguiente propiedad:

    $$P(\mathcal{X}\in A \hspace{0.08cm}|\hspace{0.08cm} \mathcal{Y}=y) \hspace{0.08cm}\approx \hspace{0.08cm}P(\mathcal{X}\in A \hspace{0.08cm} | \hspace{0.08cm} \mathcal{Y} \in [y , y+ \delta]) \hspace{0.08cm}=\hspace{0.08cm}     \dfrac{\int_{x\in A} \int_{y \in [y , y+ \delta]} \hspace{0.08cm} f_{\mathcal{X},\mathcal{Y}}(x,y)}{ \int_{y \in [y , y+ \delta]} \hspace{0.08cm}f_\mathcal{Y}(y)}\\[0.9cm]$$




- Por otro lado, también se cumple lo siguiente:

    $$P(\mathcal{X}\in A \hspace{0.08cm}|\hspace{0.08cm} \mathcal{Y}\in B) \hspace{0.08cm} = \hspace{0.08cm} \dfrac{\hspace{0.08cm}\int_{x\in A} \int_{y \in B} \hspace{0.08cm} f_{\mathcal{X},\mathcal{Y}}(x,y)\hspace{0.08cm}}{ \int_{y \in B} \hspace{0.08cm}f_\mathcal{Y}(y)}\\$$


<br>


# Independencia de variables aleatorias

## Independencia de variables aleatorias discretas

- $\mathcal{X}\hspace{0.03cm}$ e $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ son v.a. independientes $\hspace{0.05cm}(\mathcal{X} \perp \mathcal{Y}) \hspace{0.07cm}$ si y solo si: $\\$

$$P(\mathcal{X}=x \hspace{0.03cm},\hspace{0.03cm} \mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} P(\mathcal{X}=x)\cdot P(\mathcal{Y}=y) \hspace{0.2cm},\hspace{0.2cm} \forall x,y \in \mathbb{R}$$

<br>


### Teorema de Factorización para Variables Aleatorias Discretas


Sean $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$ e $\hspace{0.08cm}\mathcal{Y}\hspace{0.08cm}$ v.a. discretas, $\\$

$$\text{Si} \hspace{0.15cm}\exists \hspace{0.1cm} h,g \hspace{0.08cm}:\hspace{0.08cm} \mathbb{R} \rightarrow \mathbb{R}\hspace{0.2cm} , \hspace{0.2cm} \forall \hspace{0.08cm} x,y \in \mathbb{R} \hspace{0.15cm},\hspace{0.15cm}  P(\mathcal{X}=x \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} g(x)\cdot h(y)  \hspace{0.2cm} \Rightarrow \hspace{0.2cm}\mathcal{X} \perp \mathcal{Y}\hspace{0.08cm}$$



<br>




## Independencia de variables aleatorias continuas


- $\mathcal{X}\hspace{0.02cm}$ e $\hspace{0.02cm}\mathcal{Y}\hspace{0.03cm}$ son v.a. independientes  $\hspace{0.08cm}(\mathcal{X} \perp \mathcal{Y})\hspace{0.1cm}$ , si y solo si: $\\$



$$P(\mathcal{X}\in A \hspace{0.08cm},\hspace{0.08cm} \mathcal{Y}\in B) \hspace{0.08cm}=\hspace{0.08cm} P(\mathcal{X}\in A)\cdot P(\mathcal{Y}\in B) \hspace{0.3cm} ,\hspace{0.3cm} \forall \hspace{0.08cm} A,B \subset \mathbb{R}\\$$

Lo que es equivalente a:

$$ \int_{x\in A} \int_{y \in B} \hspace{0.08cm} f_{\mathcal{X},\mathcal{Y}}(x,y) \cdot dy \cdot dx \hspace{0.1cm}=\hspace{0.1cm} \int_{x\in A} \hspace{0.08cm} f_{\mathcal{X}}(x)  \cdot dx \cdot \int_{y \in B} f_{\mathcal{Y}}(y) \cdot dy \hspace{0.3cm},\hspace{0.3cm} \forall \hspace{0.08cm} A,B \subset \mathbb{R} $$

<br>

### Teorema de Factorización para v.a. continuas  

Sean $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$ e $\hspace{0.08cm}\mathcal{Y}\hspace{0.08cm}$ v.a. continuas,  

- $\mathcal{X}\hspace{0.08cm}$ e $\hspace{0.08cm}\mathcal{Y}\hspace{0.08cm}$ son v.a. independientes $\hspace{0.08cm}(\mathcal{X} \perp \mathcal{Y})$ , si y solo si: $\\$

 $$f_{\mathcal{X},\mathcal{Y}}(x,y) = f_X(x) \cdot f_\mathcal{Y}(y) \hspace{0.2cm},\hspace{0.2cm} \forall \hspace{0.08cm} x,y \in \mathbb{R} \\$$


#### Colorario

$$\text{Si} \hspace{0.25cm}\exists\hspace{0.1cm} h,g \hspace{0.08cm}:\hspace{0.08cm} \mathbb{R} \rightarrow \mathbb{R}\hspace{0.15cm} , \hspace{0.15cm} \forall \hspace{0.08cm} x,y \in \mathbb{R} \hspace{0.15cm},\hspace{0.15cm} f_{\mathcal{X},\mathcal{Y}}(x,y) \hspace{0.08cm}=\hspace{0.08cm} g(x)\cdot h(y) \hspace{0.3cm}
\Rightarrow\hspace{0.3cm} \mathcal{X} \perp \mathcal{Y}.$$


<br>

## Independencia (dos a dos) de múltiples v.a. 



- $\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.04cm}$ son independientes (dos a dos) $\hspace{0.25cm}\Leftrightarrow\hspace{0.25cm}$ $\mathcal{X}_i \perp \mathcal{X}_j \hspace{0.2cm},\hspace{0.2cm} \forall i\neq j = 1,...,n$


<br>

# Teorema de Bayes para variables aleatorias 

- Si $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$ e $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ son **discretas**:


    $$P(\mathcal{X}=x \hspace{0.08cm}|\hspace{0.08cm} \mathcal{Y}=y) \hspace{0.08cm}=\hspace{0.08cm} \dfrac{P(\mathcal{Y}=y \hspace{0.08cm} | \hspace{0.08cm} \mathcal{X}=x) \cdot  P(\mathcal{X}=x)}{P(\mathcal{Y}=y)}\\$$


- Si $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$ e $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ son **continuas**:

    $$f_{\mathcal{X}|\mathcal{Y}}(x,y) \hspace{0.08cm}=\hspace{0.08cm}    \dfrac{\hspace{0.08cm}f_{\mathcal{Y}|\mathcal{X}}(x, y) \cdot f_\mathcal{X}(x)\hspace{0.08cm}}{f_\mathcal{Y}(y)}$$


<br>

# Esperanza de una función de varias Variables Aleatorias

Esta es la versión en varias variables de la fórmula de transferencia para la esperanza.


Sea $\hspace{0.08cm}g \hspace{0.08cm}:\hspace{0.08cm} \mathbb{R}^2 \rightarrow \mathbb{R}\hspace{0.08cm}$,

- Si $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$ e $\hspace{0.08cm}\mathcal{Y}\hspace{0.08cm}$ son **discretas**:


    $$E[g(\mathcal{X},\mathcal{Y})] \hspace{0.08cm}=\hspace{0.08cm} \sum_{x\in \mathbb{R}} \hspace{0.08cm}\sum_{y\in \mathbb{R}}\hspace{0.08cm} g(x,y) \cdot P(\mathcal{X}=x , \mathcal{Y}=y)\\$$


- Si $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$ e $\hspace{0.08cm}\mathcal{Y}\hspace{0.08cm}$ son **continuas**:

    $$E[g(\mathcal{X},\mathcal{Y})]\hspace{0.08cm} =\hspace{0.08cm} \int_{x\in \mathbb{R}} \int_{y\in \mathbb{R}} g(x,y) \cdot f_{\mathcal{X},\mathcal{Y}}(x , y) \cdot dy \cdot dx$$


<br>

# Esperanza Condicionada

- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ es v.a. **discreta** :

    $$E\left[\mathcal{X} \hspace{0.08cm} | \hspace{0.08cm} \mathcal{Y}=y \right] \hspace{0.08cm}=\hspace{0.08cm} \sum_{x\in \mathbb{R}} \hspace{0.08cm} x \cdot P(\mathcal{X}=x \hspace{0.08cm} | \hspace{0.08cm} \mathcal{Y}=y)\\$$

- Si $\hspace{0.02cm}\mathcal{X}\hspace{0.02cm}$ es v.a. **continua** :

    $$E\left[ \mathcal{X}\hspace{0.08cm} | \hspace{0.08cm}\mathcal{Y}=y \right] \hspace{0.08cm}=\hspace{0.08cm} \int_{x\in \mathbb{R}} x \cdot f_{\mathcal{X} \hspace{0.08cm} | \hspace{0.08cm} \mathcal{Y}}(x,y)  \cdot dx$$


<br>

# Teorema de la Esperanza total 

- Si $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ es una v.a. **discreta**:

    $$E\left[\mathcal{X}\right] \hspace{0.08cm}=\hspace{0.08cm} \sum_{y\in \mathbb{R} } E\left[\mathcal{X} \hspace{0.08cm}|\hspace{0.08cm} \mathcal{Y}=y\right] \cdot P(\mathcal{Y}=y) \\$$


- Si $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ es v.a. **continua** :

    $$E\left[\mathcal{X}\right] \hspace{0.08cm}=\hspace{0.08cm} \int_{y\in \mathbb{R} } E\left[\mathcal{X} \hspace{0.08cm} | \hspace{0.08cm} \mathcal{Y}=y\right] \cdot f_\mathcal{Y}(y) \cdot dy$$


 


 
<br>


# Distribución de la Transformación de una Variable Aleatoria

Supongamos que conocemos la distribución de una v.a. $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$ continua que toma valores en $\hspace{0.08cm}A \subset \mathbb{R}\hspace{0.08cm}$, es decir, conocemos su función de densidad.

Si $\hspace{0.08cm}g \hspace{0.08cm}:\hspace{0.08cm} A \rightarrow \mathbb{R}\hspace{0.08cm}$, entonces,  $\hspace{0.05cm}\mathcal{Y}=g(\mathcal{X})\hspace{0.08cm}$ es una transformación de de la v.a. $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$.


Pues bien, en este contexto tenemos el siguiente teorema que nos permite determinar la distribución de la transformación de $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$, es decir, de  $\hspace{0.08cm}\mathcal{Y}=g(\mathcal{X})\hspace{0.08cm}$,   a partir de la distribución de la propia $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$


- Si $\hspace{0.08cm}g \hspace{0.08cm}:\hspace{0.08cm} A \rightarrow \mathbb{R}\hspace{0.08cm}$ es continua, estrictamente decreciente o creciente y $\hspace{0.08cm}g^{-1}\hspace{0.08cm}$ es diferenciable, entonces, la función de densidad de $\hspace{0.08cm}\mathcal{Y}=g(\mathcal{X})\hspace{0.08cm}$ es la siguiente: $\\$


$$f_{\mathcal{Y}}(y) \hspace{0.08cm}=\hspace{0.08cm} f_{\mathcal{X}}\left( g^{-1}(y) \right) \cdot \left| \dfrac{\partial g^{-1}(y)}{\partial y}  \right| \cdot \mathbb{I}\hspace{0.08cm}\Bigl[ \hspace{0.08cm} \lbrace g(x) \hspace{0.08cm}:\hspace{0.08cm} x \in A \rbrace \hspace{0.08cm} \Bigr]$$


<br>

 


# Distribución de la Transformación de varias Variables Aleatorias

Supongamos que conocemos la distribución conjunta de las variables aleatorias $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$ e $\hspace{0.08cm}\mathcal{Y} \hspace{0.08cm}$  continuas que toman valores en $\hspace{0.08cm}A\text{x}B \subset \mathbb{R}^2\hspace{0.08cm}$, es decir, conocemos la función de densidad conjunta $\hspace{0.08cm}f_{\mathcal{X},\mathcal{Y}}\hspace{0.08cm}$.

Si $\hspace{0.12cm}g \hspace{0.05cm}:\hspace{0.05cm} A\text{x}B \subset \mathbb{R}^2 \hspace{0.08cm}\rightarrow\hspace{0.08cm} \mathbb{R}^2\hspace{0.12cm}$, entonces  $\hspace{0.1cm}g(\mathcal{X}, \mathcal{Y})\hspace{0.08cm}=\hspace{0.08cm}\left( \hspace{0.08cm} g_1(\mathcal{X}, \mathcal{Y}) \hspace{0.08cm},\hspace{0.08cm} g_2(\mathcal{X}, \mathcal{Y}) \hspace{0.08cm} \right) \hspace{0.1cm}$ es una transformación de  las v.a.'s $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$ e $\hspace{0.08cm}\mathcal{Y} \hspace{0.08cm}$.


 

En este contexto tenemos el siguiente teorema que nos permite determinar la distribución de la transformación $\hspace{0.1cm}g(\mathcal{X}, \mathcal{Y})\hspace{0.08cm}=\hspace{0.08cm}\left(\hspace{0.08cm} g_1(\mathcal{X}, \mathcal{Y}) \hspace{0.08cm},\hspace{0.08cm} g_2(\mathcal{X}, \mathcal{Y})\hspace{0.08cm} \right) \hspace{0.1cm}$,   a partir de la distribución conjunta de    $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$ e $\hspace{0.08cm}\mathcal{Y}\hspace{0.08cm}$.


- Si $\hspace{0.08cm}g \hspace{0.05cm}:\hspace{0.05cm} A \text{x} B \subset \mathbb{R}^2 \hspace{0.08cm}\rightarrow\hspace{0.08cm} \mathbb{R}^2\hspace{0.12cm}$ es continua, y $\hspace{0.08cm}g^{-1}\hspace{0.08cm}$ es diferenciable, entonces, la función de densidad conjunta de $\hspace{0.1cm}g(\mathcal{X}, \mathcal{Y})\hspace{0.08cm}=\hspace{0.08cm}\left(\hspace{0.08cm} g_1(\mathcal{X}, \mathcal{Y}) \hspace{0.08cm},\hspace{0.08cm} g_2(\mathcal{X}, \mathcal{Y})\hspace{0.08cm} \right) \hspace{0.1cm}$ es la siguiente: $\\[0.3cm]$


    $$f_{g_1(\mathcal{X}, \mathcal{Y}), g_2(\mathcal{X}, \mathcal{Y}) }(u,v) \hspace{0.1cm}=\hspace{0.1cm} f_{\mathcal{X},\mathcal{Y}}\left( \hspace{0.08cm} g^{-1}(u,v) \hspace{0.08cm} \right) \cdot \left| \hspace{0.08cm} J^{-1}(u,v) \hspace{0.08cm} \right| \cdot \mathbb{I}\hspace{0.08cm}\Bigl[ \hspace{0.08cm} \left\{\hspace{0.08cm}  g(x,y)   \hspace{0.12cm} :\hspace{0.12cm} (x,y) \in A\text{x}B \hspace{0.08cm}\right\} \hspace{0.08cm} \Bigr] \\$$

    donde:


    $$g(x,y) \hspace{0.08cm}=\hspace{0.08cm} \left( \hspace{0.08cm} g_1(x,y) \hspace{0.08cm} , \hspace{0.08cm} g_2(x,y) \hspace{0.08cm} \right)$$


    $$g^{-1}(u,v) \hspace{0.08cm}=\hspace{0.08cm} \left( \hspace{0.08cm} h_1(u,v) \hspace{0.08cm},\hspace{0.08cm} h_2(u,v) \hspace{0.08cm}\right)$$

    $$g(\hspace{0.08cm}g^{-1}(u,v)\hspace{0.08cm}) \hspace{0.08cm}=\hspace{0.08cm} g(\hspace{0.08cm}h_1(u,v)\hspace{0.08cm},\hspace{0.08cm}h_2(u,v)\hspace{0.08cm})\hspace{0.08cm}=\hspace{0.08cm}(u,v)$$

    $$g^{-1}(\hspace{0.08cm}g(x,y)\hspace{0.08cm}) \hspace{0.08cm}=\hspace{0.08cm} g^{-1}(\hspace{0.08cm}g_1(x,y)\hspace{0.08cm},\hspace{0.08cm}g_2(x,y)\hspace{0.08cm}) \hspace{0.08cm}=\hspace{0.08cm} \left(x, y\right) \\$$

    $$\left|\hspace{0.08cm} J^{-1}(u,v) \hspace{0.08cm} \right| \hspace{0.08cm} =\hspace{0.08cm} \left| \begin{array}{cc} \dfrac{\partial h_1(u,v)}{\partial u} & \dfrac{\partial h_1(u,v)}{\partial v}\\ \dfrac{\partial h_2(u,v)}{\partial u} & \dfrac{\partial h_2(u,v)}{\partial v} \end{array} \right|$$


<br>


# Fórmula de convolución  


Sea $\hspace{0.03cm}\mathcal{Z}=\mathcal{X}+\mathcal{Y}\hspace{0.03cm}$.

- Si $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$ e $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ son v.a.'s **continuas**: 


    $$f_{\mathcal{Z}}(z)  \hspace{0.08cm}= \hspace{0.08cm} f_{\mathcal{X}+\mathcal{Y}}(z)  \hspace{0.08cm}= \hspace{0.08cm} \int_{x\in \mathbb{R}} f_{\mathcal{X}, \mathcal{Y}}(x, z-x)\hspace{0.08cm} dx \\$$



- Si $\hspace{0.03cm}\mathcal{X}\hspace{0.03cm}$ e $\hspace{0.03cm}\mathcal{Y}\hspace{0.03cm}$ son v.a.'s **discretas**:  

$$f_{\mathcal{Z}}(z) \hspace{0.08cm}= \hspace{0.08cm} f_{\mathcal{X}+\mathcal{Y}}(z)  \hspace{0.08cm}= \hspace{0.08cm} \sum_{x\in \mathbb{R}}  \hspace{0.05cm} f_{\mathcal{X}, \mathcal{Y}}(x, z-x)$$



<br>

# Variables aleatorias i.i.d.

 

$\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.03cm}$ son variables aleatorias mutuamente independientes e idénticamente distribuidas  *(i.i.d.)*, si y solo si:

-  $\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.08cm}$ son mutuamente independientes 

-  $\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.05cm}$ tienen la misma distribución de probabilidad, es decir, $\hspace{0.12cm}\mathcal{X}_i \sim D( \cdot ) \hspace{0.15cm}, \hspace{0.1cm} \forall i=1,...,n$ $\\$



 

    Donde $\hspace{0.03cm}D( \cdot )\hspace{0.08cm}$ es una distribución de probabilidad con parámetros no especificados.  

<br>

**Observaciones:**

- La primera condición equivale a:


    $$\hspace{0.08cm} P(\mathcal{X}_1=x_1,...,\mathcal{X}_n=x_n) \hspace{0.1cm}=\hspace{0.1cm} \prod_{i=1}^n P(\mathcal{X}_i=x_i) \\$$

    Lo que implica que también son independientes dos a dos , es decir, $\hspace{0.12cm} \mathcal{X}_i \perp \mathcal{X}_j \hspace{0.3cm} , \hspace{0.3cm} \forall i\neq j \hspace{0.08cm}$. $\\$




- Se suele emplear la siguiente notación: 


    $$(\mathcal{X}_1,...,\mathcal{X}_n) \underset{i.i.d.}{\sim} D(\cdot) \hspace{0.3cm} \Leftrightarrow \hspace{0.3cm} \left\lbrace\begin{array}{l}   \hspace{0.12cm}\mathcal{X}_1,...,\mathcal{X}_n \hspace{0.25cm} \text{son mutuamente independientes} \\[0.1cm] \hspace{0.12cm}\mathcal{X}_i \sim D(\cdot) \hspace{0.12cm} , \hspace{0.12cm} \forall i=1,...,n    \end{array}\right. \\[1cm]$$



<br>

# Muestra Aleatoria Simple
 

 Sea $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$ una v.a. tal que $\hspace{0.1cm}\mathcal{X} \sim D(\cdot)$ .


$\mathcal{X}_1,...,\mathcal{X}_n \hspace{0.05cm}$ es una muestra aleatoria simple (m.a.s.) de tamaño $\hspace{0.04cm}n\hspace{0.04cm}$ de $\hspace{0.04cm}\mathcal{X}$ $\hspace{0.3cm}\Leftrightarrow\hspace{0.3cm}$ $(\mathcal{X}_1,...,\mathcal{X}_n) \underset{i.i.d.}{\sim} D(\cdot)$

 

**Observación:**

Una m.a.s. de una v.a. $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$ es un vector de v.a.'s mutuamente independientes y que se distribuyen probabilisticamente igual que la v.a. $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$.




<br>


 
# Teorema de esperanza-varianza de la media muestral

 

 Si tenemos una v.a. $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$ y una m.a.s $\hspace{0.1cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.1cm}$ de $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$ , entonces: $\\[0.5cm]$

$$E\left[\hspace{0.08cm}\overline{\mathcal{X}_n}\hspace{0.08cm}\right] \hspace{0.1cm}=\hspace{0.1cm} E[\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}]\\$$

$$Var\left(\hspace{0.08cm}\overline{\mathcal{X}_n}\hspace{0.08cm}\right) \hspace{0.1cm}=\hspace{0.1cm} Var(\mathcal{X})/n \\$$

 Donde:

$\hspace{0.25cm}\overline{\mathcal{X}_n} = \dfrac{1}{n} \cdot \sum_{i=1}^n \mathcal{X}_i$

 

<br>




# Teorema de distribución de la media muestral

 
 Si tenemos una v.a. $\hspace{0.1cm}\mathcal{X}\sim N(\mu , \sigma^2)\hspace{0.1cm}$ y una m.a.s $\hspace{0.1cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.1cm}$ de $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$ , entonces: $\\[0.5cm]$


$$\overline{ \mathcal{X}_n } \sim N(\mu \hspace{0.1cm},\hspace{0.1cm} \sigma^2/n)\\$$

Donde:

$$\mu \hspace{0.08cm}=\hspace{0.08cm} E\left[\hspace{0.08cm}\overline{\mathcal{X}_n}\hspace{0.08cm}\right]$$

$$\sigma^2 \hspace{0.08cm}=\hspace{0.08cm} Var\left(\hspace{0.08cm}\overline{\mathcal{X}_n}\hspace{0.08cm}\right) \\$$


Por tanto:


$$\dfrac{\hspace{0.2cm} \overline{ \mathcal{X}_n } - \mu \hspace{0.2cm}}{ \sqrt{\sigma^2 / n}  } \hspace{0.1cm}\sim\hspace{0.1cm} N(0,1)$$

 
<br>

# Independencia entre Media Aritmética y Cuasivarianza


Dada una v.a. $\hspace{0.1cm}\mathcal{X}\sim N(\mu, \sigma^2)\hspace{0.1cm}$, y dada una m.a.s $\hspace{0.1cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.1cm}$ de $\hspace{0.1cm}\mathcal{X} \hspace{0.08cm}$ , se cumple lo siguiente : $\\[0.7cm]$

$$\overline{X}_n \hspace{0.25cm} y \hspace{0.25cm} S_n^2 \hspace{0.25cm} \text{son variables aleatorias independientes.}$$


Donde:

 $\hspace{0.25cm}\overline{\mathcal{X}_n} = \dfrac{1}{n} \cdot \sum_{i=1}^n \mathcal{X}_i \\$

 $\hspace{0.25cm}S_n^2 = \dfrac{1}{n} \cdot \sum_{i=1}^n (\mathcal{X}_i - \overline{\mathcal{X}_n} )^2$

 




<br>



# Teorema de Gosset


Dada una v.a. $\hspace{0.1cm}\mathcal{X}\sim N(\mu, \sigma^2)\hspace{0.1cm}$, y dada una m.a.s $\hspace{0.1cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.1cm}$ de $\hspace{0.1cm}\mathcal{X} \hspace{0.08cm}$ , se cumple lo siguiente : $\\[0.5cm]$ 


$$\dfrac{\hspace{0.15cm} \overline{\mathcal{X}_n} - \mu \hspace{0.15cm}}{\sqrt{\dfrac{n}{n-1} S_n^2/n \hspace{0.1cm}} } \hspace{0.1cm}\sim\hspace{0.1cm} t_{n-1} \\$$

Donde:

 $\hspace{0.25cm}\overline{\mathcal{X}_n} = \dfrac{1}{n} \cdot \sum_{i=1}^n \mathcal{X}_i \\$

 $\hspace{0.25cm}S_n^2 = \dfrac{1}{n} \cdot \sum_{i=1}^n (\mathcal{X}_i - \overline{\mathcal{X}_n} )^2$

 
<br>


# Teorema de Fisher <a class="anchor" id="1"></a>


Dada una v.a. $\hspace{0.1cm}\mathcal{X}\sim N(\mu, \sigma^2)\hspace{0.1cm}$, y dada una m.a.s $\hspace{0.1cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.1cm}$ de $\hspace{0.1cm}\mathcal{X}\hspace{0.08cm} .$ , se cumple lo siguiente : $\\[0.5cm]$ 


$$\dfrac{\hspace{0.15cm} n\cdot S^2_n \hspace{0.15cm}}{ \sigma^2 \hspace{0.1cm} } \hspace{0.1cm}\sim\hspace{0.1cm} \chi^2_{n-1}$$


Donde:



$\hspace{0.25cm}S_n^2 = \dfrac{1}{n} \cdot \sum_{i=1}^n (\mathcal{X}_i - \overline{\mathcal{X}_n} )^2\\$

$\hspace{0.25cm} \overline{\mathcal{X}_n} = \dfrac{1}{n} \cdot \sum_{i=1}^n \mathcal{X}_i$ 
 
 
 <br>

# Algunas Desigualdades 

## Desigualdad de Markov

Dada una v.a. $\hspace{0.1cm}\mathcal{X} \hspace{0.1cm}$ y una función $\hspace{0.1cm}g \hspace{0.08cm} : \hspace{0.08cm} Im(\mathcal{X}) \rightarrow \mathbb{R}^+\hspace{0.08cm}$.

Se cumple la siguiente desigualdad, conocida como desigualdad de Markov: $\\[0.35cm]$


$$P(\hspace{0.08cm} g(\mathcal{X}) \hspace{0.08cm}\geq\hspace{0.08cm} \epsilon \hspace{0.08cm}) \hspace{0.1cm}\leq\hspace{0.1cm} \dfrac{\hspace{0.08cm}E\left[ \hspace{0.08cm}g(\mathcal{X})\hspace{0.08cm} \right]\hspace{0.08cm}}{\epsilon} \hspace{0.3cm} ,  \hspace{0.3cm} \forall  \hspace{0.08cm} \epsilon > 0$$



<br>

## Desigualdad de Chebychev <a class="anchor" id="1"></a>

Dada una v.a. $\hspace{0.1cm}\mathcal{X} \hspace{0.1cm}$ con $\hspace{0.1cm}Var(\hspace{0.08cm}\mathcal{X}\hspace{0.08cm})=\sigma^2\hspace{0.06cm}$.

Se cumple la siguiente desigualdad, conocida como desigualdad de Chebychev: $\\[0.35cm]$


$$P(\hspace{0.08cm} | \hspace{0.08cm} \mathcal{X} - \mu \hspace{0.08cm} | \hspace{0.08cm}\geq\hspace{0.08cm} \epsilon \hspace{0.08cm}) \hspace{0.1cm}\leq\hspace{0.1cm} \dfrac{\hspace{0.08cm}\sigma^2 \hspace{0.08cm} }{\epsilon^2} \hspace{0.3cm} ,  \hspace{0.3cm} \forall   \hspace{0.08cm} \epsilon > 0$$





<br>


# Definiciones de Convergencia

## Convergencia en probabilidad

 Dada una v.a. $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$, una m.a.s $\hspace{0.1cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.1cm}$ de $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$, y una función de la m.a.s $\hspace{0.1cm}h(\mathcal{X}_1,...,\mathcal{X}_n)\hspace{0.1cm}$, donde $\hspace{0.1cm}h \hspace{0.1cm} : \hspace{0.1cm} Im(\mathcal{X}_1)\hspace{0.05cm}\text{x}\dots \text{x}\hspace{0.05cm}Im(\mathcal{X}_n) \hspace{0.1cm}\rightarrow\hspace{0.1cm} \mathbb{R}\hspace{0.12cm}$ , entonces: $\\[1cm]$
 

 
 $$h(\mathcal{X}_1,...,\mathcal{X}_n)\hspace{0.1cm} \text{converge en probabilidad a} \hspace{0.1cm}\mathcal{X}\hspace{0.3cm} \Leftrightarrow \\[0.7cm] \Leftrightarrow\hspace{0.3cm} \forall \hspace{0.08cm} \epsilon > 0 \hspace{0.25cm} , \hspace{0.25cm} P\left(\hspace{0.1cm} \left| \hspace{0.1cm} h(\mathcal{X}_1,...,\mathcal{X}_n) - \mathcal{X} \hspace{0.1cm} \right| \hspace{0.1cm} \geq \hspace{0.1cm} \epsilon \hspace{0.1cm} \right) \hspace{0.12cm}\rightarrow\hspace{0.12cm} 0 \hspace{0.2cm} , \hspace{0.2cm} \text{si}\hspace{0.15cm} n \hspace{0.1cm}\rightarrow\hspace{0.1cm} \infty \\[0.7cm]
 \Leftrightarrow \hspace{0.3cm} \forall \hspace{0.08cm} \epsilon > 0 \hspace{0.3cm} , \hspace{0.3cm} \underset{n\rightarrow \infty}{\text{lim}} \hspace{0.12cm} P(\hspace{0.1cm} | \hspace{0.1cm} h(\mathcal{X}_1,...,\mathcal{X}_n) - \mathcal{X} \hspace{0.1cm} | \hspace{0.1cm} \geq \hspace{0.1cm} \epsilon \hspace{0.1cm}) \hspace{0.1cm}=\hspace{0.1cm} 0 \\[0.35cm]$$

 
**Notación:** 
 
$h(\mathcal{X}_1,...,\mathcal{X}_n)\hspace{0.1cm} \text{converge en probabilidad a} \hspace{0.1cm}\mathcal{X}$ $\hspace{0.3cm}\Leftrightarrow\hspace{0.3cm}$ $h(\mathcal{X}_1,...,\mathcal{X}_n)\hspace{0.1cm} \underset{p}{\rightarrow} \hspace{0.1cm}\mathcal{X}$


<br>

## Convergencia en distribución <a class="anchor" id="1"></a>


 Dada una v.a. $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$, una m.a.s $\hspace{0.1cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.1cm}$ de $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$, y una función de la m.a.s $\hspace{0.1cm}h(\mathcal{X}_1,...,\mathcal{X}_n)\hspace{0.1cm}$, donde $\hspace{0.1cm}h \hspace{0.1cm} : \hspace{0.1cm} Im(\mathcal{X}_1)\hspace{0.05cm}\text{x}\dots \text{x}\hspace{0.05cm}Im(\mathcal{X}_n) \hspace{0.1cm}\rightarrow\hspace{0.1cm} \mathbb{R}\hspace{0.12cm}$ , entonces: $\\[1cm]$
 
 
 $$h(\mathcal{X}_1,...,\mathcal{X}_n)\hspace{0.2cm} \text{converge en distribucion a} \hspace{0.2cm}\mathcal{X}\hspace{0.3cm}\Leftrightarrow \hspace{0.3cm}  \underset{n\rightarrow \infty}{\text{lim}} \hspace{0.12cm} F_{ h(\mathcal{X}_1,...,\mathcal{X}_n)} = F_{\mathcal{X}}(x)\\[0.35cm]$$

**Notación:** 
 
$h(\mathcal{X}_1,...,\mathcal{X}_n)\hspace{0.1cm} \text{converge en distribucion a} \hspace{0.1cm}\mathcal{X}$ $\hspace{0.3cm}\Leftrightarrow\hspace{0.3cm}$ $h(\mathcal{X}_1,...,\mathcal{X}_n)\hspace{0.1cm} \underset{d}{\rightarrow} \hspace{0.1cm}\mathcal{X}$


<br>

# Momentos de una Variable Aleatoria

El momento de orden $\hspace{0.08cm}k\hspace{0.08cm}$ de la variable aleatoria $\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}$ es $\hspace{0.1cm}E\left[\hspace{0.08cm}\mathcal{X}^k\hspace{0.08cm}\right]\hspace{0.08cm}$.


<br>

# Ley de los grandes números <a class="anchor" id="1"></a>

  La ley (débil) de los grandes establece lo siguiente:


- Si tenemos una m.a.s $\hspace{0.1cm}\mathcal{X}_1 ,...,\mathcal{X}_n\hspace{0.1cm}$  de una v.a. $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$ , entonces: $\\[0.6cm]$

      
    $$\overline{\mathcal{X}_n^k}\hspace{0.08cm} =\hspace{0.08cm} \dfrac{1}{n} \cdot \sum_{i=1}^n \mathcal{X}_i^k \hspace{0.3cm} \underset{p}{\rightarrow} \hspace{0.3cm} E\left[\hspace{0.1cm}\mathcal{X}^k\hspace{0.1cm}\right] \\[1cm]$$



    - El caso canónico es con $\hspace{0.08cm}k=1\hspace{0.08cm}$ :  $\\[0.6cm]$

    $$\overline{\mathcal{X}_n} \hspace{0.08cm}=\hspace{0.08cm} \dfrac{1}{n} \cdot \sum_{i=1}^n \mathcal{X}_i \hspace{0.3cm} \underset{p}{\rightarrow} \hspace{0.3cm} E\left[\hspace{0.08cm}\mathcal{X}\hspace{0.08cm}\right]$$
 






<br>

# Teorema central del límite


  El teorema central del límite (TCL) establece lo siguiente:

- Si tenemos una v.a. $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$ con $\hspace{0.1cm}E[\mathcal{X}] = \mu\hspace{0.2cm}$ y $\hspace{0.2cm}Var(\mathcal{X})=\sigma^2 \neq 0 < \infty\hspace{0.2cm}$ , y también tenemos una m.a.s $\hspace{0.1cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.1cm}$ de $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$ , entonces: $\\[0.5cm]$

       
       
       
    $$\overline{\mathcal{X}_n}  \hspace{0.2cm}\underset{d}{\rightarrow}\hspace{0.2cm} N(\mu\hspace{0.08cm},\hspace{0.08cm}\sigma^2/n)\\$$
        
        
    Donde:
        
    $\hspace{0.15cm}\mu = E[\mathcal{X}] =E[\overline{\mathcal{X}_n}]$ 
        
         
    $\hspace{0.15cm}\sigma^2/n = Var(\mathcal{X} ) / n = Var(\overline{\mathcal{X}_n}) \\$
    
    
   
    
    
    Por tanto:
    
    
    $$\dfrac{\overline{\mathcal{X}_n} - \mu}{\sqrt{\sigma^2/n}}   \hspace{0.2cm}\underset{d}{\rightarrow}\hspace{0.2cm} N(0,1)\\[0.25cm]$$





    Usando la definicion de convergencia en distribución tenemos lo siguiente: $\\[0.5cm]$



    $$\underset{n \rightarrow \infty}{lim} \hspace{0.1cm} F_{W_n}(x) \hspace{0.1cm} =\hspace{0.1cm} F_{N(0,1)}(x) \hspace{0.25cm} , \hspace{0.25cm} \forall x\in \mathbb{R}$$

    Donde:

    $$W_n \hspace{0.08cm}=\hspace{0.08cm} \dfrac{\overline{\mathcal{X}_n} - \mu}{\sqrt{\sigma^2 / n}}\\$$
 
 
 
 
**Interpretación:**

Si $\hspace{0.08cm}n\hspace{0.08cm}$ es grande y se obtiene un número elevado $\hspace{0.08cm}k\hspace{0.08cm}$ de realizaciones de la m.a.s. $\hspace{0.1cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.1cm}$ de $\mathcal{X}\sim D(\cdot)$, de manera que $\hspace{0.1cm}x_i = (x_{i1},...,x_{in})^t\hspace{0.1cm}$ es la realizacion i estima , donde $\hspace{0.1cm}x_{ij}\hspace{0.1cm}$ es la realizacion i-esima de la v.a. $\hspace{0.08cm}\mathcal{X}_j\hspace{0.1cm}$, entonces:

La distribución de frecuencias relativas del vector $\hspace{0.08cm}\bigl( \hspace{0.08cm} x_i=(x_{i1},...,x_{in}) \hspace{0.08cm}:\hspace{0.08cm} i=1,...,k \hspace{0.08cm}\bigr)\hspace{0.1cm}$ se aproxima a la de la distribución normal con media $\hspace{0.08cm}\mu=E[\mathcal{X}]\hspace{0.1cm}$ y varianza $\hspace{0.1cm}\sigma^2/n = Var(\mathcal{X})/n\hspace{0.08cm}$.

 
<br> 
 
# Teorema de Moivre-Laplace <a class="anchor" id="1"></a>

Este teorema es un caso particular del TCL para v.a`s Binomiales.


- Si tenemos una v.a. $\hspace{0.1cm}\mathcal{X}\sim Binomial(k,p)\hspace{0.1cm}$ y una m.a.s $\hspace{0.1cm}\mathcal{X}_1,...,\mathcal{X}_n\hspace{0.1cm}$ de $\hspace{0.1cm}\mathcal{X}\hspace{0.1cm}$ , entonces: $\\[0.5cm]$

 
 
    $$\dfrac{\hspace{0.1cm}\overline{\mathcal{X}_n} - \mu \hspace{0.1cm}}{\sqrt{\sigma^2/n}} \hspace{0.1cm}= \hspace{0.1cm}\dfrac{\overline{\mathcal{X}_n} - k\cdot p}{\hspace{0.1cm}\sqrt{k\cdot p\cdot(1-p)/n}\hspace{0.1cm}}\hspace{0.2cm}\underset{d}{\rightarrow}\hspace{0.2cm} N(0,1)$$
 
 
 
<br>




# Distribución Normal Multivariante

Dado el vector de variables aleatorias $\hspace{0.08cm}\mathbf{X}=\left( \mathcal{X}_1,\dots , \mathcal{X}_p \right)^t\hspace{0.08cm}$.

El vector aleatorio $\hspace{0.1cm}\mathbf{X}\hspace{0.1cm}$ tiene distribución normal multivariante, y lo denotamos como $\hspace{0.08cm}\mathbf{X} \sim NM_p(\mu , \sigma )\hspace{0.08cm}$, si y solo si, la función de densidad conjunta de las v.a.`s del vector es la siguiente: $\\[0.5cm]$

$$f_{\mathbf{X}}(x) \hspace{0.08cm}=\hspace{0.08cm} f_{\mathcal{X}_1,\dots , \mathcal{X}_p }(x) \hspace{0.1cm}=\hspace{0.1cm} \dfrac{1}{(2\pi)^{p/2} \cdot | {\Sigma}  |^{1/2} } \hspace{0.08cm} \cdot\hspace{0.08cm} \text{exp} \Bigl(\hspace{0.08cm} -\dfrac{1}{2}\cdot (x - \mathbf{\mu})^t \cdot {\Sigma}^{-1} \cdot (x - \mathbf{\mu}) \hspace{0.08cm} \Bigr)\\$$


Donde: 

- $\hspace{0.12cm}x = (x_1,\dots , x_p)^t \in \mathbb{R}^p \\$
 
- $\hspace{0.08cm}\mathbf{\mu}=(\mu_1,\dots , \mu_p)\hspace{0.15cm}$ es el vector de esperanzas de las v.a's , es decir,  $\hspace{0.1cm}\mu_j = E[\mathcal{X}_j] \hspace{0.2cm},\hspace{0.2cm} \forall \hspace{0.08cm} j=1,..., p\hspace{0.08cm} \\$


-  $\hspace{0.08cm}{\Sigma} = \left( \hspace{0.08cm}  \sigma_{ij} \hspace{0.1cm} : \hspace{0.1cm}  i,j=1,\dots p \hspace{0.08cm}  \right)\hspace{0.15cm}$ es la matriz de covarianzas de las v.a.'s, es decir,  $\hspace{0.1cm}\sigma_{ij}=Cov(\mathcal{X}_i , \mathcal{X}_j)\hspace{0.2cm} , \hspace{0.2cm}  \forall i,j =1,\dots , p \\$




## Propiedades  

Algunas propiedades de la distribución normal multivariante son las siguientes:



- $\text{Si} \hspace{0.3cm}\mathbf{X} = (X_1, X_2, \ldots, X_p)^t  \sim NM_p(\mu, \Sigma)\hspace{0.1cm}\hspace{0.3cm}$ $\Rightarrow$  $\hspace{0.3cm}X_j \sim N(\mu_j , \sigma_j^2)\hspace{0.3cm} ,\hspace{0.25cm} \forall\hspace{0.08cm} j=1,...,p$ $\\[0.8cm]$


- Si $\hspace{0.15cm}\mathbf{X} \sim NM_p(\mu, \Sigma)\hspace{0.15cm}$ , entonces cualquier combinación lineal de las componentes de $\hspace{0.1cm}\mathbf{X}\hspace{0.1cm}$ también sigue una distribución normal multivariante.

  
    Es decir:
    
    $$\text{Si} \hspace{0.3cm}\mathbf{X} \sim NM_p(\mu, \Sigma)\hspace{0.3cm}  \Rightarrow  \hspace{0.3cm}\mathcal{Y} \hspace{0.09cm}=\hspace{0.09cm} \mathbf{a}^t \cdot \mathbf{X} \hspace{0.09cm}=\hspace{0.09cm} \sum_{i=1}^p \hspace{0.09cm} a_i \cdot \mathcal{X}_i  \hspace{0.14cm}\sim\hspace{0.14cm} N(\mathbf{a}^t\cdot \mu \hspace{0.1cm},\hspace{0.1cm} \mathbf{a}^t \Sigma \mathbf{a} )\hspace{0.1cm}$$
    
    Donde:  
    
    $$\hspace{0.15cm}\mathbf{a}^t\cdot \mu\hspace{0.12cm} =\hspace{0.12cm} \sum_{i=1}^n a_i \cdot \mu_i \hspace{0.1cm}$$
    
    $$\hspace{0.1cm}\mathbf{a}^t \cdot \Sigma \cdot \mathbf{a} \hspace{0.13cm}=\hspace{0.13cm} \sum_{i=1}^p a_i^2\sigma{i}^2 \hspace{0.1cm}+\hspace{0.1cm} 2\cdot \sum_{i, j = 1,...,p \\ i\neq j} a_i \cdot a_j\cdot \sigma_{ij}$$ $\\[0.5cm]$





- $\text{Si} \hspace{0.3cm}\mathbf{X} \sim N_p(\mu, \Sigma)  \hspace{0.3cm} \Rightarrow \hspace{0.3cm} \mathbf{Y} = A\cdot \mathbf{X}+\mathbf{b} \hspace{0.12cm}\sim\hspace{0.12cm} NM_p(A\cdot \mu + \mathbf{b} \hspace{0.11cm},\hspace{0.11cm} A\cdot \Sigma\cdot A^t )$


   
    Donde:
    
    - $A\hspace{0.1cm}$ es una matriz $\hspace{0.1cm}q \times p\hspace{0.1cm}$.
    
    - $\mathbf{b}\hspace{0.1cm}$ es un vector $\hspace{0.11cm}q \times 1\hspace{0.1cm}$. $\\[0.8cm]$


- La distribución condicional de una parte de $\hspace{0.1cm}\mathbf{X}\hspace{0.1cm}$ dado el resto de las componentes sigue también una distribución normal multivariante. $\\[0.8cm]$


- Para variables aleatorias con distribución conjunta normal multivariante, independencia equivale a no correlación lineal.







<br>

---

<br>


# Bibliografía


- Jimenez Recaredo, R. (2021). Probabilidad II [Notas de clase]. Universidad Carlos III, Madrid, España. $\\[0.6cm]$

- Rincón, L (2014). Introducción a la  probabilidad. Universidad Nacional Autónoma de México. $\\[0.6cm]$

- Rincón, L (2007). Curso intermedio de probabilidad. Universidad Nacional Autónoma de México. $\\[0.6cm]$

<br>

<br>


