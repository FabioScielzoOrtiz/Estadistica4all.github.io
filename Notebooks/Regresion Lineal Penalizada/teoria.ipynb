{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar los métodos de regresion lineal penalizada, la respuesta y los predictores deben estandarizarse para que tengan media 0 y varianza 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos el típico modelo de regresión lineal: \n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 \\cdot x_{i1} + \\beta_2 \\cdot x_{i2} + ... +  \\beta_p \\cdot x_{ip} + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estandarizamos los predictores $X_1,...,X_p$ y la respuesta $Y$:\n",
    "\n",
    "$$\\widetilde{Y} = \\dfrac{Y - \\overline{y}}{\\sigma_Y} \\\\[0.5cm]$$\n",
    "\n",
    "$$\\widetilde{X_j} = \\dfrac{X_j - \\overline{x_j}}{\\sigma_{X_j}}$$ \n",
    "\n",
    "para $j=1,..,p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo estandarizado es:\n",
    "\n",
    "$$\\widetilde{y}_i = \\beta_0 + \\widetilde{\\beta}_1 \\cdot \\widetilde{x}_{i1} + \\widetilde{\\beta}_2 \\cdot \\widetilde{x}_{i2} + ... +  \\widetilde{\\beta}_p \\cdot \\widetilde{x}_{ip} + \\widetilde{\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\widetilde{\\beta_j} = \\dfrac{\\sigma_{X_j}}{\\sigma_{Y}}\\cdot \\beta_j \\hspace{0.5cm}$  , para $j=1,...,p$\n",
    "\n",
    "$\\beta_0 = \\overline{y} - \\sum_{j=1}^{p} \\overline{x}_j \\cdot \\beta_j$\n",
    "\n",
    "$\\widetilde{\\epsilon} = \\dfrac{\\epsilon}{\\sigma_{Y}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo estandarizado estimado es:\n",
    "\n",
    "$$\\widehat{\\widetilde{y}}_i = \\widehat{\\beta}_0 + \\widehat{\\widetilde{\\beta}}_1 \\cdot  \\widetilde{x}_{i1} + \\widehat{\\widetilde{\\beta}}_2 \\cdot \\widetilde{x}_{i2} + ... +  \\widehat{\\widetilde{\\beta}}_p \\cdot \\widetilde{x}_{ip}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora los coeficientes betas estimados $\\widehat{\\widetilde{\\beta}}_1, \\widehat{\\widetilde{\\beta}}_2 , ..., \\widehat{\\widetilde{\\beta}}_p$ son directamente comparables ya que no dependen de las unidades de medida de los predictores, puesto que al haber estandarizado estos las unidades de medida se eliminan.\n",
    "\n",
    "Ahora si que se cumple que dados dos betas estimados si  $\\widehat{\\widetilde{\\beta}}_r > \\widehat{\\widetilde{\\beta}}_h$ , entonces $X_r$ es mas relevante como predictor de $Y$ (tiene más peso) que $X_h$\n",
    "\n",
    "Esto no se cumplia en la regresion lineal ordinaria puesto que las estimaciones de los betas no eran directamente comparables al depender de las unidades de medida. Por ello betas estimados cercanos a cero no implicaban necesariamente que el predictor asociado fuese no significativo, ya que si ese predictor se media, por ejemplo, en millones de euros, un beta estimado de 0.001 serian 1000 euros (0.001 millones de euros), si se cambia la unidad de medida de dicho predictor de millones de euros a simplmenente euros entonces dicho beta estimado cambiaria de 0.001 a 1000 lo cual ya no es cercano a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de regresión Ridge es básicamente un modelo de regresión lineal en el que la respuesta y los predictores están estandarizados y la estimacion de los coeficientes betas se hace resolviendo el siguiente problema de optimización :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $\\hspace{0.1cm}\\widetilde{\\beta}= (\\widetilde{\\beta}_1,\\widetilde{\\beta}_2,...,\\widetilde{\\beta}_p)$ , $\\hspace{0.15cm} \\widetilde{x}_i = (\\widetilde{x}_{i1},...,\\widetilde{x}_{ip})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{gather*}\n",
    "  \\underset{\\widetilde{\\beta}}{Min} \\hspace{0.2cm}  \\biggl\\{ \\hspace{0.1cm} RSS(\\widetilde{\\beta}) \\hspace{0.1cm} +\\hspace{0.1cm}  \\lambda \\cdot | | \\hspace{0.06cm}\\widetilde{\\beta} \\hspace{0.06cm}||^2_2 \\hspace{0.1cm} \\biggl\\} \\hspace{0.2cm} = \\hspace{0.2cm}  \\underset{\\widetilde{\\beta}}{Min} \\hspace{0.2cm} \\biggl\\{ \\hspace{0.1cm} \\sum_{i=1}^{n} \\hspace{0.1cm}(y_i - \\beta_0 - \\widetilde{x}_i\\hspace{0.05cm}^t \\cdot \\beta)\\hspace{0.02cm}^2 \\hspace{0.1cm} + \\hspace{0.1cm}  \\lambda \\cdot | | \\hspace{0.06cm}\\widetilde{\\beta} \\hspace{0.06cm}||^2_2  \\hspace{0.1cm} \\biggl\\} \\hspace{0.2cm} =  \\\\[0.8cm]\n",
    "  = \\hspace{0.2cm} \\underset{\\widetilde{\\beta}_1,...,\\widetilde{\\beta}_p}{Min} \\hspace{0.2cm} \\biggl\\{ \\hspace{0.1cm} \\sum_{i=1}^{n} \\hspace{0.1cm}(y_i - \\beta_0 - \\widetilde{\\beta}_1 \\cdot x_{i1} - \\dots - \\widetilde{\\beta}_p \\cdot x_{ip})\\hspace{0.02cm}^2  \\hspace{0.1cm} + \\hspace{0.1cm}  \\lambda \\cdot | | \\hspace{0.06cm}\\widetilde{\\beta} \\hspace{0.06cm}||^2_2 \\hspace{0.1cm} \\biggl\\}\n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde: \n",
    "\n",
    "$\\lambda \\geq 0$ es un parámetro de penalización\n",
    "\n",
    "$| | \\hspace{0.06cm}\\widetilde{\\beta} \\hspace{0.06cm}||^2_2 \\hspace{0.1cm} = \\hspace{0.1cm} \\sum_{j=1}^p \\hspace{0.1cm} \\widetilde{\\beta}_j\\hspace{0.01cm}^2\\hspace{0.15cm}$ es la norma Euclidea del vector $\\hspace{0.15cm}\\widetilde{\\beta}\\hspace{0.15cm}$, la cual es una medida del tamaño del vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observaciones :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La expresión $\\hspace{0.15cm}\\lambda \\cdot | | \\hspace{0.06cm}\\widetilde{\\beta} \\hspace{0.06cm}||^2_2\\hspace{0.15cm}$ penaliza el tamaño del vector $\\hspace{0.15cm}\\widetilde{\\beta}$ \n",
    "\n",
    "Cuanto mayor sea $\\hspace{0.15cm}\\lambda\\hspace{0.15cm}$ mayor es la penalización impuesta al tamaño del vector $\\hspace{0.15cm}\\widetilde{\\beta} \\hspace{0.15cm}$  en el problema de optimización.\n",
    "\n",
    "Si $\\hspace{0.15cm}\\lambda = 0\\hspace{0.15cm}$ el problema de optimizacion es el de minimos cuadrados ordinarios, propio del modelo de regresion lineal ordinario.\n",
    "\n",
    "Si $\\hspace{0.15cm}\\lambda\\hspace{0.15cm}$ es grande  la solucion del problema $\\hspace{0.15cm}\\widehat{\\widetilde{\\beta}}\\hspace{0.05cm}^{Ridge} \\hspace{0.15cm}$  tendra un tamaño (norma Euclidea) pequeño, es decir, los  $\\hspace{0.15cm}\\widehat{\\widetilde{\\beta}}_1, \\widehat{\\widetilde{\\beta}}_2,..., \\widehat{\\widetilde{\\beta}}_p\\hspace{0.15cm}$ estarán cerca de $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ debe ser seleccionado a priori de la resolucion del problema de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ventajas de la regresion Ridge sobre la regresion de minimos cuadrados ordinarios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Las predicciones de la regresion por minimos cuadrados ordinarios son insesgadas pero tiene alta varianza, especialmente si $p \\approx n$. Además si $p>n$ no se puede estimar la regresion por minimos cuadrados ordinarios.\n",
    "\n",
    "-  La regresión Ridge se basa en el equilibrio entre sesgo y varianza. Disminuye sustancialmente la varianza de las predicciones a costa de aumentar un poco su sesgo. Además, incluso si $\\hspace{0.1cm}p \\approx n \\hspace{0.1cm}$ o $\\hspace{0.1cm} p>n\\hspace{0.1cm}$ , la regresion Ridge puede funcionar bien.\n",
    "\n",
    "- Esto lleva a que si $\\lambda$ es seleccionado correctamente, el error cuadratico medio de prediccion $(ECMP)$es menor en la regresion Ridge que en la de minimos cuadrados ordinarios, lo que conduce a mayor capacidad predictiva de Ridge sobre minimos cuadrados ordinarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c63d8c7d738c2960218a10995aedf0a7f67a49a231e71037adf0440953cdb45b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
