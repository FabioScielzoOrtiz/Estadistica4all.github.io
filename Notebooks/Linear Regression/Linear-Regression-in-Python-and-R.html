<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="generator" content="pandoc" />

        <meta name="author" content="Fabio Scielzo Ortiz" />
    
    
    <title>Linear Regression</title>

        <script src="Linear-Regression-in-Python-and-R_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <link href="Linear-Regression-in-Python-and-R_files/bootstrap-3.3.7/css/bootstrap.min.css" rel="stylesheet" />
        <script src="Linear-Regression-in-Python-and-R_files/bootstrap-3.3.7/js/bootstrap.min.js"></script>
        <script src="Linear-Regression-in-Python-and-R_files/navigation-1.1/tabsets.js"></script>
        <link href="Linear-Regression-in-Python-and-R_files/magnific-popup-1.1.0/magnific-popup.css" rel="stylesheet" />
        <script src="Linear-Regression-in-Python-and-R_files/magnific-popup-1.1.0/jquery.magnific-popup.min.js"></script>
        <link href="Linear-Regression-in-Python-and-R_files/readthedown-0.1/readthedown.css" rel="stylesheet" />
        <link href="Linear-Regression-in-Python-and-R_files/readthedown-0.1/readthedown_fonts_embed.css" rel="stylesheet" />
        <script src="Linear-Regression-in-Python-and-R_files/readthedown-0.1/readthedown.js"></script>
    
    
        <style type="text/css">code{white-space: pre;}</style>
    <style type="text/css">
      pre > code.sourceCode { white-space: pre; position: relative; }
      pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
      pre > code.sourceCode > span:empty { height: 1.2em; }
      .sourceCode { overflow: visible; }
      code.sourceCode > span { color: inherit; text-decoration: inherit; }
      div.sourceCode { margin: 1em 0; }
      pre.sourceCode { margin: 0; }
      @media screen {
      div.sourceCode { overflow: auto; }
      }
      @media print {
      pre > code.sourceCode { white-space: pre-wrap; }
      pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
      }
      pre.numberSource code
        { counter-reset: source-line 0; }
      pre.numberSource code > span
        { position: relative; left: -4em; counter-increment: source-line; }
      pre.numberSource code > span > a:first-child::before
        { content: counter(source-line);
          position: relative; left: -1em; text-align: right; vertical-align: baseline;
          border: none; display: inline-block;
          -webkit-touch-callout: none; -webkit-user-select: none;
          -khtml-user-select: none; -moz-user-select: none;
          -ms-user-select: none; user-select: none;
          padding: 0 4px; width: 4em;
          color: #aaaaaa;
        }
      pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
      div.sourceCode
        {  background-color: #f8f8f8; }
      @media screen {
      pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
      }
      code span.al { color: #ef2929; } /* Alert */
      code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
      code span.at { color: #c4a000; } /* Attribute */
      code span.bn { color: #0000cf; } /* BaseN */
      code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
      code span.ch { color: #4e9a06; } /* Char */
      code span.cn { color: #000000; } /* Constant */
      code span.co { color: #8f5902; font-style: italic; } /* Comment */
      code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
      code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
      code span.dt { color: #204a87; } /* DataType */
      code span.dv { color: #0000cf; } /* DecVal */
      code span.er { color: #a40000; font-weight: bold; } /* Error */
      code span.ex { } /* Extension */
      code span.fl { color: #0000cf; } /* Float */
      code span.fu { color: #000000; } /* Function */
      code span.im { } /* Import */
      code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
      code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
      code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
      code span.ot { color: #8f5902; } /* Other */
      code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
      code span.sc { color: #000000; } /* SpecialChar */
      code span.ss { color: #4e9a06; } /* SpecialString */
      code span.st { color: #4e9a06; } /* String */
      code span.va { color: #000000; } /* Variable */
      code span.vs { color: #4e9a06; } /* VerbatimString */
      code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
    </style>
    
        <link rel="stylesheet" href="custom.css" type="text/css" />
    
    <!-- tabsets -->
    <script>
      $(document).ready(function () {
	  window.buildTabsets("toc");
      });
      $(document).ready(function () {
	  $('.tabset-dropdown > .nav-tabs > li').click(function () {
	      $(this).parent().toggleClass('nav-tabs-open')
	  });
      });
    </script>

    <!-- code folding -->
    
    <!-- code download -->
    
    <!-- tabsets dropdown -->

    <style type="text/css">
      .tabset-dropdown > .nav-tabs {
	  display: inline-table;
	  max-height: 500px;
	  min-height: 44px;
	  overflow-y: auto;
	  background: white;
	  border: 1px solid #ddd;
	  border-radius: 4px;
      }
      
      .tabset-dropdown > .nav-tabs > li.active:before {
	  content: "";
	  font-family: 'Glyphicons Halflings';
	  display: inline-block;
	  padding: 10px;
	  border-right: 1px solid #ddd;
      }
      
      .tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
	  content: "&#xe258;";
	  border: none;
      }
      
      .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
	  content: "";
	  font-family: 'Glyphicons Halflings';
	  display: inline-block;
	  padding: 10px;
	  border-right: 1px solid #ddd;
      }
      
      .tabset-dropdown > .nav-tabs > li.active {
	  display: block;
      }

      .tabset-dropdown > .nav-tabs > li.active a {
  	  padding: 0 15px !important;
      }

      .tabset-dropdown > .nav-tabs > li > a,
      .tabset-dropdown > .nav-tabs > li > a:focus,
      .tabset-dropdown > .nav-tabs > li > a:hover {
	  border: none;
	  display: inline-block;
	  border-radius: 4px;
	  background-color: transparent;
      }
      
      .tabset-dropdown > .nav-tabs.nav-tabs-open > li {
	  display: block;
	  float: none;
      }
      
      .tabset-dropdown > .nav-tabs > li {
	  display: none;
	  margin-left: 0 !important;
      }
    </style>
    
</head>

<body class="preload">

   	
         <!-- readthedown start -->   
   <div id="content" data-toggle="wy-nav-shift">
     <nav id="nav-top" role="navigation" aria-label="top navigation">
       <a role="button" href="#" data-toggle="wy-nav-top"><span class="glyphicon glyphicon-menu-hamburger"></span></a>
     </nav>
         
   
        
      <h1 class="title">Linear Regression</h1>
      
         <!-- readthedown authors -->
   <div id="sidebar">
    <h2><a href="#content">Linear Regression</a></h2>
    <div id="toc">
      <ul>
      <li><a href="#introduction-to-the-linear-regression-model" id="toc-introduction-to-the-linear-regression-model"><span class="toc-section-number">0.1</span> Introduction to the Linear Regression Model <a class="anchor" id="4"></a></a></li>
      <li><a href="#estimation" id="toc-estimation"><span class="toc-section-number">0.2</span> Estimation <a class="anchor" id="10"></a></a></li>
      <li><a href="#model-train-validation" id="toc-model-train-validation"><span class="toc-section-number">0.3</span> Model Train Validation <a class="anchor" id="30"></a></a></li>
      <li><a href="#model-coefficients-interpretation" id="toc-model-coefficients-interpretation"><span class="toc-section-number">0.4</span> Model Coefficients Interpretation <a class="anchor" id="33"></a></a></li>
      <li><a href="#inference" id="toc-inference"><span class="toc-section-number">0.5</span> Inference <a class="anchor" id="40"></a></a></li>
      <li><a href="#goodness-of-fit-hspace0.1cm-determination-coefficient-r2" id="toc-goodness-of-fit-hspace0.1cm-determination-coefficient-r2"><span class="toc-section-number">0.6</span> Goodness of Fit: <span class="math inline">\(\hspace{0.1cm}\)</span> Determination Coefficient <span class="math inline">\((R^2)\)</span> <a class="anchor" id="54"></a></a></li>
      <li><a href="#goodness-of-fit-hspace0.1cm-adjusted-r2" id="toc-goodness-of-fit-hspace0.1cm-adjusted-r2"><span class="toc-section-number">0.7</span> Goodness of Fit: <span class="math inline">\(\hspace{0.1cm}\)</span> Adjusted <span class="math inline">\(R^2\)</span> <a class="anchor" id="57"></a></a></li>
      <li><a href="#model-problems" id="toc-model-problems"><span class="toc-section-number">0.8</span> Model Problems <a class="anchor" id="60"></a></a></li>
      <li><a href="#multicollinearity" id="toc-multicollinearity"><span class="toc-section-number">0.9</span> Multicollinearity <a class="anchor" id="61"></a></a></li>
      <li><a href="#checking-error-assumptions" id="toc-checking-error-assumptions"><span class="toc-section-number">0.10</span> Checking Error Assumptions <a class="anchor" id="68"></a></a></li>
      <li><a href="#checking-linear-assumption" id="toc-checking-linear-assumption"><span class="toc-section-number">0.11</span> Checking Linear Assumption <a class="anchor" id="73"></a></a></li>
      <li><a href="#finding-outliers" id="toc-finding-outliers"><span class="toc-section-number">0.12</span> Finding Outliers <a class="anchor" id="74"></a></a></li>
      <li><a href="#bibliography" id="toc-bibliography"><span class="toc-section-number">0.13</span> Bibliography</a></li>
      </ul>
    </div>
    <div id="postamble" data-toggle="wy-nav-shift" class="status">
                  <p class="author"><span class="glyphicon glyphicon-user"></span> Fabio Scielzo Ortiz</p>
                        <p class="date"><span class="glyphicon glyphicon-calendar"></span> 16/10/22</p>
          </div>
   </div>
     

   
      
   
<!-- Don't indent these lines or it will mess pre blocks indentation --> 
<div id="main">
<p>More articles in my blog: <span class="math inline">\(\hspace{0.1cm}\)</span> <a href="https://fabioscielzoortiz.github.io/Estadistica4all.github.io/">Estadistica4all</a></p>
<p> </p>
<p>How to reference this article ?</p>
<p>Scielzo Ortiz, F. (2022). Linear Regression with Python and R. <em>Estadistica4all</em>. <a href="https://fabioscielzoortiz.github.io/Estadistica4all.github.io/Articulos/Linear%20Regression%20in%20Python%20and%20R.html">Linear Regression with Python and R</a></p>
<p># Data-set description <a class="anchor" id="1"></a></p>
<p>We are going to describe the data-set we will use in this article.</p>
<p>The data are 1905 observation about 38 variables on housing features.</p>
<p>Here is the link where the data was loaded:
<a href="https://www.kaggle.com/datasets/dataregress/dubai-properties-dataset?resource=download" class="uri">https://www.kaggle.com/datasets/dataregress/dubai-properties-dataset?resource=download</a></p>
<p>The variables of our interest are the following:</p>
<ul>
<li><p>id : identificator</p></li>
<li><p>neighborhood: the name of the neighborhood</p></li>
<li><p>latitude: the latitude of the house</p></li>
<li><p>longitude: the longitude of the house</p></li>
<li><p>price: the market price of the house</p></li>
<li><p>size_in_sqft: the size of the house in square foot</p>
<ul>
<li>1 sqft = 0.092903 <span class="math inline">\(m^2\)</span></li>
</ul></li>
<li><p>price_per_sqft: the market price of the house per square foot</p></li>
<li><p>no_of_bedrooms: number of bedrooms in the house</p></li>
<li><p>no_of_bathrooms: number of bathrooms in the house</p></li>
<li><p>quality: quality of the house. Based on the number of services. Her
categories are Ultra, High, Medium and Low</p></li>
<li><p>maid_room: indicates if the house has maid room (cuarto de servicio)
(true/false)</p></li>
<li><p>unfurnished: indicates if the house is unfurnished (sin amueblar)
(true/false)</p></li>
<li><p>balcony: indicates if the house has balcony (true/false)</p></li>
<li><p>barbecue_area: indicates if the house has barbecue area (true/false)</p></li>
<li><p>central_ac: indicates if the house has central air conditioning
(true/false)</p></li>
<li><p>childrens_play_area: indicatees if the house has childrens game area
(true/false)</p></li>
<li><p>childrens_pool: indicates if the house has childrens pool
(true/false)</p></li>
<li><p>concierge: indicates if the house has concierge (true/false)</p></li>
<li><p>covered_parking: indicates if the house has covered parking
(true/false)</p></li>
<li><p>kitchen_appliances: indicates if the house has kitchen appliances
(electrodomesticos de cocina) (true/false)</p></li>
<li><p>maid_service: indicates if the house has maid service (servicio de
limpieza) (true/false)</p></li>
<li><p>pets_allowed: indicates if pets are allowed(true/false)</p></li>
<li><p>private_garden: indicates if the house has private garden
(true/false)</p></li>
<li><p>private_gym: indicates if the house has private gym (true/false)</p></li>
<li><p>private_jacuzzi: indicates if the house has private jacuzzi
(true/false)</p></li>
<li><p>private_pool: indicates if the house has private pool (true/false)</p></li>
<li><p>security: indicates if the house has private secutity (true/false)</p></li>
<li><p>shared_gym: indicates if the house has shared gym (true/false)</p></li>
<li><p>shared_pool: indicates if the house has shared pool (true/false)</p></li>
<li><p>shared_spa: indicates if the house has shared spa (true/false)</p></li>
<li><p>view_of_water: indicates if the house has view of the water
(true/false)</p></li>
</ul>
<p>Now we are going to do the following:</p>
<ol style="list-style-type: decimal">
<li><p>We are going to load an manipulate the data-set in R</p></li>
<li><p>We will repeat this task in Python</p></li>
</ol>
<hr />
<div id="data-manipulation-in-r" class="section level3" number="0.0.1">
<h3><span class="header-section-number">0.0.1</span> Data Manipulation in <code>R</code> <a class="anchor" id="2"></a></h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rpy2</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext rpy2.ipython</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rpy2.robjects <span class="im">as</span> robjects</span></code></pre></div>
<p>We load the data-set with which we are going to work:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>url <span class="ot">=</span> <span class="st">&#39;https://raw.githubusercontent.com/FabioScielzoOrtiz/Estadistica4all-blog/main/Linear%20Regression%20in%20Python%20and%20R/properties_data.csv&#39;</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>properties_data <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(url)</span></code></pre></div>
<p>Now, we are going to tranformate the variables that are measured in square foot (sqft) to square meters <span class="math inline">\((m^2)\)</span></p>
<p>size_in_m_2 = 0.092903 * size_in_sqft</p>
<p>price_per_m_2 = price_per_sqft / 0.092903</p>
<p>Now, we are going to tranformate the variables that are measured in square foot (sqft) to square meters <span class="math inline">\((m^2)\)</span></p>
<p>size_in_m_2 = 0.092903 * size_in_sqft</p>
<p>price_per_m_2 = price_per_sqft / 0.092903</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>size_in_m_2 <span class="ot">&lt;-</span>  <span class="fl">0.092903</span><span class="sc">*</span>properties_data<span class="sc">$</span>size_in_sqft</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>properties_data<span class="sc">$</span>size_in_m_2 <span class="ot">&lt;-</span> size_in_m_2</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>price_per_m_2 <span class="ot">&lt;-</span> properties_data<span class="sc">$</span>price_per_sqft <span class="sc">/</span>  <span class="fl">0.092903</span> </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>properties_data<span class="sc">$</span>price_per_m_2 <span class="ot">&lt;-</span> price_per_m_2</span></code></pre></div>
<p>The following step will be convert the categorical variables to factor in R and remove in the data-set the variables that we will not take into account:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(properties_data<span class="sc">$</span>quality)</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>properties_data<span class="sc">$</span>quality <span class="ot">=</span> <span class="fu">recode_factor</span>(properties_data<span class="sc">$</span>quality , <span class="st">&quot;Low&quot;</span> <span class="ot">=</span> <span class="dv">0</span> , <span class="st">&quot;Medium&quot;</span> <span class="ot">=</span> <span class="dv">1</span> , <span class="st">&quot;High&quot;</span> <span class="ot">=</span> <span class="dv">2</span>  , <span class="st">&quot;Ultra&quot;</span> <span class="ot">=</span> <span class="dv">3</span>)</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>properties_data<span class="sc">$</span>quality <span class="ot">=</span> <span class="fu">factor</span>(properties_data<span class="sc">$</span>quality)</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>data_R <span class="ot">&lt;-</span> properties_data <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="st">&quot;price&quot;</span>, <span class="st">&quot;size_in_m_2&quot;</span>, <span class="st">&quot;longitude&quot;</span>, <span class="st">&quot;latitude&quot;</span>, <span class="st">&quot;no_of_bedrooms&quot;</span>, <span class="st">&quot;no_of_bathrooms&quot;</span>, <span class="st">&quot;quality&quot;</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data_R,<span class="dv">15</span>)</span></code></pre></div>
<hr />
</div>
<div id="data-manipulation-in-python" class="section level3" number="0.0.2">
<h3><span class="header-section-number">0.0.2</span> Data Manipulation in <code>Python</code> <a class="anchor" id="3"></a></h3>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dfply <span class="im">import</span> <span class="op">*</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">&#39;ignore&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">&#39;https://raw.githubusercontent.com/FabioScielzoOrtiz/Estadistica4all-blog/main/Linear%20Regression</span><span class="sc">%20i</span><span class="st">n%20Python%20and%20R/properties_data.csv&#39;</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>data_Python <span class="op">=</span> pd.read_csv(url)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>data_Python</span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>data_Python[<span class="st">&#39;size_in_m_2&#39;</span>] <span class="op">=</span> <span class="fl">0.092903</span><span class="op">*</span>data_Python[<span class="st">&#39;size_in_sqft&#39;</span>]</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>data_Python[<span class="st">&#39;price_per_m_2&#39;</span>] <span class="op">=</span> data_Python[<span class="st">&#39;price_per_sqft&#39;</span>]<span class="op">/</span><span class="fl">0.092903</span></span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>data_Python <span class="op">=</span> data_Python <span class="op">&gt;&gt;</span> select(X.price , X.size_in_m_2, X.longitude, X.latitude, X.no_of_bedrooms, X.no_of_bathrooms, X.quality)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>data_Python</span></code></pre></div>
<p>Converting <span class="math inline">\(quality\)</span> to categorical:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>data_Python.dtypes</span></code></pre></div>
<p>Recoding the categorical variable <span class="math inline">\(quality\)</span></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>data_Python[<span class="st">&#39;quality&#39;</span>].unique()</span></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>(data_Python[<span class="st">&#39;quality_recode&#39;</span>]) <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span> , <span class="bu">len</span>(data_Python)) :</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (data_Python[<span class="st">&#39;quality&#39;</span>])[i] <span class="op">==</span> <span class="st">&#39;Low&#39;</span> :</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        (data_Python[<span class="st">&#39;quality_recode&#39;</span>])[i] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (data_Python[<span class="st">&#39;quality&#39;</span>])[i] <span class="op">==</span> <span class="st">&#39;Medium&#39;</span> :</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        (data_Python[<span class="st">&#39;quality_recode&#39;</span>])[i] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (data_Python[<span class="st">&#39;quality&#39;</span>])[i] <span class="op">==</span> <span class="st">&#39;High&#39;</span> :</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        (data_Python[<span class="st">&#39;quality_recode&#39;</span>])[i] <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (data_Python[<span class="st">&#39;quality&#39;</span>])[i] <span class="op">==</span> <span class="st">&#39;Ultra&#39;</span> :</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        (data_Python[<span class="st">&#39;quality_recode&#39;</span>])[i] <span class="op">=</span> <span class="dv">3</span></span></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>data_Python.head()</span></code></pre></div>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>data_Python <span class="op">=</span> data_Python <span class="op">&gt;&gt;</span> select( <span class="op">~</span>X.quality )</span></code></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>data_Python <span class="op">=</span> data_Python <span class="op">&gt;&gt;</span> rename(quality <span class="op">=</span> X.quality_recode)</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>data_Python[<span class="st">&#39;quality&#39;</span>] <span class="op">=</span> data_Python[<span class="st">&#39;quality&#39;</span>].astype(<span class="st">&#39;category&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>data_Python.dtypes</span></code></pre></div>
<p>The final data-set in Python would be:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>data_Python</span></code></pre></div>
<p>We can write the last data-frame as csv as follows:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>data_Python.to_csv(<span class="st">&#39;data_Python.csv&#39;</span> , index<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<p><strong>Important</strong>: to use categorical variables in a linear regression model in Python they must be recoded (their values must be numbers that represents their categories), i.e, we cannot use the variable <em>quality</em> , insteaf of it we can use <em>quality_recode</em></p>
<p>This is the reason we have recoded <em>quality</em> in Python but not in R, because in R is not strictly necessary.</p>
<p>Note we have obtained the same data-set that was obtained with R.</p>
<ul>
<li><p>The R data-set has been called <em>data_R</em></p></li>
<li><p>The Python data-set has been called <em>data_python</em></p></li>
</ul>
<p>We will use both of them throughout this article.</p>
<hr />
</div>
<div id="introduction-to-the-linear-regression-model" class="section level2" number="0.1">
<h2><span class="header-section-number">0.1</span> Introduction to the Linear Regression Model <a class="anchor" id="4"></a></h2>
<p>The principal propose of this article is carry out a theoretical and
also practical exposition of the linear regression model.</p>
<p>Without any doubt the this is the most know statistical model.</p>
<p>There is the idea that the linear regression model is outdated compared
with other modern statistical models. But I would like to defend his
validity nowadays, first of all as a statistical tool, and second as a
previous necessary step to learn other most modern and complex methods.</p>
<p>The linear regression model is the base of many modern regression
techniques, so that is highly recommended study it enough, before to go deeper in other statistical models.</p>
<div id="usefulness-of-the-linear-regression-model" class="section level3" number="0.1.1">
<h3><span class="header-section-number">0.1.1</span> Usefulness of the Linear Regression Model <a class="anchor" id="5"></a></h3>
<p>The main usefulness of the linear regression model is to predict the
values of a <strong>quantitative</strong> variable, called <strong>response variable</strong>, depending on the values of other, <strong>quantitative</strong> or <strong>categorical</strong> variables ,
called <strong>predictors</strong>.</p>
<p>The other important usefulness of the linear regression model is to do inference, in other words, analyze the relation between the response variable and the predictors.</p>
<hr />
</div>
<div id="formal-approach-to-the-linear-regression-model" class="section level3" number="0.1.2">
<h3><span class="header-section-number">0.1.2</span> Formal Approach to the Linear Regression Model <a class="anchor" id="6"></a></h3>
<p>We have the following elements:</p>
<ul>
<li><p><strong><em>Response Variable:</em></strong>  a <strong>quantitative</strong> variable
  <span class="math inline">\(Y=(y_{1} , y_2,...,y_n)^t\)</span></p></li>
<li><p><strong><em>Predictors:</em></strong> a set of <strong>quantitative</strong> or <strong>categorical</strong>
variables:</p></li>
</ul>
<p><span class="math display">\[\begin{gather*}
X_1 = (x_{11}, x_{21}, ..., x_{n1})^t \\
X_2 = (x_{12}, x_{22}, ..., x_{n2})^t \\
... \\
X_p = (x_{1p}, x_{2p}, ..., x_{np})^t
\end{gather*}\]</span></p>
<ul>
<li><p><strong><em>Predictors Matrix:</em></strong></p>
<p><span class="math display">\[\begin{gather*}
X=(1, X_1, X_2,...,X_p) =
\begin{pmatrix}
1 &amp; x_{11}&amp;x_{12}&amp;...&amp;x_{1p}\\
1 &amp; x_{21}&amp;x_{22}&amp;...&amp;x_{2p}\\
&amp;...&amp;\\
1&amp; x_{n1}&amp;x_{n2}&amp;...&amp;x_{np}
\end{pmatrix} =
\begin{pmatrix}
x_{1}\\
x_{2}\\
...\\
x_{n}
\end{pmatrix}
\end{gather*}\]</span></p></li>
<li><p><strong><em>Beta Coefficients vector:</em></strong></p></li>
</ul>
<p><span class="math display">\[\begin{gather*}
\beta=(\beta_{1}, \beta_{2}, ..., \beta_{n})^t
\end{gather*}\]</span></p>
<ul>
<li><strong><em>Errors (residuals) vector:</em></strong></li>
</ul>
<p><span class="math display">\[\begin{gather*}
\varepsilon=(\varepsilon_{1}, \varepsilon_{2}, ..., \varepsilon_{n})^t
\end{gather*}\]</span></p>
<hr />
</div>
<div id="basic-assumptions" class="section level3" number="0.1.3">
<h3><span class="header-section-number">0.1.3</span> Basic Assumptions <a class="anchor" id="7"></a></h3>
<p>The basic assumptions of the model are the following:</p>
<ul>
<li><p>$ y_i = x_i^t + _i = <em>0 + </em>{j=1}^{p} ( <em>j x</em>{ij} ) + _i = _0 + <em>1 x</em>{i1} + <em>2 x</em>{i2} + + <em>p x</em>{ip} + _i $</p></li>
<li><p><span class="math inline">\(\hspace{0.3cm} \varepsilon_i\)</span> is a random variable such that:</p>
<ul>
<li><p><span class="math inline">\(\hspace{0.3cm} E[\varepsilon_i]=0\)</span></p></li>
<li><p><span class="math inline">\(\hspace{0.3cm} Var(\varepsilon_i)=\sigma^2\)</span></p></li>
<li><p><span class="math inline">\(\hspace{0.3cm} \varepsilon_i \sim N(0,\sigma)\)</span></p></li>
<li><p><span class="math inline">\(\hspace{0.3cm} cov(\varepsilon_i , \varepsilon_j)=0 \hspace{0.15cm} ,\forall i\neq j\)</span></p></li>
</ul></li>
<li><p><span class="math inline">\(\hspace{0.3cm}\)</span> Additional assumptions:</p>
<ul>
<li><p><span class="math inline">\(\hspace{0.3cm} n &gt; p+1 \hspace{0.3cm}\)</span> ( nº observations <span class="math inline">\(&gt;\)</span> nº of beta coefficients )</p></li>
<li><p><span class="math inline">\(\hspace{0.3cm} Rg(X)=p+1\)</span></p></li>
</ul></li>
</ul>
<p>Why are these additional assumptions important ? $ $ Dimensionality problem (will be seen it in other article of the blog)</p>
<hr />
</div>
<div id="assumptions-consequences" class="section level3" number="0.1.4">
<h3><span class="header-section-number">0.1.4</span> Assumptions Consequences <a class="anchor" id="8"></a></h3>
<ul>
<li><p><span class="math inline">\(\hspace{0.15cm} y_i\)</span> is a random variable because <span class="math inline">\(\varepsilon_i\)</span> is a random variable</p></li>
<li><p><span class="math inline">\(\hspace{0.15cm} E[y_i]= x_i^t \cdot \beta\)</span></p></li>
<li><p><span class="math inline">\(\hspace{0.15cm} Var(y_i) = \sigma^2\)</span></p></li>
<li><p><span class="math inline">\(\hspace{0.15cm} y_i \sim N(\hspace{0.1cm} x_i^t \cdot \beta \hspace{0.1cm} , \hspace{0.1cm} \sigma^2 \hspace{0.1cm} )\)</span></p></li>
<li><p><span class="math inline">\(\hspace{0.15cm} cov(y_i , y_j)=0 \hspace{0.15cm}, \forall i\neq j\)</span></p></li>
</ul>
<hr />
</div>
<div id="matrix-representation-of-the-basic-assumption-of-the-model" class="section level3" number="0.1.5">
<h3><span class="header-section-number">0.1.5</span> Matrix representation of the basic assumption of the model <a class="anchor" id="9"></a></h3>
<ul>
<li><p>$ Y=X+ $</p></li>
<li><p>$_i N(0,) i=1,…,n $</p></li>
<li><p>$cov(_i , _j)=0 ij =1,…,n $</p></li>
</ul>
<hr />
</div>
</div>
<div id="estimation" class="section level2" number="0.2">
<h2><span class="header-section-number">0.2</span> Estimation <a class="anchor" id="10"></a></h2>
<div id="prediction-of-response-variable" class="section level3" number="0.2.1">
<h3><span class="header-section-number">0.2.1</span> Prediction of Response Variable <a class="anchor" id="11"></a></h3>
<p>The linear regression model predict the response variable value <span class="math inline">\(y_i\)</span> for the combination of predictors values <span class="math inline">\(x_i = (1,x_{i1}, x_{i2}, ..., x_{ip})^t\)</span> as:</p>
<p><span class="math display">\[\begin{gather*}
\widehat{y}_i \hspace{0.1cm}=\hspace{0.1cm} x_i^t \cdot \widehat{\beta}  \hspace{0.1cm}=\hspace{0.1cm} \widehat{\beta}_0 + \sum_{j=1}^{p} \widehat{\beta}_j \cdot x_{ij} \hspace{0.1cm}=\hspace{0.1cm} \widehat{\beta}_0 + \widehat{\beta}_1 \cdot x_{i1} + \widehat{\beta}_2 \cdot x_{i2} + ... + \widehat{\beta}_p \cdot x_{ip}
\end{gather*}\]</span></p>
<hr />
</div>
<div id="estimation-of-model-coefficients" class="section level3" number="0.2.2">
<h3><span class="header-section-number">0.2.2</span> Estimation of model coefficients <a class="anchor" id="12"></a></h3>
<p>The estimation of <span class="math inline">\(\beta\)</span> in the classic linear regression model is done
using the ordinary least square (OLS) method.</p>
<p><span class="math inline">\(\widehat{\beta}\)</span> is compute as the solution of the following optimitation
problem:</p>
<p><span class="math display">\[\begin{gather*}
  \underset{\beta}{Min} \hspace{0.2cm} RSS(\beta) \hspace{0.2cm} = \hspace{0.2cm}  \underset{\beta}{Min} \hspace{0.2cm}  \sum_{i=1}^{n} \hspace{0.1cm}(y_i - x_i^t \cdot \beta)\hspace{0.02cm}^2  \hspace{0.2cm} = \hspace{0.2cm}  \underset{\beta_0,\beta_1,...,\beta_p}{Min} \hspace{0.2cm}  \sum_{i=1}^{n} \hspace{0.1cm}(y_i - \beta_0 - \beta_1 \cdot x_{i1} - \dots - \beta_p \cdot x_{ip})\hspace{0.02cm}^2
\end{gather*}\]</span></p>
<p>The problem solution is:</p>
<p><span class="math display">\[\begin{gather*}
\widehat{\beta}=(X^t \cdot X)^{-1} \cdot X^t \cdot Y
\end{gather*}\]</span></p>
<p><strong>Interpretation :</strong></p>
<p>$ = x^t $ is the hyperplane that <strong>minimize</strong> the <strong>euclidean distance</strong> between the given values of the response variable <span class="math inline">\((Y)\)</span> and the points of the hyperplane given by <span class="math inline">\(\hspace{0.1cm} \hat{y}_i = x_i^t \cdot \widehat{\beta}\)</span></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>Image(filename<span class="op">=</span><span class="st">&#39;hyperplane.png&#39;</span>) </span></code></pre></div>
<p><strong>Observation:</strong></p>
<p>We will not view here the mathematical details about the resolution of
this optimization problem. But is a classic convex optimization problem,
so it´s enough to take first derivatives of the objetive function with
respect to the coefficients <span class="math inline">\(\beta_0,\beta_1,...,\beta_p\)</span> , set them equal to zero (0), and solve the resultant equation system with respect
to <span class="math inline">\(\beta\)</span></p>
<hr />
</div>
<div id="estimation-of-model-errors-residuals" class="section level3" number="0.2.3">
<h3><span class="header-section-number">0.2.3</span> Estimation of model errors (residuals) <a class="anchor" id="13"></a></h3>
<p>The model errors  <span class="math inline">\(\varepsilon_i\)</span>  are estimated as:</p>
<p><span class="math display">\[
\widehat{\varepsilon}_i \hspace{0.1 cm} = \hspace{0.1 cm} y_i - \widehat{y}_i \hspace{0.1 cm} = \hspace{0.1 cm} y_i - x_i^t \cdot \widehat{\beta}  
\]</span></p>
<p>for <span class="math inline">\(\hspace{0.1cm}\)</span> <span class="math inline">\(i=1,...,n\)</span></p>
<p><strong>Observation:</strong></p>
<p><span class="math inline">\(\hat{\varepsilon}_i\)</span>  is the error done by the model when it
predicts  <span class="math inline">\(y_i\)</span>  as  <span class="math inline">\(\hat{y}_i=x_i^t \cdot \hat{\beta}\)</span></p>
<hr />
</div>
<div id="residual-sum-of-squares-rss" class="section level3" number="0.2.4">
<h3><span class="header-section-number">0.2.4</span> Residual sum of squares (RSS) <a class="anchor" id="13.1"></a></h3>
<p>The size of the errors is quantify as the estimated errors sum of squares :</p>
<p><span class="math display">\[ RSS \hspace{0.1 cm} = \hspace{0.1 cm} \sum_{i=1}^{n} \hat{\varepsilon}_i\hspace{0.02cm}^2 \hspace{0.1 cm} = \hspace{0.1 cm} \sum_{i=1}^{n} (y_i - \widehat{y}_i)\hspace{0.02cm}^2 \hspace{0.1 cm} = \hspace{0.1 cm} \sum_{i=1}^{n} (y_i - x_i^t \cdot \widehat{\beta})\hspace{0.02cm}^2 \]</span></p>
<hr />
</div>
<div id="regression-hyperplane" class="section level3" number="0.2.5">
<h3><span class="header-section-number">0.2.5</span> Regression Hyperplane <a class="anchor" id="14"></a></h3>
<p>The regression hyperplane is the matrix expression of the predictions
that the model does of the response variable values:</p>
<p><span class="math display">\[ \hat{Y} = X \cdot \hat{\beta}\]</span></p>
<p>Where:   <span class="math inline">\(\hat{Y}=(\hat{y}_1,\hat{y}_2,...,\hat{y}_n)^t\)</span></p>
<hr />
</div>
<div id="hat-matrix" class="section level3" number="0.2.6">
<h3><span class="header-section-number">0.2.6</span> Hat-Matrix <a class="anchor" id="15"></a></h3>
<p><span class="math display">\[\begin{gather*}
\hat{Y} = X \cdot \hat{\beta} = X \cdot (X^t \cdot X)^{-1} \cdot X^t \cdot Y = H \cdot Y  
\end{gather*}\]</span></p>
<p>Where:</p>
<p> <span class="math display">\[H= X \cdot (X^t \cdot X)^{-1} \cdot X^t\]</span></p>
<p> is called <strong>Hat-Matrix</strong></p>
<hr />
</div>
<div id="estimation-of-the-linear-regression-model-in-r" class="section level3" number="0.2.7">
<h3><span class="header-section-number">0.2.7</span> Estimation of the Linear Regression Model in <code>R</code> <a class="anchor" id="16"></a></h3>
<p>In this section we are going to show how estimate a linear regression
model in R, using for this purpose the data-set that was showed at the begining of the article.</p>
<p>The linear regression model that we propose is the following:</p>
<p><span class="math display">\[\begin{gather*}
price_i = \beta_0 +  \beta_1 \cdot size\_in\_m\_2_i + \beta_2 \cdot no\_of\_bedrooms_i +  \beta_3 \cdot no\_of\_bathrooms_i + \\ + \beta_4 \cdot quality_i + \beta_5\cdot  latitude_i +  \beta_6 \cdot longitude_i + \varepsilon_i
\end{gather*}\]</span></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>model_R <span class="ot">&lt;-</span> <span class="fu">lm</span>( price <span class="sc">~</span> size_in_m_2 <span class="sc">+</span> no_of_bedrooms <span class="sc">+</span> no_of_bathrooms <span class="sc">+</span> quality <span class="sc">+</span> latitude <span class="sc">+</span> longitude ,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="at">data =</span> data_R)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model_R)</span></code></pre></div>
<hr />
</div>
<div id="estimation-of-linear-regression-model-in-python" class="section level3" number="0.2.8">
<h3><span class="header-section-number">0.2.8</span> Estimation of Linear Regression Model in <code>Python</code></h3>
</div>
<div id="estimation-of-linear-regression-model-in-python-with-statsmodels" class="section level3" number="0.2.9">
<h3><span class="header-section-number">0.2.9</span> Estimation of Linear Regression Model in <code>Python</code> with <code>statsmodels</code> <a class="anchor" id="17"></a></h3>
<p>We can implement a linear regression model in Python with <code>statsmodels</code> in two ways.</p>
<p><strong><em>First way:</em></strong> <span class="math inline">\(\hspace{0.1cm}\)</span> <code>statsmodels.formula.api</code></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install statmodels</span></span></code></pre></div>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span></code></pre></div>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>model_Py_smf <span class="op">=</span> smf.ols(formula <span class="op">=</span> <span class="st">&#39;price ~ size_in_m_2 + no_of_bedrooms + no_of_bathrooms + quality + latitude + longitude&#39;</span>, </span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>                       data <span class="op">=</span>data_Python)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>model_Py_smf <span class="op">=</span> model_Py_smf.fit()</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_Py_smf.summary())</span></code></pre></div>
<p><strong><em>Second way:</em></strong> <span class="math inline">\(\hspace{0.1cm}\)</span> <code>statsmodels.api</code></p>
<p>One of the most important differences is that <code>statsmodels.api</code> does not fit the intercept, while <code>statsmodels.formula.api</code> does.</p>
<p>Another important difference is that <code>statsmodels.api</code> doesn’t deal well with categorical predictors, while <code>statsmodels.formula.api</code> does.</p>
<p>To fit the intercept and properly treat categorical predictors with <code>statsmodels.api</code> we need to add the intercept and the dummy (binary) variables associated with the categorical predictors to the data matrix <span class="math inline">\(X\)</span> as new predictors, and this can be done using the function <code>varcharProcessing</code>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> varcharProcessing(X, varchar_process <span class="op">=</span> <span class="st">&quot;dummy_dropfirst&quot;</span>):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    dtypes <span class="op">=</span> X.dtypes</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> varchar_process <span class="op">==</span> <span class="st">&quot;drop&quot;</span>:   </span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> X.drop(columns <span class="op">=</span> dtypes[dtypes <span class="op">==</span> np.<span class="bu">object</span>].index.tolist())</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> varchar_process <span class="op">==</span> <span class="st">&quot;dummy&quot;</span>:</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> pd.get_dummies(X,drop_first<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> varchar_process <span class="op">==</span> <span class="st">&quot;dummy_dropfirst&quot;</span>:</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> pd.get_dummies(X,drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> pd.get_dummies(X,drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    X[<span class="st">&quot;intercept&quot;</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    cols <span class="op">=</span> X.columns.tolist()</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    cols <span class="op">=</span> cols[<span class="op">-</span><span class="dv">1</span>:] <span class="op">+</span> cols[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X[cols]</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X</span></code></pre></div>
<p>Let’s see how <code>varcharProcessing</code> works:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data_Python[[<span class="st">&#39;size_in_m_2&#39;</span>, <span class="st">&#39;longitude&#39;</span>, <span class="st">&#39;latitude&#39;</span>, <span class="st">&#39;no_of_bedrooms&#39;</span>, <span class="st">&#39;no_of_bathrooms&#39;</span>, <span class="st">&#39;quality&#39;</span>]]</span></code></pre></div>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>X.head()</span></code></pre></div>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>varcharProcessing(X, varchar_process <span class="op">=</span> <span class="st">&quot;dummy_dropfirst&quot;</span>).head()</span></code></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span></code></pre></div>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data_Python[[<span class="st">&#39;size_in_m_2&#39;</span>, <span class="st">&#39;longitude&#39;</span>, <span class="st">&#39;latitude&#39;</span>, <span class="st">&#39;no_of_bedrooms&#39;</span>, <span class="st">&#39;no_of_bathrooms&#39;</span>, <span class="st">&#39;quality&#39;</span>]]</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> varcharProcessing(X, varchar_process <span class="op">=</span> <span class="st">&quot;dummy_dropfirst&quot;</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data_Python[<span class="st">&#39;price&#39;</span>]</span></code></pre></div>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>model_Py_sm <span class="op">=</span> sm.OLS(y , X).fit()</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_Py_sm.summary())</span></code></pre></div>
<p>The previous output gives us the estimation of the model coefficients
(betas), both outputs give similar results (but we will consider the python output):</p>
<p><br></p>
<p>$
<em>0 = -6.207e+07 \
</em>{quality1} =1.4e+05\
<em>{quality2} = 3.406e+05 \
</em>{quality3} = 2.788e+05 \
<em>{size_in_m_2} =3.566e+04 \
</em>{no_of_bedrooms} = -8.367e+05 \
<em>{no_of_bathrooms} = -5.712e+04 \
</em>{latitude}=6.115e+06 \
_{longitude}= -1.677e+06 \
$</p>
<p><br></p>
<p>So, the estimated model is:</p>
<p><span class="math display">\[\begin{gather*}
\widehat{price}_i =  -6.207e+07 +  3.566e+04 \cdot size\_in\_m\_2_i -8.367e+05 \cdot no\_of\_bedrooms_i -5.712e+04 \cdot no\_of\_bathrooms_i +\\ 1.4e+05 \cdot quality1_i + 3.406e+05\cdot quality2_i + 2.788e+05  \cdot quality3_i  +6.115e+06\cdot  latitude_i -1.677e+06   \cdot longitude_i
\end{gather*}\]</span></p>
<p><br></p>
<p><strong>Observation:</strong></p>
<p>The categorical variable, <em>quality</em>, that has 4 categories (Low (0), Medium (1),
High (2), Ultra (3)), enter in the model with 3 variables (quality1 ,
quality2, quality3 ). The category. that is out of the model is Low (0) because is the firs category.</p>
<p>This isn´t a particularity of this variable, but rather it´s a property of the categorical variables in the regression models.</p>
<p>Later it will be seen how this affects model coefficients interpretation.</p>
<hr />
</div>
<div id="estimation-of-linear-regression-model-in-python-with-scikit-learn" class="section level3" number="0.2.10">
<h3><span class="header-section-number">0.2.10</span> Estimation of Linear Regression Model in <code>Python</code> with <code>scikit-learn</code> <a class="anchor" id="18"></a></h3>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install sklearn</span></span></code></pre></div>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span></code></pre></div>
<p>We can use a training data-set to train the model in Python with the <code>scikit-learn</code> module.</p>
<p>This concepts will be seen with much more detail in a specific article about cross validation techniques.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data_Python[[<span class="st">&#39;size_in_m_2&#39;</span>, <span class="st">&#39;longitude&#39;</span>, <span class="st">&#39;latitude&#39;</span>, <span class="st">&#39;no_of_bedrooms&#39;</span>, <span class="st">&#39;no_of_bathrooms&#39;</span>, <span class="st">&#39;quality&#39;</span>]]</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data_Python[<span class="st">&#39;price&#39;</span>]</span></code></pre></div>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> varcharProcessing(X, varchar_process <span class="op">=</span> <span class="st">&quot;dummy_dropfirst&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>X.head()</span></code></pre></div>
<p>We can fit a linear regression model with <code>sk-learn</code> :</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span></code></pre></div>
<p>We can fit the model with the full data-set:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>Model_Py_sklearn <span class="op">=</span> LinearRegression().fit(X, y)</span></code></pre></div>
<p>We can compute the coefficients of the model:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>Model_Py_sklearn.intercept_</span></code></pre></div>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>Model_Py_sklearn.coef_</span></code></pre></div>
<p>Note that the order of the array values follows the order of <span class="math inline">\(X\)</span> columns.</p>
<hr />
</div>
<div id="precision-of-the-estimation-of-beta-coefficients" class="section level3" number="0.2.11">
<h3><span class="header-section-number">0.2.11</span> Precision of the estimation of beta coefficients <a class="anchor" id="19"></a></h3>
<p>The precision of the estimations of model beta coefficients is given by the
variance of the beta coefficient estimators, that is, by <span class="math inline">\(\hspace{0.02cm}\)</span> <span class="math inline">\(Var(\widehat{\beta}_j)\)</span></p>
<p>It´s true that <span class="math inline">\(\hspace{0.07cm}\)</span>
<span class="math inline">\(\hat{\beta_j} \sim N(\beta_j , \sqrt{ \sigma^2 \cdot q_{jj} } )\)</span> <span class="math inline">\(\hspace{0.07cm}\)</span> then we have:</p>
<p><span class="math display">\[Var(\hat{\beta_j})=\sigma^2 \cdot q_{jj}\]</span></p>
<p>Therefore, the estimation of the variance of <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\widehat{\beta}_j\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is :</p>
<p><span class="math display">\[\widehat{Var}(\widehat{\beta}_j) = \widehat{\sigma}\hspace{0.02cm}^2 \cdot q_{jj}\]</span></p>
<p>Where:</p>
<p><span class="math inline">\(\widehat{\sigma}\hspace{0.02cm}^2 \hspace{0.05cm}\)</span> is the estimation of the error variance , i.e, <span class="math inline">\(\hspace{0.06cm}\)</span> <span class="math inline">\(\widehat{\sigma}\hspace{0.02cm}^2 = \widehat{Var}(\varepsilon_i)\)</span></p>
<p><span class="math inline">\(q_{jj}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is the element <span class="math inline">\(j+1\)</span> of the principal diagonal of the matrix <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\((X^t \cdot X)^{-1}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> , <span class="math inline">\(\forall j=0,1,...,p\)</span></p>
<p><strong><em>¿ Why are the variance of the coefficient estimators important ?</em></strong></p>
<p>The standard deviation of the coefficient estimators indicates how much
the estimations of the coefficients deviate/vary, in mean, when the model is re-trained using many different samples.</p>
<p>Suppose many samples are obtained, and with each of them a linear
regression model is trained. Then, we get many estimations of the model
coefficients, one with each sample.</p>
<p>Then  <span class="math inline">\(\sqrt{\widehat{Var}(\hat{\beta_j})}\)</span>  indicates how much
<span class="math inline">\(\hat{\beta_j}\)</span> varies, in mean, from one sample to another.</p>
<p>If the standard deviation is <strong>high</strong>, this indicates that will be obtained
big differences when <span class="math inline">\(\beta_j\)</span> is estimate with <span class="math inline">\(\hat{\beta_j}\)</span>
depending on the sample that is used for estimate it, that means
estimator <span class="math inline">\(\hat{\beta_j}\)</span> is <strong>imprecise</strong>, because it will be much
dispersion of the values of <span class="math inline">\(\hat{\beta_j}\)</span> respect to the mean.</p>
<p>On the contrary, if the standard deviation is <strong>low</strong>, this indicates that
will be obtained small differences when <span class="math inline">\(\beta_j\)</span> is estimate with
<span class="math inline">\(\hat{\beta_j}\)</span> depending on the sample that is used for estimate it,
that means estimator <span class="math inline">\(\hat{\beta_j}\)</span> is <strong>precise</strong>, because it will be little
dispersion of the values of <span class="math inline">\(\hat{\beta_j}\)</span> respect to the mean.</p>
<p>Also <span class="math inline">\(\hspace{0.05cm}\widehat{Var}(\widehat{\beta}_j)\hspace{0.05cm}\)</span> allow us to create a confidence interval for <span class="math inline">\(\hspace{0.05cm}\widehat{\beta}_j\)</span></p>
<hr />
<div id="estimation-of-the-standard-deviation-of-the-beta-coefficient-estimators-in-r" class="section level4" number="0.2.11.1">
<h4><span class="header-section-number">0.2.11.1</span> Estimation of the standard deviation of the beta coefficient estimators in <code>R</code> <a class="anchor" id="20"></a></h4>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R </span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model_R)</span></code></pre></div>
<hr />
</div>
</div>
<div id="estimation-of-the-standard-deviation-of-the-coefficient-estimators-in-python" class="section level3" number="0.2.12">
<h3><span class="header-section-number">0.2.12</span> Estimation of the standard deviation of the coefficient estimators in <code>Python</code> <a class="anchor" id="21"></a></h3>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_Py_sm.summary())</span></code></pre></div>
<p>This output give us a lot of information about the model, some of this
information has been seen (estimates coefficients), an other information
will be seen later.</p>
<p>Now we will focus in the part of the output where are the estimation of coefficients estimators standard error (<code>std err</code> in Python , <code>Std.Error</code> in R).</p>
<p>$
=2.995e+07 \
=8.358e+04 \
=1.551e+05\
= 1.976e+05 \
= 7.238e+02 \
=8.282e+04 \
=6.829e+04 \<br />
=7.809e+05\
=6.908e+05
$</p>
<p>The standard deviation estimates of the coefficients estimators are, in
general, so high. This implies if we train the model with another
samples, we will get estimates of the coefficients quite different than
the one obtained with our initial sample.</p>
<p>And this is a big problem, because from one sample to another are
obtained very different linear regression models, so that, very
different results with each sample.</p>
<hr />
</div>
<div id="model-predictions-in-r" class="section level3" number="0.2.13">
<h3><span class="header-section-number">0.2.13</span> Model Predictions in <code>R</code> <a class="anchor" id="22"></a></h3>
<p>With the function <code>predict</code> we can get the predictions made by the model
for the response variable <span class="math inline">\((y)\)</span> in R, for the data-set with which the model has been trained and also for a new data-set that the model hasn’t seen.</p>
<p><strong>Predictions with the train data:</strong></p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(model_R)[<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>]</span></code></pre></div>
<p><strong>Predictions with new data:</strong></p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>size_in_m_2  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">140</span>,<span class="dv">160</span>,<span class="dv">180</span>,<span class="dv">98</span>,<span class="dv">120</span>,<span class="dv">200</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>longitude  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">55.1</span>,<span class="fl">55.3</span>,<span class="fl">55.2</span>,<span class="fl">55.1</span>,<span class="fl">55.2</span>,<span class="fl">55.1</span>,<span class="fl">55.1</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>latitude <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">25.1</span>,<span class="fl">25.2</span>,<span class="fl">25.1</span>,<span class="fl">25.1</span>,<span class="fl">25.3</span>,<span class="fl">25.1</span>,<span class="fl">25.2</span>)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>no_of_bedrooms <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">4</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>no_of_bathrooms <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>quality <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>new_data <span class="ot">=</span> <span class="fu">cbind</span>(size_in_m_2,longitude,latitude,no_of_bedrooms,no_of_bathrooms,quality)</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>new_data <span class="ot">=</span> <span class="fu">as.data.frame</span>(new_data)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>new_data<span class="sc">$</span>quality <span class="ot">=</span> <span class="fu">factor</span>(new_data<span class="sc">$</span>quality)</span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>new_data</span></code></pre></div>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(model_R, new_data)</span></code></pre></div>
<hr />
</div>
<div id="model-predictions-in-python-with-statsmodels" class="section level3" number="0.2.14">
<h3><span class="header-section-number">0.2.14</span> Model Predictions in <code>Python</code> with <code>Statsmodels</code> <a class="anchor" id="23"></a></h3>
<p>With <code>smf</code> :</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>model_Py_smf <span class="op">=</span> smf.ols(formula <span class="op">=</span> <span class="st">&#39;price ~ size_in_m_2 + no_of_bedrooms + no_of_bathrooms + quality + latitude + longitude&#39;</span>, </span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>                 data <span class="op">=</span>data_Python).fit()</span></code></pre></div>
<p>Using the training data:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a> model_Py_smf.predict(data_Python)</span></code></pre></div>
<p>Using new data:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>size_in_m_2  <span class="op">=</span> pd.Series([<span class="dv">100</span>, <span class="dv">140</span>,<span class="dv">160</span>,<span class="dv">180</span>,<span class="dv">98</span>,<span class="dv">120</span>,<span class="dv">200</span>]) </span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>longitude  <span class="op">=</span> pd.Series([<span class="fl">55.1</span>,<span class="fl">55.3</span>,<span class="fl">55.2</span>,<span class="fl">55.1</span>,<span class="fl">55.2</span>,<span class="fl">55.1</span>,<span class="fl">55.1</span>]) </span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>latitude <span class="op">=</span> pd.Series([<span class="fl">25.1</span>,<span class="fl">25.2</span>,<span class="fl">25.1</span>,<span class="fl">25.1</span>,<span class="fl">25.3</span>,<span class="fl">25.1</span>,<span class="fl">25.2</span>])</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>no_of_bedrooms <span class="op">=</span> pd.Series([<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">4</span>])</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>no_of_bathrooms <span class="op">=</span> pd.Series([<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>])</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>quality <span class="op">=</span> pd.Series([<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>])</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>new_data <span class="op">=</span> pd.DataFrame({<span class="st">&#39;size_in_m_2&#39;</span>:size_in_m_2, <span class="st">&#39;longitude&#39;</span>:longitude , <span class="st">&#39;latitude&#39;</span>:latitude, <span class="st">&#39;no_of_bedrooms&#39;</span>:no_of_bedrooms , <span class="st">&#39;no_of_bathrooms&#39;</span>:no_of_bathrooms , <span class="st">&#39;quality&#39;</span>:quality }) </span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>new_data[<span class="st">&#39;quality&#39;</span>] <span class="op">=</span> new_data[<span class="st">&#39;quality&#39;</span>].astype(<span class="st">&#39;category&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>new_data.head()</span></code></pre></div>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a> model_Py_smf.predict(new_data)</span></code></pre></div>
<p>With <code>sm</code> :</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>model_Py_sm <span class="op">=</span> sm.OLS(y , X).fit()</span></code></pre></div>
<p>Using the training data:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a> model_Py_sm.predict(X)</span></code></pre></div>
<p>Using new data :</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>intercept <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>size_in_m_2  <span class="op">=</span> pd.Series([<span class="dv">100</span>, <span class="dv">140</span>,<span class="dv">160</span>,<span class="dv">180</span>,<span class="dv">98</span>,<span class="dv">120</span>,<span class="dv">200</span>]) </span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>longitude  <span class="op">=</span> pd.Series([<span class="fl">55.1</span>,<span class="fl">55.3</span>,<span class="fl">55.2</span>,<span class="fl">55.1</span>,<span class="fl">55.2</span>,<span class="fl">55.1</span>,<span class="fl">55.1</span>]) </span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>latitude <span class="op">=</span> pd.Series([<span class="fl">25.1</span>,<span class="fl">25.2</span>,<span class="fl">25.1</span>,<span class="fl">25.1</span>,<span class="fl">25.3</span>,<span class="fl">25.1</span>,<span class="fl">25.2</span>])</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>no_of_bedrooms <span class="op">=</span> pd.Series([<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">4</span>])</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>no_of_bathrooms <span class="op">=</span> pd.Series([<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>])</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>quality_1 <span class="op">=</span> pd.Series([<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>quality_2 <span class="op">=</span> pd.Series([<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>])</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>quality_3 <span class="op">=</span> pd.Series([<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>])</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>new_data <span class="op">=</span> pd.DataFrame({<span class="st">&#39;intercept&#39;</span>:intercept , <span class="st">&#39;size_in_m_2&#39;</span>:size_in_m_2, <span class="st">&#39;longitude&#39;</span>:longitude , <span class="st">&#39;latitude&#39;</span>:latitude, <span class="st">&#39;no_of_bedrooms&#39;</span>:no_of_bedrooms , <span class="st">&#39;no_of_bathrooms&#39;</span>:no_of_bathrooms , <span class="st">&#39;quality_1&#39;</span>:quality_1, <span class="st">&#39;quality_2&#39;</span>:quality_2, <span class="st">&#39;quality_3&#39;</span>:quality_3 }) </span></code></pre></div>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>new_data.head()</span></code></pre></div>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a> model_Py_sm.predict(new_data)</span></code></pre></div>
<hr />
</div>
<div id="model-predictions-in-python-with-sk-learn" class="section level3" number="0.2.15">
<h3><span class="header-section-number">0.2.15</span> Model Predictions in Python with <code>Sk-learn</code> <a class="anchor" id="24"></a></h3>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a> Model_Py_sklearn <span class="op">=</span> LinearRegression().fit(X, y)</span></code></pre></div>
<p>Using the training data:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>Model_Py_sklearn.predict(X)</span></code></pre></div>
<p>Using new data:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>intercept <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>size_in_m_2  <span class="op">=</span> pd.Series([<span class="dv">100</span>, <span class="dv">140</span>,<span class="dv">160</span>,<span class="dv">180</span>,<span class="dv">98</span>,<span class="dv">120</span>,<span class="dv">200</span>]) </span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>longitude  <span class="op">=</span> pd.Series([<span class="fl">55.1</span>,<span class="fl">55.3</span>,<span class="fl">55.2</span>,<span class="fl">55.1</span>,<span class="fl">55.2</span>,<span class="fl">55.1</span>,<span class="fl">55.1</span>]) </span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>latitude <span class="op">=</span> pd.Series([<span class="fl">25.1</span>,<span class="fl">25.2</span>,<span class="fl">25.1</span>,<span class="fl">25.1</span>,<span class="fl">25.3</span>,<span class="fl">25.1</span>,<span class="fl">25.2</span>])</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>no_of_bedrooms <span class="op">=</span> pd.Series([<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">4</span>])</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>no_of_bathrooms <span class="op">=</span> pd.Series([<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>])</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>quality_1 <span class="op">=</span> pd.Series([<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>quality_2 <span class="op">=</span> pd.Series([<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>])</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>quality_3 <span class="op">=</span> pd.Series([<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>])</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>new_data <span class="op">=</span> pd.DataFrame({<span class="st">&#39;intercept&#39;</span>:intercept , <span class="st">&#39;size_in_m_2&#39;</span>:size_in_m_2, <span class="st">&#39;longitude&#39;</span>:longitude , <span class="st">&#39;latitude&#39;</span>:latitude, <span class="st">&#39;no_of_bedrooms&#39;</span>:no_of_bedrooms , <span class="st">&#39;no_of_bathrooms&#39;</span>:no_of_bathrooms , <span class="st">&#39;quality_1&#39;</span>:quality_1, <span class="st">&#39;quality_2&#39;</span>:quality_2, <span class="st">&#39;quality_3&#39;</span>:quality_3 }) </span></code></pre></div>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>new_data.head()</span></code></pre></div>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>Model_Py_sklearn.predict(new_data)</span></code></pre></div>
<hr />
</div>
<div id="estimation-of-model-errors-in-r" class="section level3" number="0.2.16">
<h3><span class="header-section-number">0.2.16</span> Estimation of model errors in <code>R</code> <a class="anchor" id="25"></a></h3>
<p>We estimate the model errors as <span class="math inline">\(\hspace{0.1cm}\)</span> <span class="math inline">\(\hat{\varepsilon}_i= y_i - \hat{y}_i\)</span></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R </span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> data_R<span class="sc">$</span>price</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>( estimated_errors <span class="ot">&lt;-</span> y <span class="sc">-</span> <span class="fu">predict</span>(model_R) )[<span class="dv">1</span><span class="sc">:</span><span class="dv">30</span>]</span></code></pre></div>
<p>We put in a data frame the values of response variable observed in the
sample, the model predictions of the response, and the estimates of
model errors:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>(df_predictions <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">y=</span>y , </span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>                         <span class="at">y_predictions=</span><span class="fu">predict</span>(model_R), </span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>                         estimated_errors))[<span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>, ]</span></code></pre></div>
<hr />
</div>
<div id="estimation-of-model-errors-in-python" class="section level3" number="0.2.17">
<h3><span class="header-section-number">0.2.17</span> Estimation of model errors in <code>Python</code> <a class="anchor" id="26"></a></h3>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> pd.DataFrame( {<span class="st">&#39;predictions&#39;</span>:  model_Py_sm.predict(X)} )</span></code></pre></div>
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>df_predictions_Python <span class="op">=</span> pd.concat([y, predictions], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>df_predictions_Python.head()</span></code></pre></div>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>estimated_errors <span class="op">=</span> y <span class="op">-</span> model_Py_sm.predict(X)</span></code></pre></div>
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>df_predictions_Python[<span class="st">&#39;estimated_errors&#39;</span>] <span class="op">=</span> estimated_errors </span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>df_predictions_Python.head()</span></code></pre></div>
<hr />
</div>
<div id="estimation-of-the-error-variance-residual-variance" class="section level3" number="0.2.18">
<h3><span class="header-section-number">0.2.18</span> Estimation of the error variance (Residual Variance) <a class="anchor" id="27"></a></h3>
<p>The estimator of the error variance <span class="math inline">\(\hspace{0.1cm} Var(\varepsilon_i)=\sigma^2 \hspace{0.1cm}\)</span> is
called <strong>residual variance</strong>, and is defined as:</p>
<p><span class="math display">\[
\widehat{Var}(\varepsilon_i) = \widehat{\sigma}^2 = S_R^2 \hspace{0.1cm}=\hspace{0.1cm} \dfrac{1}{n-p-1} \cdot \sum_{i=1}^{n} \hat{\varepsilon}_i\hspace{0.02cm}^2 \hspace{0.1cm}=\hspace{0.1cm}  \dfrac{1}{n-p-1} \cdot (Y^t \cdot Y - \hat{\beta}^t \cdot X^t \cdot Y)
\]</span></p>
<p>The following is fulfilled:</p>
<p><span class="math display">\[
\dfrac{n-p-1}{\sigma^2} \cdot \widehat{\sigma}^2 \hspace{0.1cm} \sim \hspace{0.1cm} \chi_{n-p-1}^2
\]</span></p>
<p><span class="math display">\[
E[\widehat{\sigma}^2]=\sigma^2
\]</span></p>
<p><span class="math display">\[
Var(\widehat{\sigma}^2)=\dfrac{2 \cdot \sigma^4}{n-p-1}
\]</span></p>
<hr />
</div>
<div id="estimation-of-the-error-variance-in-r" class="section level3" number="0.2.19">
<h3><span class="header-section-number">0.2.19</span> Estimation of the error variance in <code>R</code> <a class="anchor" id="28"></a></h3>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span> <span class="fu">length</span>(estimated_errors)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>p<span class="ot">&lt;-</span><span class="dv">6</span></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>( estimated_variance_error <span class="ot">&lt;-</span> <span class="fu">sum</span>(estimated_errors<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(n<span class="sc">-</span>p<span class="dv">-1</span>) )</span></code></pre></div>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>( estimated_standard_deviation_error <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(estimated_variance_error) )</span></code></pre></div>
<hr />
</div>
<div id="estimation-of-the-error-variance-in-python" class="section level3" number="0.2.20">
<h3><span class="header-section-number">0.2.20</span> Estimation of the error variance in <code>Python</code> <a class="anchor" id="29"></a></h3>
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(data_Python)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">6</span></span></code></pre></div>
<div class="sourceCode" id="cb73"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>estimated_variance_error <span class="op">=</span> <span class="bu">sum</span>(df_predictions_Python[<span class="st">&#39;estimated_errors&#39;</span>]<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span>(n<span class="op">-</span>p<span class="op">-</span><span class="dv">1</span>) </span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>estimated_variance_error</span></code></pre></div>
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>estimated_standard_deviation_error <span class="op">=</span> math.sqrt(estimated_variance_error)</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>estimated_standard_deviation_error</span></code></pre></div>
<hr />
</div>
</div>
<div id="model-train-validation" class="section level2" number="0.3">
<h2><span class="header-section-number">0.3</span> Model Train Validation <a class="anchor" id="30"></a></h2>
<p>We can compute some metric in order to measure how much distant are the
predictions and observations of the response variable.</p>
<p>Two of the most common metrics are the mean absolute deviation (MAD) and the mean square error (MSE):</p>
<p><span class="math display">\[
MAD \hspace{0.1cm} = \hspace{0.1cm}   \dfrac{1}{n} \sum_{i=1}^{n} \mid \hat{\varepsilon}_i \mid \hspace{0.1cm} = \hspace{0.1cm} \dfrac{1}{n} \sum_{i=1}^{n} \hspace{0.1cm} \mid y_i - \hat{y}_i \mid \hspace{0.1cm} = \hspace{0.1cm} \dfrac{1}{n} \sum_{i=1}^{n} \hspace{0.1cm}\mid   y_i - x_i \cdot \widehat{\beta}  \mid
\]</span></p>
<p><span class="math display">\[
MSE \hspace{0.1cm} = \hspace{0.1cm} \dfrac{1}{n} \sum_{i=1}^{n} \hat{\varepsilon}_i^2 \hspace{0.1cm} =\hspace{0.1cm} \dfrac{1}{n} \sum_{i=1}^{n} \hspace{0.1cm}(\hspace{0.1cm} y_i - \hat{y}_i \hspace{0.1cm})^2 = \dfrac{1}{n} \sum_{i=1}^{n} \hspace{0.1cm}(\hspace{0.1cm} y_i - x_i \cdot \widehat{\beta} \hspace{0.1cm})^2
\]</span></p>
<p>When these metrics are computing using the data for <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span> that were used to train the model it is called <em>model train validation</em>.</p>
<hr />
<div id="model-train-validation-in-r" class="section level3" number="0.3.1">
<h3><span class="header-section-number">0.3.1</span> Model Train Validation in <code>R</code> <a class="anchor" id="31"></a></h3>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>( MAD <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>((estimated_errors))) ) </span></code></pre></div>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>( MSE <span class="ot">&lt;-</span> <span class="fu">mean</span>((estimated_errors)<span class="sc">^</span><span class="dv">2</span>) ) </span></code></pre></div>
<hr />
</div>
<div id="model-train-validation-in-python" class="section level3" number="0.3.2">
<h3><span class="header-section-number">0.3.2</span> Model Train Validation in <code>Python</code> <a class="anchor" id="32"></a></h3>
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>MAD_Py <span class="op">=</span> ( (y <span class="op">-</span> model_Py_sm.predict(X)).<span class="bu">abs</span>() ).mean()</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>MAD_Py</span></code></pre></div>
<div class="sourceCode" id="cb78"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>MSE_Py <span class="op">=</span> ( (y <span class="op">-</span> model_Py_sm.predict(X))<span class="op">**</span><span class="dv">2</span> ).mean()</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>MSE_Py</span></code></pre></div>
<p>In mean, the predictions that the model made of the response variable
deviates from the observations, in absolute value, in 938065 units.</p>
<p>This is an estimation of model error, but training error, because we
have used the predictions of the response variables made by the model
using the observations with which it has been trained.</p>
<p>There is a more interesting model error, called test error, that is
computed with predictors observations which haven´t been used to train
the model.</p>
<p>In this article, we will not go deeper unto that, but this concepts will
be more developed in another article about validation techniques.</p>
<hr />
</div>
</div>
<div id="model-coefficients-interpretation" class="section level2" number="0.4">
<h2><span class="header-section-number">0.4</span> Model Coefficients Interpretation <a class="anchor" id="33"></a></h2>
<div id="null-coefficient" class="section level3" number="0.4.1">
<h3><span class="header-section-number">0.4.1</span> Null Coefficient <a class="anchor" id="34"></a></h3>
<p>We have the following estimated linear regression model</p>
<p>  <span class="math display">\[\hat{y}= \hat{\beta_0} + \hat{\beta_0}\cdot x_{i1} + ...+ \hat{\beta_p}\cdot x_{ip}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\beta}_0\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is the model estimated value for the response variable, i.e <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span>
, when <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ij}=0\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> , <span class="math inline">\(\forall j=1,2,...,p\)</span></li>
</ul>
</div>
<div id="quantitative-predictor-coefficient" class="section level3" number="0.4.2">
<h3><span class="header-section-number">0.4.2</span> Quantitative Predictor Coefficient <a class="anchor" id="35"></a></h3>
<p>Let <span class="math inline">\(X_k\)</span> a <strong>quantitative</strong> variable, and <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(h&gt;0\)</span>,</p>
<p>We have the following estimated linear regression model  </p>
<p><span class="math display">\[\hat{y}_i= \hat{\beta_0} + \hat{\beta_0}\cdot x_{i1} + .. + \hat{\beta_k}\cdot x_{ik} + ..+ \hat{\beta_p}\cdot x_{ip}\]</span></p>
<ul>
<li><p>If <span class="math inline">\(\hat{\beta}_k &gt; 0\)</span> , then</p>
<ul>
<li><p>If <span class="math inline">\(x_{ik}\)</span> <strong>increases</strong> in <span class="math inline">\(h\)</span> units <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{y}_i\)</span>
<strong>increases</strong> in <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{\beta}_k \cdot h\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units.</p>
<p>And the opposite if it decreases.</p></li>
</ul></li>
<li><p>If <span class="math inline">\(\hat{\beta}_k &lt; 0\)</span> , then</p>
<ul>
<li><p>If <span class="math inline">\(x_{ik}\)</span> <strong>increases</strong> in <span class="math inline">\(h\)</span> units <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{y}_i\)</span>
<strong>decreases</strong> in <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{\beta}_k \cdot h\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units.</p>
<p>And the opposite if it decreases.</p></li>
</ul></li>
<li><p>If <span class="math inline">\(\hat{\beta}_k = 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> doesn´t depend on <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}\)</span></li>
</ul></li>
</ul>
<p><strong>Observation:</strong></p>
<p>The above affirmations are based in the following:</p>
<ul>
<li><span class="math inline">\((\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ik}=c+h ) - (\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ik}=c ) = \hat{\beta_k}\cdot h\)</span></li>
</ul>
</div>
<div id="categorical-predictor-coefficient" class="section level3" number="0.4.3">
<h3><span class="header-section-number">0.4.3</span> Categorical Predictor Coefficient <a class="anchor" id="36"></a></h3>
<div id="categorical-predictors-with-2-categories" class="section level4" number="0.4.3.1">
<h4><span class="header-section-number">0.4.3.1</span> Categorical Predictors with 2 categories</h4>
<p>Let <span class="math inline">\(X_k\)</span> a categorical variable with 2 categories
<span class="math inline">\(\lbrace 0 , 1 \rbrace\)</span>,</p>
<p>If the reference category is <span class="math inline">\(0\)</span> , then <span class="math inline">\(X_k\)</span> enter in the model as the binary (0,1) variable <span class="math inline">\(X_{k1}\)</span> defined as:</p>
<p><span class="math display">\[
x_{i k1}=1  \hspace{0.05cm} \Leftrightarrow \hspace{0.05cm}  x_{i k}=1
\]</span></p>
<p><br></p>
<p>In addition, we define the variable <span class="math inline">\(X_{k0}\)</span> as:</p>
<p><span class="math display">\[
x_{i k0}=1  \hspace{0.05cm} \Leftrightarrow \hspace{0.05cm}  x_{i k}=0
\]</span></p>
<p><br></p>
<p>We have the following estimated linear regression model:</p>
<p><span class="math display">\[\hat{y}_i= \hat{\beta_0} + \hat{\beta_0}\cdot x_{i1} + .. + \hat{\beta}_{k1} \cdot x_{ik1} + ..+ \hat{\beta_p}\cdot x_{ip}\]</span></p>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hat{\beta}_{k1} &gt; 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{\beta}_{k1}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units greater if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}=1\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 0\)</span></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hat{\beta}_{k1} &lt; 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{\beta}_{k1}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units less if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 1\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}=0\)</span></li>
</ul></li>
</ul>
<p><strong>Observation:</strong></p>
<p>The above affirmations are based in the following:</p>
<ul>
<li><span class="math inline">\((\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ik1}=1 ) - (\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ik1}=0 ) = \hat{\beta}_{k1}\)</span></li>
</ul>
</div>
<div id="categorical-predictors-with-3-categories" class="section level4" number="0.4.3.2">
<h4><span class="header-section-number">0.4.3.2</span> Categorical Predictors with 3 categories:</h4>
<p>Let <span class="math inline">\(X_k\)</span> a categorical variable with 3 categories
<span class="math inline">\(\lbrace 0 , 1, 2 \rbrace\)</span>,</p>
<p>If the reference category is <span class="math inline">\(0\)</span>, then <span class="math inline">\(X_k\)</span> enter in the model with
two binary <span class="math inline">\(\lbrace 0,1\rbrace\)</span> variables <span class="math inline">\(X_{k1}\)</span> y <span class="math inline">\(X_{k2}\)</span> defined as:</p>
<p><span class="math display">\[\begin{gather*}
x_{i k1}=1   \Leftrightarrow   x_{i k}=1 \\
x_{i k2}=1  \Leftrightarrow     x_{i k}=2
\end{gather*}\]</span></p>
<p><br></p>
<p>In addition, we define the variable <span class="math inline">\(X_{k0}\)</span> as:</p>
<p><span class="math display">\[
x_{i k0}=1  \hspace{0.05cm} \Leftrightarrow \hspace{0.05cm}  x_{i k}=0
\]</span></p>
<p><br></p>
<p>We have the following estimated linear regression model:</p>
<p><span class="math display">\[\hat{y}= \hat{\beta_0} + \hat{\beta_0}\cdot x_{i1} + .. + \hat{\beta}_{k1} \cdot x_{ik1} + \hat{\beta}_{k2} \cdot x_{ik2} + ..+ \hat{\beta_p}\cdot x_{ip}\]</span></p>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hat{\beta}_{k1} &gt; 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{\beta}_{k1}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>greater</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 1\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 0\)</span></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hat{\beta}_{k1} &lt; 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{\beta}_{k1}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>less</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 1\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 0\)</span></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hat{\beta}_{k2} &gt; 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{\beta}_{k2}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>greater</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 1\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 0\)</span></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hat{\beta}_{k2} &lt; 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{\beta}_{k2}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>less</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 1\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 0\)</span></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hat{\beta}_{k2} - \hat{\beta}_{k1} &gt; 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{\beta}_{k2} - \hat{\beta}_{k1}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>greater</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 2\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 1\)</span></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hat{\beta}_{k2} - \hat{\beta}_{k1} &lt; 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hat{\beta}_{k2} - \hat{\beta}_{k1}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units
<strong>less</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 2\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ik}= 1\)</span></li>
</ul></li>
</ul>
<p><br></p>
<p>Note that the above is easily extrapolated to the case in which we have a
categorical predictor with <span class="math inline">\(r\)</span> categories, for <span class="math inline">\(r&gt;3\)</span>.</p>
<p><strong>Observation:</strong></p>
<p>The above affirmations are based in the following:</p>
<ul>
<li><p><span class="math inline">\((\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ik1}=1 ) - (\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ik0}=1 ) = \hat{\beta}_{k1}\)</span></p></li>
<li><p><span class="math inline">\((\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ik2}=1 ) - (\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ik0}=1 ) = \hat{\beta}_{k2}\)</span></p></li>
<li><p>$(<em>i | x</em>{ik2}=1 ) - (<em>i | x</em>{ik1}=1 ) = <em>{k2} - </em>{k1} $</p></li>
</ul>
<p>Note that this can easily be extrapolated to the case of an <span class="math inline">\(r\)</span>-ary categorical variable with <span class="math inline">\(r &gt; 3\)</span></p>
<hr />
</div>
</div>
<div id="example-of-coefficient-interpretation" class="section level3" number="0.4.4">
<h3><span class="header-section-number">0.4.4</span> Example of coefficient interpretation <a class="anchor" id="37"></a></h3>
<p>We had obtained the following estimated model:</p>
<p><span class="math display">\[\begin{gather*}
\widehat{price}_i =  -6.207e+07 +  3.566e+04 \cdot size\_in\_m\_2_i -8.367e+05 \cdot no\_of\_bedrooms_i -5.712e+04 \cdot no\_of\_bathrooms_i + \\ 1.4e+05 \cdot quality1_i + 3.406e+05\cdot quality2_i + 2.788e+05  \cdot quality3_i  +6.115e+06\cdot  latitude_i -1.677e+06   \cdot longitude_i
\end{gather*}\]</span></p>
<p>The interpretation of the estimated model coefficients is the following:</p>
<p><br></p>
<ul>
<li><p><span class="math inline">\(\hat{\beta}_0 = -6.207e+07\)</span>   is the estimated <span class="math inline">\(price\)</span> by the model for the houses with <span class="math inline">\(size\_in\_m\_2_i =0\)</span> , <span class="math inline">\(no\_of\_bedrooms_i =0\)</span> , <span class="math inline">\(no\_of\_bathrooms_i =0\)</span> , <span class="math inline">\(qualityLow_i=0\)</span> , <span class="math inline">\(qualityMedium_i=0\)</span> , <span class="math inline">\(qualityUltra_i=0\)</span> , <span class="math inline">\(latitude_i=longitude_i=0\)</span></p></li>
<li><p><span class="math inline">\(\hat{\beta}_{size\_in\_m\_2} =3.566e+04\)</span>  <span class="math inline">\(\Rightarrow\)</span>  if
<span class="math inline">\(size\_in\_m\_2_i\)</span> increases in <span class="math inline">\(h\)</span> units, the estimated housing
<span class="math inline">\(price\)</span> <strong>increases</strong> in <span class="math inline">\(h\cdot 3.566e+04\)</span> units.</p></li>
<li><p><span class="math inline">\(\hat{\beta}_{no\_of\_bedrooms} = -8.367e+05\)</span>  <span class="math inline">\(\Rightarrow\)</span>  if
<span class="math inline">\(no\_of\_bedrooms_i\)</span> increases in <span class="math inline">\(h\)</span> units, the estimated housing
<span class="math inline">\(price\)</span> <strong>decreases</strong> in <span class="math inline">\(-h\cdot 8.367e+05\)</span> units.</p></li>
<li><p><span class="math inline">\(\hat{\beta}_{no\_of\_bathrooms} = -5.712e+04\)</span>  <span class="math inline">\(\Rightarrow\)</span>  if
<span class="math inline">\(no\_of\_bathrooms_i\)</span> increases in <span class="math inline">\(h\)</span> units, the estimated housing
<span class="math inline">\(price\)</span> <strong>decreases</strong> in <span class="math inline">\(-h\cdot 5.712e+04\)</span> units.</p></li>
<li><p><span class="math inline">\(\hat{\beta}_{quality1} = 1.4e+05\)</span>  <span class="math inline">\(\Rightarrow\)</span>   the estimated
<span class="math inline">\(price\)</span> of houses with medium quality <span class="math inline">\((quality1_i=1)\)</span> is <span class="math inline">\(1.4e+05\)</span> units <strong>greater</strong> than the estimated price of houses with low quality
<span class="math inline">\((quality0_i=1)\)</span> , because low quality is the reference category of <span class="math inline">\(quality\)</span> variable.</p></li>
<li><p><span class="math inline">\(\hat{\beta}_{quality2} = 3.406e+05\)</span>  <span class="math inline">\(\Rightarrow\)</span>   the
estimated <span class="math inline">\(price\)</span> of houses with medium quality
<span class="math inline">\((quality2_i=1)\)</span> is <span class="math inline">\(3.406e+05\)</span> units <strong>greater</strong> than the estimated
price of houses with high quality <span class="math inline">\((quality0_i=1)\)</span></p></li>
<li><p><span class="math inline">\(\hat{\beta}_{quality3} = 2.788e+05\)</span>  <span class="math inline">\(\Rightarrow\)</span>   the estimated
<span class="math inline">\(price\)</span> of houses with ultra quality <span class="math inline">\((quality1_i=1)\)</span> is
<span class="math inline">\(2.788e+05\)</span> units <strong>greater</strong> than the estimated price of houses with low quality <span class="math inline">\((quality0_i=1)\)</span></p></li>
<li><p><span class="math inline">\(\hat{\beta}_{quality2} - \hat{\beta}_{quality1} = 3.406e+05 - 1.4e+05 =2.006e+05\)</span>   <span class="math inline">\(\Rightarrow\)</span>  the estimated price of houses with high quality <span class="math inline">\((quality2_i=1)\)</span> is <span class="math inline">\(2.006e+05\)</span> units <strong>greater</strong> than the estimated price of houses with medium quality <span class="math inline">\((quality1_i=1)\)</span></p></li>
<li><p><span class="math inline">\(\hat{\beta}_{quality3} - \hat{\beta}_{quality2} = 2.788e+05 - 3.406e+05 =-61800\)</span>   <span class="math inline">\(\Rightarrow\)</span> the estimated price of houses with ultra quality <span class="math inline">\((quality3_i=1)\)</span> is <span class="math inline">\(61800\)</span> units <strong>less</strong> than the estimated price of houses with high quality <span class="math inline">\((quality2_i=1)\)</span></p></li>
</ul>
<p><br></p>
<p>Note that in all these cases it is assumed that the rest of the model variables don´t vary (have the same values) from one scenario to another.</p>
<hr />
</div>
<div id="interaction-coefficient" class="section level3" number="0.4.5">
<h3><span class="header-section-number">0.4.5</span> Interaction Coefficient <a class="anchor" id="38"></a></h3>
<div id="interaction-between-binary-and-quantitative-variables" class="section level4" number="0.4.5.1">
<h4><span class="header-section-number">0.4.5.1</span> Interaction between binary and quantitative variables</h4>
<p>Let <span class="math inline">\(X_k\)</span> a quantitative variable, and <span class="math inline">\(X_r\)</span> a <strong>binary</strong> <span class="math inline">\(\lbrace 0, 1\rbrace\)</span> categorical variable.</p>
<p>If the reference category is <span class="math inline">\(0\)</span> , then <span class="math inline">\(X_r\)</span> enter in the model as the binary <span class="math inline">\(\lbrace 0, 1\rbrace\)</span> variable <span class="math inline">\(X_{r1}\)</span> defined as:</p>
<p><span class="math display">\[
x_{i r1}=1  \hspace{0.05cm} \Leftrightarrow \hspace{0.05cm}  x_{i r}=1
\]</span></p>
<p>In addition, we define the variable <span class="math inline">\(X_{r0}\)</span> as:</p>
<p><span class="math display">\[
x_{i r0}=1  \hspace{0.05cm} \Leftrightarrow \hspace{0.05cm}  x_{i r}=0
\]</span></p>
<p>We have the following estimated linear regression model  </p>
<p><span class="math display">\[\hat{y}_i= \hat{\beta_0} + \hat{\beta_0}\cdot x_{i1} + ... + \hat{\beta_p}\cdot x_{ip} + \hat{\beta}_{r}\cdot x_{ir} + \hat{\beta}_{k}\cdot x_{ik} + \hat{\beta}_{rk}\cdot x_{ir}\cdot x_{ik}  \]</span></p>
<ul>
<li><p>If <span class="math inline">\(\hspace{0.05cm} \hat{\beta}_{r1} + \hat{\beta}_{r1}\cdot x_{ki} \hspace{0.05cm} &gt; \hspace{0.05cm} 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm} \hat{\beta}_{r1} + \hat{\beta}_{r1}\cdot x_{ki} \hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>greater</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 1\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 0\)</span></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hspace{0.05cm} \hat{\beta}_{r1} + \hat{\beta}_{r1}\cdot x_{ki} \hspace{0.05cm} &lt; \hspace{0.05cm} 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm} \hat{\beta}_{r1} + \hat{\beta}_{r1}\cdot x_{ki} \hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>less</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 1\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 0\)</span></li>
</ul></li>
</ul>
<p>Note these magnitudes depend on <span class="math inline">\(x_{ik}\)</span> value, because we have set an interaction between <span class="math inline">\(X_k\)</span> and <span class="math inline">\(X_r\)</span> in the model.</p>
<p><strong>Observation:</strong></p>
<p>The above affirmations are based in the following:</p>
<p><span class="math inline">\((\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ir1}=1 ) - (\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ir0}=1 ) = \hat{\beta}_{r1} + \hat{\beta}_{r1k}\cdot x_{ik}\)</span></p>
</div>
<div id="interaction-between-ternary-and-quantitative-variables" class="section level4" number="0.4.5.2">
<h4><span class="header-section-number">0.4.5.2</span> Interaction between ternary and quantitative variables</h4>
<p>Let <span class="math inline">\(X_k\)</span> a quantitative variable, and <span class="math inline">\(X_r\)</span> a <strong>ternary</strong> <span class="math inline">\(\lbrace 0, 1, 2 \rbrace\)</span> categorical variable.</p>
<p><br></p>
<p>If the reference category is <span class="math inline">\(0\)</span> , then <span class="math inline">\(X_r\)</span> enter in the model with
two binary <span class="math inline">\(\lbrace 0,1\rbrace\)</span> variables <span class="math inline">\(X_{r1}\)</span> y <span class="math inline">\(X_{r2}\)</span> defined as:</p>
<p><span class="math display">\[\begin{gather*}
x_{i r1}=1   \Leftrightarrow   x_{i r}=1 \\
x_{i r2}=1  \Leftrightarrow     x_{i r}=2
\end{gather*}\]</span></p>
<p><br></p>
<p>In addition, we define the variable <span class="math inline">\(X_{r0}\)</span> as:</p>
<p><span class="math display">\[
x_{i r0}=1  \hspace{0.05cm} \Leftrightarrow \hspace{0.05cm}  x_{i r}=0
\]</span></p>
<p><br></p>
<p>We have the following estimated linear regression model  </p>
<p><span class="math display">\[\hat{y}_i= \hat{\beta_0} + \hat{\beta_0}\cdot x_{i1} + ... + \hat{\beta_p}\cdot x_{ip}  + \hat{\beta}_{k}\cdot x_{ik} + \hat{\beta}_{r1}\cdot x_{ir1} + \hat{\beta}_{r2}\cdot x_{ir2} + \hat{\beta}_{r1k}\cdot x_{ir1}\cdot x_{ik} + \hat{\beta}_{r2 k}\cdot x_{ir2}\cdot x_{ik} \]</span></p>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hspace{0.05cm} \hat{\beta}_{r1} + \hat{\beta}_{r1}\cdot x_{ki} \hspace{0.05cm} &gt; \hspace{0.05cm} 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm} \hat{\beta}_{r1} + \hat{\beta}_{r1}\cdot x_{ki} \hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>greater</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 1\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 0\)</span></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hspace{0.05cm} \hat{\beta}_{r1} + \hat{\beta}_{r1}\cdot x_{ki} \hspace{0.05cm} &lt; \hspace{0.05cm} 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm} \hat{\beta}_{r1} + \hat{\beta}_{r1}\cdot x_{ki} \hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>less</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 1\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 0\)</span></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hspace{0.05cm} \hat{\beta}_{r2} + \hat{\beta}_{r2}\cdot x_{ki} \hspace{0.05cm} &gt; \hspace{0.05cm} 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm} \hat{\beta}_{r2} + \hat{\beta}_{r2}\cdot x_{ki} \hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>greater</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 2\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 0\)</span></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hspace{0.05cm} \hat{\beta}_{r2} + \hat{\beta}_{r2}\cdot x_{ki} \hspace{0.05cm} &lt; \hspace{0.05cm} 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm} \hat{\beta}_{r2} + \hat{\beta}_{r2}\cdot x_{ki} \hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>less</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 2\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 0\)</span></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hspace{0.05cm} (\hat{\beta}_{r2} - \hat{\beta}_{r1} ) + (\hat{\beta}_{r2} - \hat{\beta}_{r1})\cdot x_{ki} \hspace{0.05cm} &gt; \hspace{0.05cm} 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm} (\hat{\beta}_{r2} - \hat{\beta}_{r1} ) + (\hat{\beta}_{r2} - \hat{\beta}_{r1})\cdot x_{ki} \hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>greater</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 2\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 1\)</span></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p>If <span class="math inline">\(\hspace{0.05cm} (\hat{\beta}_{r2} - \hat{\beta}_{r1} ) + (\hat{\beta}_{r2} - \hat{\beta}_{r1})\cdot x_{ki} \hspace{0.05cm} &lt; \hspace{0.05cm} 0\)</span> , then</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> is <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm} (\hat{\beta}_{r2} - \hat{\beta}_{r1} ) + (\hat{\beta}_{r2} - \hat{\beta}_{r1})\cdot x_{ki} \hspace{0.05cm}\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> units <strong>less</strong> if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 2\)</span> <span class="math inline">\(\hspace{0.05cm}\)</span> than if <span class="math inline">\(\hspace{0.05cm}\)</span> <span class="math inline">\(x_{ir}= 1\)</span></li>
</ul></li>
</ul>
<p><br></p>
<p>Note these magnitudes depend on <span class="math inline">\(x_{ik}\)</span> value, because we have set an interaction between <span class="math inline">\(X_k\)</span> and <span class="math inline">\(X_r\)</span> in the model.</p>
<p><strong>Observation:</strong></p>
<p>The above affirmations are based in the following:</p>
<ul>
<li><p><span class="math inline">\((\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ir1}=1 ) - (\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ir0}=1 ) = \hat{\beta}_{r1} + \hat{\beta}_{r1k}\cdot x_{ik}\)</span></p></li>
<li><p><span class="math inline">\((\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ir2}=1 ) - (\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ir0}=1 ) = \hat{\beta}_{r2} + \hat{\beta}_{r2k}\cdot x_{ik}\)</span></p></li>
<li><p><span class="math inline">\((\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ir2}=1 ) - (\hat{y}_i \hspace{0.05cm} | \hspace{0.05cm} x_{ir1}=1 ) = (\hat{\beta}_{r2} - \hat{\beta}_{r1} ) + (\hat{\beta}_{r2k} - \hat{\beta}_{r1k})\cdot x_{ki}\)</span></p></li>
</ul>
<p>Note that this can easily be extrapolated to the case of interaction between an <span class="math inline">\(r\)</span>-ary categorical variable and a quantitative variable, for <span class="math inline">\(r&gt;3\)</span>.</p>
<hr />
</div>
</div>
<div id="example-of-interaction-coefficient-interpretation" class="section level3" number="0.4.6">
<h3><span class="header-section-number">0.4.6</span> Example of interaction coefficient interpretation <a class="anchor" id="39"></a></h3>
<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>model_Python_2 <span class="op">=</span> smf.ols(formula <span class="op">=</span> <span class="st">&#39;price ~  quality*size_in_m_2 + no_of_bedrooms + no_of_bathrooms + latitude + longitude&#39;</span>, </span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>                               data <span class="op">=</span>data_Python)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>model_Python_2 <span class="op">=</span> model_Python_2.fit()</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_Python_2.summary())</span></code></pre></div>
<p>We have got the following:</p>
<p><span class="math inline">\(\left( \widehat{price}_i | quality1_i=1 \right) - \left(\widehat{price}_i | quality0_i=1 \right) = \hat{\beta}_{quality1} + \hat{\beta}_{quality1:size\_in\_m\_2} \cdot size\_in\_m\_2_i = -2.353\cdot 10^5 + 2908.2719 \cdot size\_in\_m\_2_i\)</span></p>
<p>This magnitude depends on the value of <span class="math inline">\(size\_in\_m\_2_i\)</span></p>
<p>For example if <span class="math inline">\(size\_in\_m\_2_i = 100\)</span> , then:</p>
<p><span class="math inline">\(\left( \widehat{price}_i | quality1_i=1 \right) - \left(\widehat{price}_i | quality0_i=1 \right) = 55527.19\)</span></p>
<p>So the estimated price of a 100 <span class="math inline">\(m^2\)</span> house is <span class="math inline">\(55527.19\)</span> units greater if <span class="math inline">\(quality1_i=1\)</span> (it has medium quality) than <span class="math inline">\(quality0_i=1\)</span> (it has low quality)</p>
<p><span class="math inline">\(\left( \widehat{price}_i | quality2_i=1 \right) - \left(\widehat{price}_i | quality0_i=1 \right) = \hat{\beta}_{quality2} + \hat{\beta}_{quality2:size\_in\_m\_2} \cdot size\_in\_m\_2_i = -1.373\cdot 10^6 + 1.208\cdot 10^4 \cdot size\_in\_m\_2_i\)</span></p>
<p>This magnitude depends on the value of <span class="math inline">\(size\_in\_m\_2_i\)</span></p>
<p>For example if <span class="math inline">\(size\_in\_m\_2_i = 100\)</span> , then:</p>
<p><span class="math inline">\(\left( \widehat{price}_i | quality2_i=1 \right) - \left(\widehat{price}_i | quality0_i=1 \right) = 55527.19\)</span></p>
<p>So the estimated price of a 100 <span class="math inline">\(m^2\)</span> house is <span class="math inline">\(165000\)</span> units less if <span class="math inline">\(quality2_i=1\)</span> (it has high quality) than <span class="math inline">\(quality0_i=1\)</span> (it has low quality)</p>
<p>$( <em>i | quality3_i=1 ) - (<em>i | quality1_i=1 ) = (</em>{quality3} - </em>{quality1}) + ( <em>{quality3:size_in_m_2} - </em>{quality1:size_in_m_2}) size_in_m_2_i = (1.318e+06 - (-2.353e+05 )) + (-1.145e+04 + 2908.2719 )size_in_m_2_i $</p>
<p>This magnitude depends on the value of <span class="math inline">\(size\_in\_m\_2_i\)</span></p>
<p>For example if <span class="math inline">\(size\_in\_m\_2_i = 100\)</span> , then:</p>
<p><span class="math inline">\(\left( \widehat{price}_i | quality3_i=1 \right) - \left(\widehat{price}_i | quality1_i=1 \right) = 699127.19\)</span></p>
<p>So the estimated price of a 100 <span class="math inline">\(m^2\)</span> house is <span class="math inline">\(699127.19\)</span> units greater if <span class="math inline">\(quality3_i=1\)</span> (it has ultra quality) than <span class="math inline">\(quality1_i=1\)</span> (it has medium quality)</p>
<p>Note that in all these cases it is assumed that the rest of the model variables don´t vary (have the same values) from one scenario to another.</p>
<hr />
</div>
</div>
<div id="inference" class="section level2" number="0.5">
<h2><span class="header-section-number">0.5</span> Inference <a class="anchor" id="40"></a></h2>
<div id="confidence-interval-for-beta_j" class="section level3" number="0.5.1">
<h3><span class="header-section-number">0.5.1</span> Confidence Interval for <span class="math inline">\(\beta_j\)</span> <a class="anchor" id="41"></a></h3>
<p><span class="math inline">\(\hspace{0.05cm}\widehat{Var}(\widehat{\beta}_j)\hspace{0.05cm}\)</span></p>
<p><span class="math display">\[\begin{gather*}
IC\left(\beta_j  \right)= \left[ \hspace{0.05cm} \hat{\beta}_j \ \hspace{0.1cm} \pm \hspace{0.1cm}  \ t_{\alpha/2}^{n-p-1} \cdot \sqrt{\widehat{Var}(\widehat{\beta}_j)} \hspace{0.05cm} \right] = \left[\hspace{0.05cm}  \hat{\beta}_j \ \hspace{0.1cm} \pm \hspace{0.1cm}  \ t_{\alpha/2}^{n-p-1} \cdot \sqrt{S_R^2 \cdot q_{jj}} \hspace{0.05cm} \right]
\end{gather*}\]</span></p>
<p>Where:</p>
<p><span class="math inline">\(q_{jj} \hspace{0.1cm}\)</span> is the element <span class="math inline">\(\hspace{0.1cm} j+1 \hspace{0.1cm}\)</span> of the principal diagonal of the matrix
<span class="math inline">\(\hspace{0.1cm} (X^t \cdot X)^{-1} \hspace{0.1cm}\)</span> for <span class="math inline">\(\hspace{0.1cm} j=0,1,...,p\)</span></p>
<p><strong>Observation:</strong></p>
<p>The smaller <span class="math inline">\(\hspace{0.1cm}\sqrt{\widehat{Var}(\widehat{\beta}_j)}\hspace{0.1cm}\)</span> is, the smaller the confidence interval of <span class="math inline">\(\hspace{0.1cm}\widehat{\beta}_j\hspace{0.1cm}\)</span> will be.</p>
</div>
<div id="confidence-interval-for-sigma2" class="section level3" number="0.5.2">
<h3><span class="header-section-number">0.5.2</span> Confidence Interval for <span class="math inline">\(\sigma^2\)</span> <a class="anchor" id="42"></a></h3>
<p><span class="math display">\[\begin{gather*}
IC\left(\sigma^2  \right)= \left[ 0 \ , \ \dfrac{n-p-1}{\chi_{1-\alpha/2}^{n-p-1}}\cdot S_R^2  \right]
\end{gather*}\]</span></p>
<hr />
<div id="confidence-interval-for-beta_j-in-r" class="section level4" number="0.5.2.1">
<h4><span class="header-section-number">0.5.2.1</span> Confidence Interval for  <span class="math inline">\(\beta_j\)</span>  in <code>R</code> <a class="anchor" id="43"></a></h4>
<p>We can compute de confidence interval of <span class="math inline">\(\beta_j\)</span> with a confidence
level of <span class="math inline">\(95\%\)</span> for <span class="math inline">\(j=0,1,...,p\)</span></p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(model_R , <span class="at">conf.level=</span><span class="fl">0.95</span>)</span></code></pre></div>
<hr />
</div>
<div id="confidence-interval-for-beta_j-in-python" class="section level4" number="0.5.2.2">
<h4><span class="header-section-number">0.5.2.2</span> Confidence Interval for  <span class="math inline">\(\beta_j\)</span>  in <code>Python</code> <a class="anchor" id="44"></a></h4>
<div class="sourceCode" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>beta_intervals <span class="op">=</span> model_Py_sm.conf_int(alpha<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>beta_intervals.columns <span class="op">=</span> [<span class="st">&#39;2.5%&#39;</span>, <span class="st">&#39;97.5%&#39;</span>]</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>beta_intervals</span></code></pre></div>
<p>Then, for example, we have:</p>
<p><span class="math display">\[
IC(\beta_0)=\left[-1.208060e+08 \ , \ -3.336873e+06 \right]
\]</span>
<span class="math display">\[
IC(\beta_{size\_in\_m\_2})=\left[ 3.424446e+04   \ , \ 3.708364e+04 \right]
\]</span></p>
<p>We also have this information in the output obtained with <code>print(model_Python_1.summary())</code></p>
<hr />
</div>
<div id="confidence-interval-for-sigma2-in-r" class="section level4" number="0.5.2.3">
<h4><span class="header-section-number">0.5.2.3</span> Confidence Interval for  <span class="math inline">\(\sigma^2\)</span>  in <code>R</code> <a class="anchor" id="45"></a></h4>
<p>We can compute de confidence interval of <span class="math inline">\(\sigma^2\)</span> with a confidence
level of <span class="math inline">\(95\%\)</span> usingl the package “model”.</p>
<p>We can load this package running the following code:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R </span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;remotes&quot;)</span></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="co"># remotes::install_github(&quot;fhernanb/model&quot;)</span></span></code></pre></div>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(model)</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="fu">confint_sigma2</span>(<span class="at">object=</span>model_R , <span class="at">level=</span><span class="fl">0.95</span>)</span></code></pre></div>
<p>Then, we have:</p>
<p><span class="math display">\[\begin{gather*}
IC(\sigma^2)=\left[2.418577e+12   \ , \ 2.747045e+12 \right]
\end{gather*}\]</span></p>
<hr />
</div>
<div id="confidence-interval-for-sigma2-in-python" class="section level4" number="0.5.2.4">
<h4><span class="header-section-number">0.5.2.4</span> Confidence Interval for  <span class="math inline">\(\sigma^2\)</span>  in <code>Python</code> <a class="anchor" id="46"></a></h4>
<p>In this case we will use the interval expression before defined:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy</span></code></pre></div>
<div class="sourceCode" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="bu">len</span>(data_Python)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>p<span class="op">=</span><span class="dv">6</span></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>scipy.stats.chi2.ppf(<span class="fl">0.95</span>, n<span class="op">-</span>p)</span></code></pre></div>
<div class="sourceCode" id="cb86"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> ( n<span class="op">-</span>p<span class="op">-</span><span class="dv">1</span> <span class="op">/</span> scipy.stats.chi2.ppf(<span class="fl">0.95</span>, n<span class="op">-</span>p<span class="op">-</span><span class="dv">1</span>))<span class="op">*</span>estimated_variance_error</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>interval_sigma2 <span class="op">=</span> [<span class="dv">0</span>,b]</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>interval_sigma2</span></code></pre></div>
<hr />
</div>
</div>
<div id="hypothesis-test-for-beta_j" class="section level3" number="0.5.3">
<h3><span class="header-section-number">0.5.3</span> Hypothesis Test for <span class="math inline">\(\beta_j\)</span> <a class="anchor" id="47"></a></h3>
<p>We can carry out the following three test:</p>
<table>
<colgroup>
<col width="36%" />
<col width="31%" />
<col width="31%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(H_0: \beta_j = \beta_j^*\)</span></td>
<td align="center"><span class="math inline">\(H_0: \beta_j = \beta_j^*\)</span></td>
<td align="center"><span class="math inline">\(H_0: \beta_j = \beta_j^*\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(H_1: \beta_j \neq \beta_j^*\)</span></td>
<td align="center"><span class="math inline">\(H_1: \beta_j &gt; \beta_j^*\)</span></td>
<td align="center"><span class="math inline">\(H_1: \beta_j &lt; \beta_j^*\)</span></td>
</tr>
</tbody>
</table>
<div id="test-statistic" class="section level4" number="0.5.3.1">
<h4><span class="header-section-number">0.5.3.1</span> Test Statistic:</h4>
<p>The test statistic for any of the previous test is:</p>
<p><span class="math display">\[\begin{gather*}
t_{exp | H_0}=\dfrac{\hat{\beta}_j - \beta_j^*}{\sqrt{S_R \cdot q_{jj}}} \sim t_{n-p}
\end{gather*}\]</span></p>
</div>
<div id="decision-rule" class="section level4" number="0.5.3.2">
<h4><span class="header-section-number">0.5.3.2</span> Decision Rule</h4>
<p>For a fixed signification level <span class="math inline">\(\alpha\)</span></p>
<ul>
<li><p>Case   <span class="math inline">\(H_0: \beta_j = \beta_j^*\)</span>  vs  <span class="math inline">\(H_1: \beta_j \neq \beta_j^*\)</span></p></li>
<li><ul>
<li>Based in the test statistic:</li>
</ul></li>
</ul>
<p><span class="math display">\[\begin{gather*}
Reject \ H_0 \ \Leftrightarrow \ t_{exp|H_0} &gt; t_{\alpha/2}^{n-p} \ ó \ t_{exp|H_0} &lt; t_{1-\alpha/2}^{n-p}
\end{gather*}\]</span></p>
<ul>
<li><ul>
<li>Based in p-value:
         </li>
</ul></li>
</ul>
<p><span class="math display">\[\begin{gather*}
pvalue=2\cdot P( t_{n-p} \geqslant \mid t_{exp|H_0} \mid ) \\ \\
Reject \ H_0  \ \Leftrightarrow \ pvalue &lt; \alpha
\end{gather*}\]</span></p>
<ul>
<li><p>Case   <span class="math inline">\(H_0: \beta_j = \beta_j^*\)</span>  vs  <span class="math inline">\(H_1: \beta_j &gt; \beta_j^*\)</span></p></li>
<li><ul>
<li>Based in the test statistic:</li>
</ul></li>
</ul>
<p><span class="math display">\[\begin{gather*}
Reject \ H_0 \ \Leftrightarrow \ t_{exp|H_0} &gt; t_{\alpha/2}^{n-p}
\end{gather*}\]</span></p>
<ul>
<li><ul>
<li>Based in p-value:</li>
</ul></li>
</ul>
<p><span class="math display">\[\begin{gather*}
pvalue=2\cdot P( t_{n-p} \geqslant t_{exp|H_0} ) \\ \\
Reject \ H_0  \ \Leftrightarrow \ pvalue &lt; \alpha
\end{gather*}\]</span></p>
<ul>
<li><p>Case   <span class="math inline">\(H_0: \beta_j = \beta_j^*\)</span>  vs  <span class="math inline">\(H_1: \beta_j &lt; \beta_j^*\)</span></p></li>
<li><ul>
<li>Based in the test statistic:</li>
</ul></li>
</ul>
<p><span class="math display">\[\begin{gather*}
Reject \ H_0 \ \Leftrightarrow \ t_{exp|H_0} &lt; t_{1-\alpha/2}^{n-p}
\end{gather*}\]</span></p>
<ul>
<li><ul>
<li>Based in p-value:</li>
</ul></li>
</ul>
<p><span class="math display">\[\begin{gather*}
pvalue=2\cdot P( t_{n-p} \leqslant t_{exp|H_0} ) \\ \\
Reject \ H_0  \ \Leftrightarrow \ pvalue &lt; \alpha
\end{gather*}\]</span></p>
<hr />
</div>
</div>
<div id="test-of-significance-for-beta_j" class="section level3" number="0.5.4">
<h3><span class="header-section-number">0.5.4</span> Test of Significance for <span class="math inline">\(\beta_j\)</span> <a class="anchor" id="48"></a></h3>
<p>The test of significance for the coefficient <span class="math inline">\(\beta_j\)</span> is the following
test:</p>
<p><span class="math display">\[\begin{gather*}
H_0: \beta_j=0 \\
H_1: \beta_j \neq 0
\end{gather*}\]</span></p>
<p>The test statistic is the previously exposed, taking into account that
now <span class="math inline">\(\beta_j^*=0\)</span></p>
<p><span class="math display">\[\begin{gather*}
t_{exp | H_0}=\dfrac{\hat{\beta}_j - 0}{\sqrt{S_R \cdot q_{jj}}} = \dfrac{\hat{\beta}_j}{\sqrt{\widehat{Var}(\widehat{\beta}_j) }} \sim t_{n-p}
\end{gather*}\]</span></p>
<p>The decision rules are the same too</p>
<p><span class="math display">\[\begin{gather*}
Reject \ H_0  \ \Leftrightarrow \ pvalue &lt; \alpha
\end{gather*}\]</span></p>
<hr />
<div id="test-of-significance-in-r" class="section level4" number="0.5.4.1">
<h4><span class="header-section-number">0.5.4.1</span> Test of Significance in <code>R</code> <a class="anchor" id="49"></a></h4>
<p>The value of <span class="math inline">\(\hspace{0.1cm} t_{exp|H_0} \hspace{0.1cm}\)</span> and also the p-value of the test of significance for <span class="math inline">\(\beta_j\)</span> could be found in the output obtained with <code>summary(model_R_1)</code></p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model_R)</span></code></pre></div>
<hr />
</div>
<div id="test-of-significance-in-python" class="section level4" number="0.5.4.2">
<h4><span class="header-section-number">0.5.4.2</span> Test of Significance in <code>Python</code> <a class="anchor" id="50"></a></h4>
<p>The value of <span class="math inline">\(\hspace{0.1cm} t_{exp|H_0} \hspace{0.1cm}\)</span> and also the p-value of the test of significance for <span class="math inline">\(\beta_j\)</span> could be found in the output obtained with <code>print(model_Python_1.summary())</code></p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_Py_sm.summary())</span></code></pre></div>
<p>The p-values we have got are the following:</p>
<p><span class="math display">\[\begin{gather*}
H_0: \beta_{quality1}=0 \\
H_1: \beta_{quality1} \neq 0 \\ \\
pvalue = 0.094   
\end{gather*}\]</span></p>
<p>For <span class="math inline">\(\hspace{0.05cm} \alpha = 0.05 &lt; 0.094\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(Not \hspace{0.08cm} Reject \hspace{0.08cm} H_0 : \hspace{0.05cm} \beta_{quality1}=0 \hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(quality1 \hspace{0.1cm}\)</span> isn´t a significance variable</p>
<p><span class="math display">\[\begin{gather*}
H_0: \beta_{quality2}=0 \\
H_1: \beta_{quality2} \neq 0 \\ \\
pvalue = 0.028    
\end{gather*}\]</span></p>
<p>For <span class="math inline">\(\hspace{0.05cm} \alpha = 0.05 &gt; 0.028\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(Reject \hspace{0.08cm} H_0 : \hspace{0.05cm} \beta_{quality2}=0 \hspace{0.1cm} \Rightarrow \hspace{0.1cm} Accept \hspace{0.08cm} H_1 : \hspace{0.05cm} \beta_{quality2}\neq 0\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(quality2 \hspace{0.1cm}\)</span> is a significance variable</p>
<p><span class="math display">\[\begin{gather*}
H_0: \beta_{quality3}=0 \\
H_1: \beta_{quality3} \neq 0 \\ \\
pvalue = 0.159
\end{gather*}\]</span></p>
<p>For <span class="math inline">\(\hspace{0.05cm} \alpha = 0.05 &lt; 0.159\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(Not Reject \hspace{0.08cm} H_0 : \hspace{0.05cm} \beta_{quality3}=0 \hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(quality3 \hspace{0.1cm}\)</span> isn´t a significance variable</p>
<p><span class="math display">\[\begin{gather*}
H_0: \beta_{size\_in\_m\_2}=0 \\
H_1: \beta_{size\_in\_m\_2} \neq 0 \\ \\
pvalue \simeq 0
\end{gather*}\]</span></p>
<p>For <span class="math inline">\(\hspace{0.05cm} \alpha = 0.05 &lt; 0\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(Reject \hspace{0.08cm} H_0 : \hspace{0.05cm} \beta_{size\_in\_m\_2}=0 \hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(size\_in\_m\_2 \hspace{0.1cm}\)</span> is a significance variable</p>
<p><span class="math display">\[\begin{gather*}
H_0: \beta_{no\_of\_bedrooms}=0 \\
H_1: \beta_{no\_of\_bedrooms} \neq 0 \\ \\
pvalue \simeq 0
\end{gather*}\]</span></p>
<p>For <span class="math inline">\(\hspace{0.05cm} \alpha = 0.05 &lt; 0\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(Reject \hspace{0.08cm} H_0 : \hspace{0.05cm} \beta_{no\_of\_bedrooms}=0 \hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(no\_of\_bedrooms \hspace{0.1cm}\)</span> is a significance variable</p>
<p><span class="math display">\[\begin{gather*}
H_0: \beta_{no\_of\_bathrooms}=0 \\
H_1: \beta_{no\_of\_bathrooms} \neq 0 \\ \\
pvalue =  0.403
\end{gather*}\]</span></p>
<p>For <span class="math inline">\(\hspace{0.05cm} \alpha = 0.05 &lt; 0.403\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(Not Reject \hspace{0.08cm} H_0 : \hspace{0.05cm} \beta_{no\_of\_bathrooms}=0 \hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(no\_of\_bathrooms \hspace{0.1cm}\)</span> isn´t a significance variable</p>
<p><span class="math display">\[\begin{gather*}
H_0: \beta_{latitude}=0 \\
H_1: \beta_{latitude} \neq 0 \\ \\
pvalue \simeq 0
\end{gather*}\]</span></p>
<p>For <span class="math inline">\(\hspace{0.05cm} \alpha = 0.05 &gt; 0\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(Reject \hspace{0.08cm} H_0 : \hspace{0.05cm} \beta_{latitude}=0 \hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(latitude \hspace{0.1cm}\)</span> is a significance variable</p>
<p><span class="math display">\[\begin{gather*}
H_0: \beta_{longitude}=0 \\
H_1: \beta_{longitude} \neq 0 \\ \\
pvalue = 0.015
\end{gather*}\]</span></p>
<p>For <span class="math inline">\(\hspace{0.05cm} \alpha = 0.05 &gt; 0.015\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(Reject \hspace{0.08cm} H_0 : \hspace{0.05cm} \beta_{longitude}=0 \hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <span class="math inline">\(longitude \hspace{0.1cm}\)</span> is a significance variable</p>
<hr />
</div>
</div>
<div id="anova-test" class="section level3" number="0.5.5">
<h3><span class="header-section-number">0.5.5</span> ANOVA Test <a class="anchor" id="51"></a></h3>
<p>The ANOVA test is also called <strong>test of model global significance</strong> :</p>
<p><span class="math display">\[\begin{gather*}
\hspace{-0.7 cm} H_0: \hspace{0.15cm} \beta_1=\dots =\beta_p=0 \\
H_1: \hspace{0.15cm} \exists \ j=1,...,p , \hspace{0.2cm} \beta_j \neq 0
\end{gather*}\]</span></p>
<div id="statistic-test" class="section level4" number="0.5.5.1">
<h4><span class="header-section-number">0.5.5.1</span> Statistic test</h4>
<p>For define the test statistic , firt we have to define some elements:</p>
<ul>
<li>Total Sum Squares <span class="math inline">\((TSS)\)</span></li>
</ul>
<p><span class="math display">\[\begin{gather*}
TSS =  \sum_{i=1}^n ( y_i - \overline{y})\hspace{0.02cm}^2
\end{gather*}\]</span></p>
<ul>
<li>Residual Sum Squares <span class="math inline">\((RSS)\)</span></li>
</ul>
<p><span class="math display">\[\begin{gather*}
RSS=  \sum_{i=1}^n \widehat{\varepsilon}_i\hspace{0.01cm}^2 =  \sum_{i=1}^n ( y_i - \hat{y}_i)\hspace{0.02cm}^2
\end{gather*}\]</span></p>
<ul>
<li>Regression Sum Squares <span class="math inline">\((RegSS)\)</span></li>
</ul>
<p><span class="math display">\[\begin{gather*}
RegSS =  \sum_{i=1}^n ( \hat{y}_i - \overline{y} )\hspace{0.02cm}^2
\end{gather*}\]</span></p>
<p>It can be proved that:</p>
<p><span class="math display">\[\begin{gather*}
TSS=RSS+RegSS
\end{gather*}\]</span></p>
<p><br></p>
<ul>
<li><p><span class="math inline">\(TSS \hspace{0.1cm}\)</span> is the total variance of the response variable <span class="math inline">\(Y\)</span></p></li>
<li><p><span class="math inline">\(RegSS \hspace{0.1cm}\)</span> is the variance of the response variable <span class="math inline">\(Y\)</span> <strong>explained</strong> by the
model using <span class="math inline">\(X\)</span></p></li>
<li><p><span class="math inline">\(RSS \hspace{0.1cm}\)</span> is the variance of the response variable <span class="math inline">\(Y\)</span> <strong>not explained</strong> by the
model using <span class="math inline">\(X\)</span></p></li>
</ul>
<p>Now we can define the test statistic as:</p>
<p><span class="math display">\[\begin{gather*}
F_{exp|H_0}= \dfrac{(TSS-RSS)/p}{RSS/(n-p-1)} = \dfrac{(TSS-RSS)/p}{ \hat{\varepsilon}_i\hspace{0.02cm}^2  } \sim F_{\hspace{0.05cm}p,\hspace{0.05cm} n-p-1}
\end{gather*}\]</span></p>
<p><br></p>
<p>Where:  <span class="math inline">\(\hspace{0.1cm} p\)</span> is the number of predictor variables <span class="math inline">\(X_1,...,X_p\)</span> of the
model</p>
<p> </p>
</div>
<div id="decision-rule-1" class="section level4" number="0.5.5.2">
<h4><span class="header-section-number">0.5.5.2</span> Decision Rule</h4>
<ul>
<li>Based on statistic test:</li>
</ul>
<p><span class="math display">\[\begin{gather*}
Reject \ H_0 \ \Leftrightarrow \ F_{exp|H_0} &gt; F_{\alpha}^{\hspace{0.1cm}p,\hspace{0.05cm} n-p-1}
\end{gather*}\]</span></p>
<ul>
<li>Based on p-value:</li>
</ul>
<p><span class="math display">\[\begin{gather*}
Reject \ H_0  \ \Leftrightarrow \ pvalue &lt; \alpha
\end{gather*}\]</span></p>
<hr />
</div>
<div id="anova-test-in-r" class="section level4" number="0.5.5.3">
<h4><span class="header-section-number">0.5.5.3</span> ANOVA test in <code>R</code> <a class="anchor" id="52"></a></h4>
<p>The value of <span class="math inline">\(\hspace{0.1cm} F_{exp|H_0}\)</span> and also the p-value of the ANOVA test could be found in the output obtained with <code>summary(model_R_1)</code></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model_R)</span></code></pre></div>
<hr />
</div>
<div id="anova-test-in-python" class="section level4" number="0.5.5.4">
<h4><span class="header-section-number">0.5.5.4</span> ANOVA test in <code>Python</code> <a class="anchor" id="53"></a></h4>
<p>The value of <span class="math inline">\(\hspace{0.1cm} F_{exp|H_0}\)</span> and also the p-value of the ANOVA test could be found in the output obtained with <code>print(model_Python_1.summary())</code></p>
<div class="sourceCode" id="cb90"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_Py_sm.summary())</span></code></pre></div>
<p>We have got the following:</p>
<p><span class="math inline">\(\hspace{4cm} F_{exp|H_0} = 547.4\)</span> <span class="math inline">\(\hspace{0.3cm}\)</span> (F-statistic)</p>
<p><span class="math inline">\(\hspace{4cm} pvalue \simeq 0\)</span> <span class="math inline">\(\hspace{0.3cm}\)</span> (Prob (F-statistic))</p>
<p>So for any <span class="math inline">\(\alpha\)</span> we can reject <span class="math inline">\(\hspace{0.1cm} H_0: \hspace{0.05cm} \beta_1=\dots =\beta_p=0 \hspace{0.15cm}\)</span>, so we can affirm the model is globally significant</p>
<hr />
</div>
</div>
<div id="prediction-interval-for-hspace0.1cm-y_i" class="section level3" number="0.5.6">
<h3><span class="header-section-number">0.5.6</span> Prediction Interval for <span class="math inline">\(\hspace{0.1cm} y_i\)</span> <a class="anchor" id="53.1"></a></h3>
<p>We have that</p>
<p><span class="math display">\[\hat{y}_i = x_i^t \cdot \hat{\beta} \hspace{0.1cm} \sim \hspace{0.1cm} N \left( \hspace{0.1cm} E[\hat{y}_i] \hspace{0.15cm},\hspace{0.15cm} \sigma^2 \cdot (1 + x_i^t \cdot (X^t \cdot X)^{-1} \cdot x_i ) \hspace{0.1cm} \right)\]</span></p>
<p>Using that we can get the following probability interval for <span class="math inline">\(y_i\)</span></p>
<p><span class="math display">\[ IP(y_i)_{1-\alpha} = \left[\hspace{0.1cm} \hat{y}_i  \hspace{0.1cm}\pm\hspace{0.1cm} t_{\alpha/2}^{n-p-1} \sqrt{ \widehat{\sigma}^2 \cdot \left( 1 + x_i^t \cdot (X^t \cdot X)^{-1} \cdot x_i \right) } \hspace{0.1cm} \right] \]</span></p>
<hr />
</div>
<div id="prediction-interval-for-hspace0.1cm-y_i-in-python" class="section level3" number="0.5.7">
<h3><span class="header-section-number">0.5.7</span> Prediction Interval for <span class="math inline">\(\hspace{0.1cm} y_i\)</span> in <code>Python</code> <a class="anchor" id="53.2"></a></h3>
<div class="sourceCode" id="cb91"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> data_Python[[<span class="st">&#39;price&#39;</span>]]</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data_Python[[<span class="st">&#39;size_in_m_2&#39;</span>, <span class="st">&#39;longitude&#39;</span>, <span class="st">&#39;latitude&#39;</span>, <span class="st">&#39;no_of_bedrooms&#39;</span>, <span class="st">&#39;no_of_bathrooms&#39;</span>, <span class="st">&#39;quality&#39;</span>]]</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> varcharProcessing(X, varchar_process <span class="op">=</span> <span class="st">&quot;dummy_dropfirst&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb92"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>Probability_Intervals_Response <span class="op">=</span> pd.DataFrame({ <span class="st">&#39;y&#39;</span>: <span class="dv">0</span> , <span class="st">&#39;y_predict&#39;</span>: <span class="dv">0</span> ,  <span class="st">&#39;Prob_Interval_lower&#39;</span>: <span class="dv">0</span>, <span class="st">&#39;Prob_Interval_upper&#39;</span>: <span class="dv">0</span>}, index<span class="op">=</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(data_Python)))</span></code></pre></div>
<div class="sourceCode" id="cb93"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>Probability_Intervals_Response.head()</span></code></pre></div>
<div class="sourceCode" id="cb94"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="bu">len</span>(data_Python)</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>p<span class="op">=</span>model_Py_smf.df_model</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> scipy.stats.t.ppf(<span class="fl">0.95</span>, n<span class="op">-</span>p<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>XtX_inv <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X)</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>Probability_Intervals_Response.iloc[:, <span class="dv">0</span>] <span class="op">=</span> Y</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>Probability_Intervals_Response.iloc[:, <span class="dv">1</span>] <span class="op">=</span> predictions</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(data_Python)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>    x_i <span class="op">=</span> X.to_numpy()[i, ]</span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>    x_i <span class="op">=</span> np.array([x_i]) <span class="co"># necessary step to transpose a 1D array</span></span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>    hat_y_i <span class="op">=</span> predictions.to_numpy()[i, ]</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> hat_y_i <span class="op">-</span> np.sqrt( estimated_variance_error <span class="op">*</span> ( <span class="dv">1</span> <span class="op">+</span> x_i <span class="op">@</span> XtX_inv <span class="op">@</span> x_i.T ) )</span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> hat_y_i <span class="op">+</span> np.sqrt( estimated_variance_error <span class="op">*</span> ( <span class="dv">1</span> <span class="op">+</span> x_i <span class="op">@</span> XtX_inv <span class="op">@</span> x_i.T ) )</span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> <span class="bu">float</span>(a)</span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> <span class="bu">float</span>(b)</span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a>    Probability_Intervals_Response.iloc[i, <span class="dv">2</span>] <span class="op">=</span> a</span>
<span id="cb94-27"><a href="#cb94-27" aria-hidden="true" tabindex="-1"></a>    Probability_Intervals_Response.iloc[i, <span class="dv">3</span>] <span class="op">=</span> b</span></code></pre></div>
<div class="sourceCode" id="cb95"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a> Probability_Intervals_Response</span></code></pre></div>
<hr />
</div>
</div>
<div id="goodness-of-fit-hspace0.1cm-determination-coefficient-r2" class="section level2" number="0.6">
<h2><span class="header-section-number">0.6</span> Goodness of Fit: <span class="math inline">\(\hspace{0.1cm}\)</span> Determination Coefficient <span class="math inline">\((R^2)\)</span> <a class="anchor" id="54"></a></h2>
<p>The determination coefficient, also called R-square, is defined as:</p>
<p><span class="math display">\[\begin{gather*}
R^2 = \dfrac{RegSS}{TSS} = \dfrac{TSS-RSS}{TSS} =1 - \dfrac{RSS}{TSS}
\end{gather*}\]</span></p>
<p><strong>Properties</strong></p>
<ul>
<li><p>$ R^2$ is the proportion of total variance of the response
variable <span class="math inline">\(Y\)</span> that is explained by the model using <span class="math inline">\(X\)</span></p></li>
<li><p><span class="math inline">\(R^2 \in \left[ 0 , 1 \right]\)</span></p></li>
</ul>
<p>For this reason <span class="math inline">\(R^2\)</span> is used as a measure of how well the model fits the
response variable .</p>
<p><strong>Interpretation</strong></p>
<p>The interpretation of <span class="math inline">\(R^2\)</span> is the following:</p>
<ul>
<li><p>If <span class="math inline">\(R^2\)</span> is close to 1, indicates good fit of model to the response
variable data</p></li>
<li><p>If <span class="math inline">\(R^2\)</span> is clode to 0, indicates bad fit of model to the response
variable data</p></li>
</ul>
<hr />
<div id="compute-r2-in-r" class="section level3" number="0.6.1">
<h3><span class="header-section-number">0.6.1</span> Compute <span class="math inline">\(R^2\)</span> in <code>R</code> <a class="anchor" id="55"></a></h3>
<p>The value of <span class="math inline">\(\hspace{0.1cm} R^2\)</span> could be found in the output obtained with <code>summary(model_R)</code></p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model_R)<span class="sc">$</span>r.squared </span></code></pre></div>
<hr />
</div>
<div id="compute-r2-in-python" class="section level3" number="0.6.2">
<h3><span class="header-section-number">0.6.2</span> Compute <span class="math inline">\(R^2\)</span> in <code>Python</code> <a class="anchor" id="56"></a></h3>
<p>The value of <span class="math inline">\(\hspace{0.1cm} R^2\)</span> could be found in the output obtained with <code>print(model_Py_smf.summary())</code></p>
<div class="sourceCode" id="cb97"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>model_Py_smf.rsquared</span></code></pre></div>
<p>We can compute <span class="math inline">\(R^2\)</span> with <code>sk-learn</code> as follows:</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>Model_Py_sklearn.score(X, y)</span></code></pre></div>
<p>We have got the following:</p>
<p><span class="math display">\[ R^2 = 0.698\]</span></p>
<hr />
</div>
</div>
<div id="goodness-of-fit-hspace0.1cm-adjusted-r2" class="section level2" number="0.7">
<h2><span class="header-section-number">0.7</span> Goodness of Fit: <span class="math inline">\(\hspace{0.1cm}\)</span> Adjusted <span class="math inline">\(R^2\)</span> <a class="anchor" id="57"></a></h2>
<p><span class="math inline">\(R^2\)</span> has several problems.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(R^2\)</span> always increase when increase the number of predictors,
although they are not significative.</p></li>
<li><p>It´s possible estimate two models with the same prediction power but
with different <span class="math inline">\(R^2\)</span></p></li>
</ol>
<p>For avoid the disadvantages of <span class="math inline">\(R^2\)</span> was created the adjusted <span class="math inline">\(R^2\)</span> ,
denoted as <span class="math inline">\(\widehat{R^2}\)</span>, and defined as:</p>
<p><span class="math display">\[\begin{gather*}
\widehat{R}^2 =  1 - \dfrac{RSS/(n-p-1)}{TSS/(n-1)} = 1 - \left( 1- R^2 \right) \cdot \dfrac{n-1}{n-p}
\end{gather*}\]</span></p>
<p>This metric doesn’t grow when including irrelevant predictors since if <span class="math inline">\(RSS\)</span> is small as <span class="math inline">\(p\)</span> is large, <span class="math inline">\(1/(n-p-1)\)</span> will be large compensating the <span class="math inline">\(RSS\)</span> value</p>
<hr />
<div id="compute-widehatr2-in-r" class="section level3" number="0.7.1">
<h3><span class="header-section-number">0.7.1</span> Compute <span class="math inline">\(\widehat{R^2}\)</span> in <code>R</code> <a class="anchor" id="58"></a></h3>
<p>The value of <span class="math inline">\(\hspace{0.1cm} \widehat{R^2}\)</span> could be found in the output obtained with <code>summary(model_R)</code></p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model_R)<span class="sc">$</span>adj.r.squared </span></code></pre></div>
<hr />
</div>
<div id="compute-widehatr2-in-python" class="section level3" number="0.7.2">
<h3><span class="header-section-number">0.7.2</span> Compute <span class="math inline">\(\widehat{R^2}\)</span> in <code>Python</code> <a class="anchor" id="59"></a></h3>
<p>The value of <span class="math inline">\(\hspace{0.1cm} \widehat{R^2}\)</span> could be found in the output obtained with <code>print(model_Py_smf.summary())</code></p>
<div class="sourceCode" id="cb100"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>model_Py_smf.rsquared_adj</span></code></pre></div>
<p>We have got the following:</p>
<p><span class="math display">\[ \widehat{R^2} = 0.697\]</span></p>
<hr />
</div>
</div>
<div id="model-problems" class="section level2" number="0.8">
<h2><span class="header-section-number">0.8</span> Model Problems <a class="anchor" id="60"></a></h2>
</div>
<div id="multicollinearity" class="section level2" number="0.9">
<h2><span class="header-section-number">0.9</span> Multicollinearity <a class="anchor" id="61"></a></h2>
<p>Multicollinearity problem happens when some of the predictors are lineary dependients.</p>
<p>Why multicollinearity is a problem ?</p>
<p><span class="math inline">\(1.\hspace{0.1cm}\)</span> In the worst case it makes the estimation of the model impossible.</p>
<p><span class="math inline">\(2.\hspace{0.1cm}\)</span> At best, it causes the beta estimators to have a lot of variance, which makes the model’s predictions and inference very imprecise. And it also causes predictors that are relevant to come out as non-significant in the significance test.</p>
<div id="types-of-multicollinearity" class="section level3" number="0.9.1">
<h3><span class="header-section-number">0.9.1</span> Types of multicollinearity <a class="anchor" id="62"></a></h3>
<div id="perfect-multicollinearity" class="section level4" number="0.9.1.1">
<h4><span class="header-section-number">0.9.1.1</span> Perfect Multicollinearity</h4>
<p>At least one of the predictors is a linear combination of the rest of the predictors, so <span class="math inline">\(\hspace{0.1cm}Rg(X) &lt; p+1 \hspace{0.1cm}\)</span> (not full range)</p>
<p>By the null rank theorem, <span class="math inline">\(\hspace{0.1cm}R(X^t \cdot X) &lt; p+1\hspace{0.1cm}\)</span> , therefore there is no <span class="math inline">\(\hspace{0.1cm}(X^t \cdot X)^{-1}\hspace{0.1cm}\)</span> , so it cannot be estimate <span class="math inline">\(\hspace{0.1cm}\beta\hspace{0.1cm}\)</span> with the ordinary least squares method .</p>
</div>
<div id="high-multicollinearity" class="section level4" number="0.9.1.2">
<h4><span class="header-section-number">0.9.1.2</span> High Multicollinearity</h4>
<p>There are predictors with high linear correlation between them.</p>
<p>In this case is still possible estimate <span class="math inline">\(\beta\)</span> because the multicolinearity isn´t perfect, but the variance of the <span class="math inline">\(\beta\)</span> estimators, <span class="math inline">\(Var(\hat{\beta}_j)\)</span>, will be too high, therefore the model will be very imprecise, so the results will not be good.</p>
<hr />
</div>
</div>
<div id="identification-of-multicollinearity" class="section level3" number="0.9.2">
<h3><span class="header-section-number">0.9.2</span> Identification of multicollinearity <a class="anchor" id="63"></a></h3>
<p>Identification of multicollinearity is carry out by several ways:</p>
<p>If the linear regression model has only <strong>quantitative</strong> predictors:</p>
<ol style="list-style-type: decimal">
<li>With the correlation matrix of the predictors <span class="math inline">\(\hspace{0.05cm} R\)</span></li>
<li>With <span class="math inline">\(VIF\)</span> (variance increase factor )</li>
<li>With condition number of <span class="math inline">\(\hspace{0.05cm} R\)</span></li>
</ol>
<p>If the linear model has <strong>quantitative and categorical</strong> predictors:</p>
<ol style="list-style-type: decimal">
<li>With <span class="math inline">\(GVIF\)</span> (generalize variance increase factor)</li>
</ol>
<hr />
</div>
<div id="identification-of-multicollinearity-with-r" class="section level3" number="0.9.3">
<h3><span class="header-section-number">0.9.3</span> Identification of multicollinearity with <span class="math inline">\(R\)</span> <a class="anchor" id="64"></a></h3>
<p>The Pearson correlation matrix of a given data matrix <span class="math inline">\(X=[X_1 ,..., X_p]\)</span> is defined as:</p>
<p><br></p>
<p><span class="math display">\[
R= \begin{pmatrix}
    r_{11} &amp; r_{12}&amp;...&amp;r_{1p}\\
    r_{21} &amp; r_{22}&amp;...&amp;r_{2p}\\
    &amp;...&amp;\\
    r_{p1}&amp; r_{p2}&amp;...&amp;r_{pp}
\end{pmatrix} = [r_{ij} ]\hspace{0.05cm}_{i,j=1,..,p}
\]</span></p>
<p>Where: <span class="math inline">\(\hspace{0.2cm} r_{i j} \hspace{0.1cm}\)</span> is the Pearson linear correlation between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span></p>
<p><strong>Criteria :</strong> Identification of multicollinearity with <span class="math inline">\(R\)</span></p>
<p>If we have the following estimated linear regression model <span class="math inline">\(\hspace{0.1cm} Y=X\cdot \widehat{\beta} \hspace{0.1cm}\)</span> where <span class="math inline">\(\hspace{0.1cm}X=(1, X_1, ..., X_p)\hspace{0.1cm}\)</span> are all <strong>quantitative</strong> predictors.</p>
<p>We compute the Pearson linear correlation matrix of the predictors <span class="math inline">\(\hspace{0.1cm} [ X_1 ,..., X_p] \hspace{0.2cm} \Rightarrow \hspace{0.2cm} R = [r_{ij} ]\hspace{0.05cm}_{i,j=1,..,p}\)</span></p>
<p>If there is any <strong>high</strong> <span class="math inline">\(\hspace{0.1cm} r_{ij}\hspace{0.1cm} (\hspace{0.1cm} &gt; 0.75 \hspace{0.1cm})\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow\hspace{0.1cm}\)</span> possible multicollinearity problem</p>
<hr />
<div id="identification-of-multicollinearity-with-r-in-r" class="section level4" number="0.9.3.1">
<h4><span class="header-section-number">0.9.3.1</span> Identification of multicollinearity with <span class="math inline">\(R\)</span> in <code>R</code></h4>
<p>We will suposse our linear regression model only has <strong>quatitative</strong> predictors.</p>
<p>We need to compute the Pearson correlation matrix for these quantitative predictors.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">cor</span>(data_R <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>price, <span class="sc">-</span>quality))</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>R</span></code></pre></div>
<p>Now we will compute a heatmap with the correlation matrix:</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggcorr</span>(data_R <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>price, <span class="sc">-</span>quality), <span class="at">label=</span><span class="cn">TRUE</span>, <span class="at">digits=</span><span class="dv">3</span>)</span></code></pre></div>
<hr />
</div>
<div id="identification-of-multicollinearity-with-r-in-python" class="section level4" number="0.9.3.2">
<h4><span class="header-section-number">0.9.3.2</span> Identification of multicollinearity with <span class="math inline">\(R\)</span> in <code>Python</code></h4>
<div class="sourceCode" id="cb103"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data_Python.loc[: , [<span class="st">&#39;size_in_m_2&#39;</span>, <span class="st">&#39;longitude&#39;</span>, <span class="st">&#39;latitude&#39;</span>, <span class="st">&#39;no_of_bedrooms&#39;</span>, <span class="st">&#39;no_of_bathrooms&#39;</span>]]</span></code></pre></div>
<div class="sourceCode" id="cb104"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> X.corr()</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>R</span></code></pre></div>
<div class="sourceCode" id="cb105"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install seaborn</span></span></code></pre></div>
<div class="sourceCode" id="cb106"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sb</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>corr <span class="op">=</span> R</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>sb.heatmap(corr, cmap<span class="op">=</span><span class="st">&quot;Blues&quot;</span>, annot<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>We can see there are several pairs of predictor with high correlation, such as <strong>(no_of_bathrooms , no_of_bedrooms )</strong> , <strong>(size_in_m_2 , no_of_bedrooms)</strong> , <strong>(size_in_m_2 , no_of_bathrooms)</strong></p>
<p>So following this criteria the linear regression model <strong>price ~ size_in_m_2 + longitude + latitude + no_of_bedrooms + no_of_bathrooms</strong> could has multicollinearity problems.</p>
<hr />
</div>
</div>
<div id="identification-of-multicollinearity-with-vif" class="section level3" number="0.9.4">
<h3><span class="header-section-number">0.9.4</span> Identification of multicollinearity with <span class="math inline">\(VIF\)</span> <a class="anchor" id="64"></a></h3>
<p>If we have the following estimated linear regression model <span class="math inline">\(\hspace{0.1cm} Y=X\cdot \widehat{\beta} \hspace{0.1cm}\)</span> where <span class="math inline">\(\hspace{0.1cm}X=(1, X_1, ..., X_p)\hspace{0.1cm}\)</span> are all <strong>quantitative</strong> predictors.</p>
<p>We compute the <strong>inverse</strong> of the Pearson correlation matrix of the predictors $ [ X_1 ,…, X_p] R^{-1} $</p>
<p>The variance increase factor <span class="math inline">\(\left(VIF\right)\)</span> of the quantitative predictor <span class="math inline">\(X_j\)</span> is:</p>
<p><span class="math display">\[VIF(X_j) \hspace{0.05cm}=\hspace{0.05cm} Diag(R\hspace{0.07cm}^{-1})[\hspace{0.1cm} j \hspace{0.1cm}] \hspace{0.05cm}=\hspace{0.05cm} j \text{ element of} \hspace{0.1cm} Diag(R\hspace{0.07cm}^{-1})\]</span></p>
<p>And it´s fullfield that:</p>
<p><span class="math display">\[ VIF(X_j) = \dfrac{1}{1- R^2_{X_j}} \]</span></p>
<p>Where:</p>
<p>$R^2_{X_j} $ is the determination coefficient <span class="math inline">\(\hspace{0.07cm} (R^2)\hspace{0.07cm}\)</span> of the linear regression model <span class="math inline">\(\hspace{0.1cm} X_i = \beta_0 + \beta_1 \cdot X_1 +...+ \beta_{j-1}\cdot X_{j-1} + \beta_{i+1}\cdot X_{i+1} + ... + \beta_p \cdot X_p\)</span></p>
<p>So, <span class="math inline">\(\hspace{0.07cm} R^2_{X_i} \hspace{0.07cm}\)</span> is the proportion of variability of <span class="math inline">\(\hspace{0.07cm}X_j\hspace{0.07cm}\)</span> explained by <span class="math inline">\(\hspace{0.07cm}X_1,..,X_{j-1},X_{j+1},..,X_p\)</span></p>
<p><strong>Criteria:</strong> Identification of multicollinearity with <span class="math inline">\(VIF\)</span></p>
<p>If <span class="math inline">\(\hspace{0.15cm}VIF(X_j) &gt; 10\)</span> <span class="math inline">\(\hspace{0.15cm}\left( R^2_{X_j} &gt; 0.90 \right)\)</span> <span class="math inline">\(\hspace{0.15cm}\Rightarrow\hspace{0.15cm}\)</span> <strong>High</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.15cm}VIF(X_j) \in [4, 10]\)</span> <span class="math inline">\(\hspace{0.15cm}\left(R^2_{X_j} \in [0.75 , 0.90]\right)\)</span> <span class="math inline">\(\hspace{0.15cm}\Rightarrow\hspace{0.15cm}\)</span> <strong>Medium</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.15cm}VIF(X_j) &lt; 4\)</span> <span class="math inline">\(\hspace{0.15cm}\left(R^2_{X_j} &lt; 0.75 \right)\)</span> <span class="math inline">\(\hspace{0.15cm}\Rightarrow\hspace{0.15cm}\)</span> <strong>Low</strong> multicollinearity</p>
<hr />
<div id="identification-of-multicollinearity-with-vif-in-r" class="section level4" number="0.9.4.1">
<h4><span class="header-section-number">0.9.4.1</span> Identification of multicollinearity with <span class="math inline">\(VIF\)</span> in <code>R</code></h4>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>R_inverse <span class="ot">&lt;-</span> <span class="fu">solve</span>(R)</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(R_inverse)</span></code></pre></div>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R </span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(carData)</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(<span class="fu">lm</span>(price <span class="sc">~</span> size_in_m_2  <span class="sc">+</span> longitude <span class="sc">+</span> latitude <span class="sc">+</span> no_of_bedrooms <span class="sc">+</span> no_of_bathrooms , <span class="at">data=</span>data_R))</span></code></pre></div>
<hr />
</div>
<div id="identification-of-multicollinearity-with-vif-in-python" class="section level4" number="0.9.4.2">
<h4><span class="header-section-number">0.9.4.2</span> Identification of multicollinearity with <span class="math inline">\(VIF\)</span> in <code>Python</code></h4>
<div class="sourceCode" id="cb109"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data_Python.loc[: , [<span class="st">&#39;size_in_m_2&#39;</span>, <span class="st">&#39;longitude&#39;</span>, <span class="st">&#39;latitude&#39;</span>, <span class="st">&#39;no_of_bedrooms&#39;</span>, <span class="st">&#39;no_of_bathrooms&#39;</span>]]</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> X.corr()</span></code></pre></div>
<div class="sourceCode" id="cb110"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>R_inv_diag <span class="op">=</span> np.diagonal( np.linalg.inv( R ) )</span></code></pre></div>
<div class="sourceCode" id="cb111"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>predictors_names <span class="op">=</span> X.columns</span></code></pre></div>
<div class="sourceCode" id="cb112"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>VIF_Python <span class="op">=</span> pd.DataFrame({<span class="st">&#39;predictor&#39;</span>: predictors_names , <span class="st">&#39;VIF&#39;</span>: R_inv_diag })</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>VIF_Python <span class="op">=</span> VIF_Python.set_index(<span class="st">&#39;predictor&#39;</span>)</span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>VIF_Python</span></code></pre></div>
<p>There is a quantitative predictor with <span class="math inline">\(\hspace{0.1cm} VIF &gt; 4 \hspace{0.1cm}\)</span> , but none with <span class="math inline">\(\hspace{0.1cm} VIF &gt; 10 \hspace{0.1cm}\)</span> , so following this criteria the linear regression model <span class="math inline">\(\hspace{0.1cm}\)</span> <strong>price ~ size_in_m_2 + longitude + latitude + no_of_bedrooms + no_of_bathrooms</strong> <span class="math inline">\(\hspace{0.1cm}\)</span> has <strong>medium</strong> multicollinearity .</p>
<hr />
</div>
</div>
<div id="identification-of-multicollinearity-with-condition-number-of-hspace0.05cm-r" class="section level3" number="0.9.5">
<h3><span class="header-section-number">0.9.5</span> Identification of multicollinearity with Condition Number of <span class="math inline">\(\hspace{0.05cm} R\)</span> <a class="anchor" id="66"></a></h3>
<p>If we have the following estimated linear regression model <span class="math inline">\(\hspace{0.1cm} Y=X\cdot \widehat{\beta} \hspace{0.1cm}\)</span> where <span class="math inline">\(\hspace{0.1cm}X=(1, X_1, ..., X_p)\hspace{0.1cm}\)</span> are all <strong>quantitative</strong> predictors.</p>
<p>We compute the Pearson linear correlation matrix of the predictors <span class="math inline">\(\hspace{0.1cm} [ X_1 ,..., X_p] \hspace{0.2cm} \Rightarrow \hspace{0.2cm} R = [r_{ij} ]\hspace{0.05cm}_{i,j=1,..,p}\)</span></p>
<p>The <strong>condition number</strong> of <span class="math inline">\(R\)</span> is defined as:</p>
<p><span class="math display">\[cond(R) = \sqrt{\dfrac{max\lbrace eigenvalues(R)\rbrace}{min\lbrace eigenvalues(R)\rbrace}} \]</span></p>
<p><strong>Criteria :</strong> Identification of multicollinearity with Condition Number of <span class="math inline">\(\hspace{0.05cm} R\)</span></p>
<p>If <span class="math inline">\(\hspace{0.1cm} cond(R) &gt; 30\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow\hspace{0.1cm}\)</span> <strong>High</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.1cm}cond(R) \in (10 , 30)\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow\hspace{0.1cm}\)</span> <strong>Medium</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.1cm}cond(R) &lt; 10\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow\hspace{0.1cm}\)</span> <strong>Low</strong> multicollinearity</p>
<hr />
<div id="identification-of-multicollinearity-with-condition-number-of-hspace0.05cm-r-in-r" class="section level5" number="0.9.5.0.1">
<h5><span class="header-section-number">0.9.5.0.1</span> Identification of multicollinearity with condition number of <span class="math inline">\(\hspace{0.05cm} R\)</span> in <code>R</code></h5>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>cond_R <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">max</span>(<span class="fu">eigen</span>(R)<span class="sc">$</span>values)<span class="sc">/</span><span class="fu">min</span>(<span class="fu">eigen</span>(R)<span class="sc">$</span>values))</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>cond_R</span></code></pre></div>
<hr />
</div>
<div id="identification-of-multicollinearity-with-condition-number-of-hspace0.05cm-r-in-python" class="section level5" number="0.9.5.0.2">
<h5><span class="header-section-number">0.9.5.0.2</span> Identification of multicollinearity with condition number of <span class="math inline">\(\hspace{0.05cm} R\)</span> in <code>Python</code></h5>
<div class="sourceCode" id="cb114"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data_Python.loc[: , [<span class="st">&#39;size_in_m_2&#39;</span>, <span class="st">&#39;longitude&#39;</span>, <span class="st">&#39;latitude&#39;</span>, <span class="st">&#39;no_of_bedrooms&#39;</span>, <span class="st">&#39;no_of_bathrooms&#39;</span>]]</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> X.corr()</span></code></pre></div>
<div class="sourceCode" id="cb115"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> linalg <span class="im">as</span> LA</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>eigenvalues , eigenvectors <span class="op">=</span> LA.eig(R)</span></code></pre></div>
<div class="sourceCode" id="cb116"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>cond_R <span class="op">=</span> math.sqrt(<span class="bu">max</span>(eigenvalues)<span class="op">/</span><span class="bu">min</span>(eigenvalues))</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>cond_R</span></code></pre></div>
<p>We have get $ cond(R) = 4.33 &lt; 10 $ , so following this criterion, the linear regression model <span class="math inline">\(\hspace{0.1cm}\)</span> <strong>price ~ size_in_m_2 + longitude + latitude + no_of_bedrooms + no_of_bathrooms</strong> <span class="math inline">\(\hspace{0.1cm}\)</span> has <strong>low</strong> multicollinearity</p>
<hr />
</div>
</div>
<div id="identification-of-multicollinearity-with-gvif" class="section level3" number="0.9.6">
<h3><span class="header-section-number">0.9.6</span> Identification of multicollinearity with <span class="math inline">\(GVIF\)</span> <a class="anchor" id="67"></a></h3>
<p><span class="math inline">\(VIF\)</span> is only a good multicollinearity measure when <strong>all</strong> the <strong>predictors</strong> of the model are <strong>quantitative</strong>.</p>
<p><span class="math inline">\(GVIF\)</span> is the measure proposed by Fox and Monette (1992) to deal with linear regression models that have <strong>categorical predictors</strong>.</p>
<p>Suppose we have the following estimated linear regression model <span class="math inline">\(\hspace{0.1cm} Y=X\cdot \widehat{\beta} \hspace{0.1cm}\)</span> where <span class="math inline">\(\hspace{0.1cm}X=(1, X_1, ..., X_p)\hspace{0.1cm}\)</span> are some <strong>quantitative</strong> and others <strong>categorical</strong> predictors.</p>
<hr />
<p>If <span class="math inline">\(X_j\)</span> is a <strong>quantitative</strong> predictor</p>
<p>We have to consider the following matrix:</p>
<ul>
<li><p><span class="math inline">\(\mathcal{X}_1 = [\hspace{0.1cm} X_j \hspace{0.1cm}]\)</span></p></li>
<li><p><span class="math inline">\(\mathcal{X}_2 = [\hspace{0.1cm} X_1 , .. , X_{j-1}, X_{j+1},.., X_p \hspace{0.1cm}]\)</span></p></li>
<li><p>$_3 = [ X_1,..,X_j,.., X_p ] $</p></li>
</ul>
<p>And the following Pearson correlation matrix:</p>
<ul>
<li><p><span class="math inline">\(R_1\)</span> is the Pearson correlation matrix of <span class="math inline">\(\mathcal{X}_1\)</span></p></li>
<li><p><span class="math inline">\(R_2\)</span> is the Pearson correlation matrix of <span class="math inline">\(\mathcal{X}_2\)</span></p></li>
<li><p><span class="math inline">\(R_3\)</span> is the Pearson correlation matrix of <span class="math inline">\(\mathcal{X}_3\)</span></p></li>
</ul>
<p>Then, <span class="math inline">\(\hspace{0.1cm} GVIF \hspace{0.1cm}\)</span> of <span class="math inline">\(\hspace{0.1cm} X_j \hspace{0.1cm}\)</span> is defined as:</p>
<p><span class="math display">\[ GVIF(X_j) = det(R_1) \cdot \dfrac{det(R_2)}{det(R_3)} \]</span></p>
<hr />
<p>If <span class="math inline">\(X_j\)</span> is a <strong>categorical</strong> predictor with <span class="math inline">\(r\)</span> categories, <span class="math inline">\(\hspace{0.1cm} Range(X_j) = \lbrace 0,1,..., r-1 \rbrace \hspace{0.1cm}\)</span> , that enter in the model with the dummy variables <span class="math inline">\(\hspace{0.1cm} X_{j1},X_{j2},...,X_{j(r-1)}\)</span></p>
<p>We have to consider the following matrix:</p>
<ul>
<li><p><span class="math inline">\(\mathcal{X}_1 = [\hspace{0.1cm} X_{j1},X_{j2},..,X_{j(r-1)} \hspace{0.1cm}]\)</span></p></li>
<li><p><span class="math inline">\(\mathcal{X}_2 = [\hspace{0.1cm} X_1 , .. , X_{j-1}, X_{j+1},.., X_p \hspace{0.1cm}]\)</span></p></li>
<li><p>$_3 = [ X_1,..,X_{j-1},X_{j1},X_{j2},..,X_{j(r-1)}, X_{j+1},.., X_p ] $</p></li>
</ul>
<p>And the following Pearson correlation matrix:</p>
<ul>
<li><p><span class="math inline">\(R_1\)</span> is the Pearson correlation matrix of <span class="math inline">\(\mathcal{X}_1\)</span></p></li>
<li><p><span class="math inline">\(R_2\)</span> is the Pearson correlation matrix of <span class="math inline">\(\mathcal{X}_2\)</span></p></li>
<li><p><span class="math inline">\(R_3\)</span> is the Pearson correlation matrix of <span class="math inline">\(\mathcal{X}_3\)</span></p></li>
</ul>
<p>Then, <span class="math inline">\(\hspace{0.05cm} GVIF \hspace{0.05cm}\)</span> of <span class="math inline">\(\hspace{0.05cm} X_j \hspace{0.05cm}\)</span> is defined as:</p>
<p><span class="math display">\[ GVIF(X_j) = det(R_1) \cdot \dfrac{det(R_2)}{det(R_3)} \]</span></p>
<hr />
<p><strong>Criteria:</strong> Identification of multicollinearity with <span class="math inline">\(GVIF\)</span></p>
<p>In this criteria the key quantity is <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/2 df}\)</span></p>
<p>Where: <span class="math inline">\(\hspace{0.15cm}df\hspace{0.1cm}\)</span> is the number of parameters that enter in the linear regression model when <span class="math inline">\(\hspace{0.1cm} X_j\hspace{0.1cm}\)</span> is added as predictor</p>
<p>If <span class="math inline">\(X_j\)</span> is a <strong>quantitative</strong> predictor <span class="math inline">\(\hspace{0.1cm} ( \Rightarrow df=1)\)</span> :</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/2} &gt; \sqrt{10} = 3.16\)</span> <span class="math inline">\(\hspace{0.1cm}\Rightarrow\hspace{0.1cm}\)</span> <strong>High</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/2} \in [\sqrt{4}, \sqrt{10}]= [2 \hspace{0.1cm},\hspace{0.1cm} 3.16]\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow\hspace{0.1cm}\)</span> <strong>Medium</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/2} &lt; \sqrt{4} = 2\)</span> <span class="math inline">\(\hspace{0.1cm}\Rightarrow\hspace{0.1cm}\)</span> <strong>Low</strong> multicollinearity</p>
<p>If <span class="math inline">\(X_j\)</span> is a <strong>categorical</strong> predictor with <span class="math inline">\(2\)</span> categories <span class="math inline">\(\hspace{0.1cm} ( \Rightarrow df=2-1=1)\)</span> :</p>
<p>If $ GVIF(X_j)^{1/2} &gt; 10^{1/2} = 3.16 $ <span class="math inline">\(\hspace{0.1cm}\Rightarrow\hspace{0.1cm}\)</span> <strong>High</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/2} \in [4^{1/2}, 10^{1/2}] = [2 \hspace{0.1cm} , \hspace{0.1cm} 3.16]\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow\hspace{0.1cm}\)</span> <strong>Medium</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/2} &lt; 4^{1/2} = 2\)</span> <span class="math inline">\(\hspace{0.1cm}\Rightarrow\hspace{0.1cm}\)</span> <strong>Low</strong> multicollinearity</p>
<p>If <span class="math inline">\(X_j\)</span> is a <strong>categorical</strong> predictor with <span class="math inline">\(3\)</span> categories <span class="math inline">\(\hspace{0.1cm} ( \Rightarrow df=3-1=2 )\)</span> :</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/4} &gt; 10^{1/4} = 1.78\)</span> <span class="math inline">\(\hspace{0.1cm}\Rightarrow\hspace{0.1cm}\)</span> <strong>High</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/4} \in [4^{1/4}, 10^{1/4}] = [1.41 \hspace{0.1cm} , \hspace{0.1cm} 1.78]\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow\hspace{0.1cm}\)</span> <strong>Medium</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/4} &lt; 4^{1/4} = 1.41\)</span> <span class="math inline">\(\hspace{0.1cm}\Rightarrow\hspace{0.1cm}\)</span> <strong>Low</strong> multicollinearity</p>
<p>If <span class="math inline">\(X_j\)</span> is a <strong>categorical</strong> predictor with <span class="math inline">\(4\)</span> categories <span class="math inline">\((\hspace{0.1cm} \Rightarrow df=4-1=3 )\)</span> :</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/6} &gt; 10^{1/6} = 1.47\)</span> <span class="math inline">\(\hspace{0.1cm}\Rightarrow\hspace{0.1cm}\)</span> <strong>High</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/6} \in [4^{1/6}, 10^{1/6}] = [1.26 \hspace{0.1cm} , \hspace{0.1cm} 1.47]\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow\hspace{0.1cm}\)</span> <strong>Medium</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/6} &lt; 4^{1/6} = 1.26\)</span> <span class="math inline">\(\hspace{0.1cm}\Rightarrow\hspace{0.1cm}\)</span> <strong>Low</strong> multicollinearity</p>
<p>If <span class="math inline">\(X_j\)</span> is a <strong>categorical</strong> predictor with <span class="math inline">\(r\)</span> categories <span class="math inline">\((\hspace{0.1cm} \Rightarrow df=r-1 )\)</span> :</p>
<p>If $ GVIF(X_j)^{1/(2(r-1))} &gt; 10^{1/(2(r-1))} $ <span class="math inline">\(\hspace{0.1cm}\Rightarrow\hspace{0.1cm}\)</span> <strong>High</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/(2(r-1))} \in [4^{1/(2(r-1))}, 10^{1/(2(r-1))}]\)</span> <span class="math inline">\(\hspace{0.1cm} \Rightarrow\hspace{0.1cm}\)</span> <strong>Medium</strong> multicollinearity</p>
<p>If <span class="math inline">\(\hspace{0.15cm} GVIF(X_j)^{1/(2(r-1))} &lt; 4^{1/(2(r-1))}\)</span> <span class="math inline">\(\hspace{0.1cm}\Rightarrow\hspace{0.1cm}\)</span> <strong>Low</strong> multicollinearity</p>
<hr />
<div id="identification-of-multicollinearity-with-gvif-in-r" class="section level5" number="0.9.6.0.1">
<h5><span class="header-section-number">0.9.6.0.1</span> Identification of multicollinearity with <span class="math inline">\(GVIF\)</span> in <code>R</code></h5>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R </span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(carData)</span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(<span class="fu">lm</span>(price <span class="sc">~</span> size_in_m_2  <span class="sc">+</span> longitude <span class="sc">+</span> latitude <span class="sc">+</span> no_of_bedrooms <span class="sc">+</span> no_of_bathrooms <span class="sc">+</span> quality , <span class="at">data=</span>data_R))</span></code></pre></div>
<hr />
</div>
<div id="identification-of-multicollinearity-with-gvif-in-python" class="section level5" number="0.9.6.0.2">
<h5><span class="header-section-number">0.9.6.0.2</span> Identification of multicollinearity with <span class="math inline">\(GVIF\)</span> in <code>Python</code></h5>
<p><span class="math inline">\(GVIF\)</span> for the <strong>categorical</strong> predictor <span class="math inline">\(quality\)</span></p>
<div class="sourceCode" id="cb118"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data_Python.loc[: , [<span class="st">&#39;size_in_m_2&#39;</span>, <span class="st">&#39;longitude&#39;</span>, <span class="st">&#39;latitude&#39;</span>, <span class="st">&#39;no_of_bedrooms&#39;</span>, <span class="st">&#39;no_of_bathrooms&#39;</span>, <span class="st">&#39;quality&#39;</span>]]</span></code></pre></div>
<div class="sourceCode" id="cb119"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> varcharProcessing(X).loc[:, [<span class="st">&#39;quality_1&#39;</span>, <span class="st">&#39;quality_2&#39;</span>, <span class="st">&#39;quality_3&#39;</span>]]</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> X.drop({<span class="st">&#39;quality&#39;</span>}, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>X3 <span class="op">=</span> varcharProcessing(X).drop({<span class="st">&#39;intercept&#39;</span>}, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>GVIF_quality <span class="op">=</span> np.linalg.det(X1.corr()) <span class="op">*</span> np.linalg.det(X2.corr()) <span class="op">/</span> np.linalg.det(X3.corr())</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a>df_quality <span class="op">=</span> X1.shape[<span class="dv">1</span>]</span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true" tabindex="-1"></a>GVIF_quality_scaled <span class="op">=</span> GVIF_quality<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>df_quality))</span></code></pre></div>
<p><span class="math inline">\(GVIF\)</span> for the rest of <strong>quantitative</strong> predictors</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data_Python.loc[: , [<span class="st">&#39;size_in_m_2&#39;</span>, <span class="st">&#39;longitude&#39;</span>, <span class="st">&#39;latitude&#39;</span>, <span class="st">&#39;no_of_bedrooms&#39;</span>, <span class="st">&#39;no_of_bathrooms&#39;</span>, <span class="st">&#39;quality&#39;</span>]]</span></code></pre></div>
<div class="sourceCode" id="cb121"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> X[[<span class="st">&#39;size_in_m_2&#39;</span>]]  </span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> X.drop([<span class="st">&#39;size_in_m_2&#39;</span>], axis<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>X3 <span class="op">=</span> X</span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a>GVIF_size_in_m_2 <span class="op">=</span> np.linalg.det(X1.corr()) <span class="op">*</span> np.linalg.det(X2.corr()) <span class="op">/</span> np.linalg.det( X3.corr() )</span>
<span id="cb121-6"><a href="#cb121-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-7"><a href="#cb121-7" aria-hidden="true" tabindex="-1"></a>df_size_in_m_2 <span class="op">=</span> X1.shape[<span class="dv">1</span>]</span>
<span id="cb121-8"><a href="#cb121-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-9"><a href="#cb121-9" aria-hidden="true" tabindex="-1"></a>GVIF_size_in_m_2_scaled <span class="op">=</span> GVIF_size_in_m_2<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>df_size_in_m_2))</span></code></pre></div>
<div class="sourceCode" id="cb122"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> X[[<span class="st">&#39;longitude&#39;</span>]]  </span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> X.drop([<span class="st">&#39;longitude&#39;</span>], axis<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true" tabindex="-1"></a>X3 <span class="op">=</span> X</span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-5"><a href="#cb122-5" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb122-6"><a href="#cb122-6" aria-hidden="true" tabindex="-1"></a>GVIF_longitude <span class="op">=</span> np.linalg.det(X1.corr()) <span class="op">*</span> np.linalg.det(X2.corr()) <span class="op">/</span> np.linalg.det( X3.corr() )</span>
<span id="cb122-7"><a href="#cb122-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-8"><a href="#cb122-8" aria-hidden="true" tabindex="-1"></a>df_longitude <span class="op">=</span> X1.shape[<span class="dv">1</span>]</span>
<span id="cb122-9"><a href="#cb122-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-10"><a href="#cb122-10" aria-hidden="true" tabindex="-1"></a>GVIF_longitude_scaled <span class="op">=</span> GVIF_longitude<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>df_longitude))</span></code></pre></div>
<div class="sourceCode" id="cb123"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> X[[<span class="st">&#39;latitude&#39;</span>]]  </span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> X.drop([<span class="st">&#39;latitude&#39;</span>], axis<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a>X3 <span class="op">=</span> X</span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb123-5"><a href="#cb123-5" aria-hidden="true" tabindex="-1"></a>GVIF_latitude <span class="op">=</span> np.linalg.det(X1.corr()) <span class="op">*</span> np.linalg.det(X2.corr()) <span class="op">/</span> np.linalg.det(  X3.corr() )</span>
<span id="cb123-6"><a href="#cb123-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-7"><a href="#cb123-7" aria-hidden="true" tabindex="-1"></a>df_latitude <span class="op">=</span> X1.shape[<span class="dv">1</span>]</span>
<span id="cb123-8"><a href="#cb123-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-9"><a href="#cb123-9" aria-hidden="true" tabindex="-1"></a>GVIF_latitude_scaled <span class="op">=</span> GVIF_latitude<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>df_latitude))</span></code></pre></div>
<div class="sourceCode" id="cb124"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> X[[<span class="st">&#39;no_of_bedrooms&#39;</span>]]  </span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> X.drop([<span class="st">&#39;no_of_bedrooms&#39;</span>], axis<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>X3 <span class="op">=</span> X</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a>GVIF_no_of_bedrooms <span class="op">=</span> np.linalg.det(X1.corr()) <span class="op">*</span> np.linalg.det(X2.corr()) <span class="op">/</span> np.linalg.det( X3.corr() )</span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true" tabindex="-1"></a>df_no_of_bedrooms <span class="op">=</span> X1.shape[<span class="dv">1</span>]</span>
<span id="cb124-8"><a href="#cb124-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-9"><a href="#cb124-9" aria-hidden="true" tabindex="-1"></a>GVIF_no_of_bedrooms_scaled <span class="op">=</span> GVIF_no_of_bedrooms<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>df_no_of_bedrooms))</span></code></pre></div>
<div class="sourceCode" id="cb125"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> X[[<span class="st">&#39;no_of_bathrooms&#39;</span>]]  </span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> X.drop([<span class="st">&#39;no_of_bathrooms&#39;</span>], axis<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>X3 <span class="op">=</span> X</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a>GVIF_no_of_bathrooms <span class="op">=</span> np.linalg.det(X1.corr()) <span class="op">*</span> np.linalg.det(X2.corr()) <span class="op">/</span> np.linalg.det( X3.corr())</span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-7"><a href="#cb125-7" aria-hidden="true" tabindex="-1"></a>df_no_of_bathrooms <span class="op">=</span> X1.shape[<span class="dv">1</span>]</span>
<span id="cb125-8"><a href="#cb125-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-9"><a href="#cb125-9" aria-hidden="true" tabindex="-1"></a>GVIF_no_of_bathrooms_scaled <span class="op">=</span> GVIF_no_of_bathrooms<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>df_no_of_bathrooms))</span></code></pre></div>
<div class="sourceCode" id="cb126"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> X.columns</span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> [GVIF_size_in_m_2, GVIF_longitude, GVIF_latitude, GVIF_no_of_bedrooms, GVIF_no_of_bathrooms, GVIF_quality]</span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> [GVIF_size_in_m_2_scaled, GVIF_longitude_scaled, GVIF_latitude_scaled, GVIF_no_of_bedrooms_scaled, GVIF_no_of_bathrooms_scaled, GVIF_quality_scaled]</span>
<span id="cb126-6"><a href="#cb126-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-7"><a href="#cb126-7" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> [df_size_in_m_2, df_longitude, df_latitude, df_no_of_bedrooms, df_no_of_bathrooms, df_quality]</span>
<span id="cb126-8"><a href="#cb126-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-9"><a href="#cb126-9" aria-hidden="true" tabindex="-1"></a>GVIF_Python_df <span class="op">=</span> pd.DataFrame({<span class="st">&#39;predictor&#39;</span>: a , <span class="st">&#39;GVIF&#39;</span>: b , <span class="st">&#39;GVIF^(1/(2*df))&#39;</span>: c , <span class="st">&#39;df&#39;</span>: d})</span>
<span id="cb126-10"><a href="#cb126-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-11"><a href="#cb126-11" aria-hidden="true" tabindex="-1"></a>GVIF_Python_df</span></code></pre></div>
<p>In this case all the quantitative predictors have <span class="math inline">\(\hspace{0.1cm} GVIF^{1/2df} &lt; 3.16 \hspace{0.1cm}\)</span>, but <span class="math inline">\(\hspace{0.1cm} GVIF^{1/2df} \hspace{0.1cm}\)</span> of <span class="math inline">\(\hspace{0.1cm}\)</span> <strong>no_of_bathrooms</strong> <span class="math inline">\(\hspace{0.1cm}\)</span> is <span class="math inline">\(\hspace{0.1cm} 2.14 &gt; 2\)</span> $ $ <strong>medium</strong> multicollinearity</p>
<p>On the other hand the 4-ary categorical predictor <strong>quality</strong> has <span class="math inline">\(\hspace{0.1cm} GVIF^{1/2df}\hspace{0.1cm} = 1.01 &lt; 1.26\)</span> $ $ <strong>low</strong> multicollinearity</p>
<p>So using the GVIF method, we can conclude that there is a <strong>medium</strong> multicollinearity between the predictors of the model <span class="math inline">\(\hspace{0.1cm}\)</span> <strong>price ~ size_in_m_2 + longitude + latitude + no_of_bedrooms + no_of_bathrooms</strong> <span class="math inline">\(\hspace{0.1cm}\)</span></p>
<hr />
</div>
</div>
</div>
<div id="checking-error-assumptions" class="section level2" number="0.10">
<h2><span class="header-section-number">0.10</span> Checking Error Assumptions <a class="anchor" id="68"></a></h2>
<ul>
<li><p>$_i N(0,) i=1,…,n $</p></li>
<li><p>$corr(_i , _j)=0 ij =1,…,n $</p></li>
</ul>
<div id="checking-error-constant-variance" class="section level3" number="0.10.1">
<h3><span class="header-section-number">0.10.1</span> Checking Error Constant Variance <a class="anchor" id="69"></a></h3>
<p>It is not possible to check the assumption of constant variance just by examining
the residuals alone, some will be large and some will be small, but this proves nothing. We need to check whether the variance in the residuals is related to some
other quantity.</p>
<p>The most useful diagnostic is a plot of <span class="math inline">\(\hat{\varepsilon}\)</span> vs <span class="math inline">\(\hat{y}\)</span></p>
<p>Interpretation of the graph <span class="math inline">\(\hat{\varepsilon}\)</span> vs <span class="math inline">\(\hat{y}\)</span>:</p>
<ul>
<li><p>If the dispersion of the points is uniform random <span class="math inline">\(\Rightarrow\)</span> There is no problem (evidence of constant variance of the error).</p></li>
<li><p>If we can see a cone shape at the points <span class="math inline">\(\Rightarrow\)</span> Problem (evidence of error not constant variance)</p></li>
</ul>
<p><br></p>
<p>This plot can also provide evidence of nonlinearity between the response variable and the predictors:</p>
<ul>
<li>If we can see a non-linear shape at the points <span class="math inline">\(\Rightarrow\)</span> evidence of non-linearity between the response variable and the predictors</li>
</ul>
<hr />
<div id="checking-error-constant-variance-in-r" class="section level5" number="0.10.1.0.1">
<h5><span class="header-section-number">0.10.1.0.1</span> Checking Error Constant Variance in <code>R</code></h5>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_point</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x=</span>model_R<span class="sc">$</span>fitted.values , <span class="at">y =</span> estimated_errors ), <span class="at">color=</span><span class="st">&quot;blue&quot;</span>)<span class="sc">+</span></span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_hline</span>(<span class="at">yintercept =</span><span class="dv">0</span> , <span class="at">color=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<hr />
</div>
<div id="checking-error-constant-variance-in-python" class="section level5" number="0.10.1.0.2">
<h5><span class="header-section-number">0.10.1.0.2</span> Checking Error Constant Variance in <code>Python</code></h5>
<div class="sourceCode" id="cb128"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> plotnine <span class="im">import</span> ggplot, aes, geom_line, geom_point, geom_smooth, geom_histogram, geom_bar, geom_boxplot, scale_y_continuous, scale_x_continuous, scale_x_discrete, scale_y_discrete, labs, after_stat,  geom_vline, scale_color_manual, theme_gray, theme_xkcd, scale_color_identity, geom_hline, facet_wrap, scale_fill_discrete, scale_fill_manual,  scale_fill_hue, guides, guide_legend, geom_hline, stat_function </span></code></pre></div>
<div class="sourceCode" id="cb129"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>ggplot()</span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_point(mapping <span class="op">=</span> aes(x<span class="op">=</span>predictions , y <span class="op">=</span> df_predictions_Python[<span class="st">&#39;estimated_errors&#39;</span>] ), color<span class="op">=</span><span class="st">&quot;green&quot;</span>)</span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_hline(yintercept <span class="op">=</span><span class="dv">0</span> , color<span class="op">=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> labs(x<span class="op">=</span><span class="st">&#39;response predictions&#39;</span> , y<span class="op">=</span><span class="st">&#39;estimated errors&#39;</span>)</span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Since we can see a cone shape at the points, we cannot accept the constant error variance assumption <span class="math inline">\(Var(\varepsilon)=\sigma^2\)</span>.</p>
<hr />
</div>
</div>
<div id="checking-null-error-mean" class="section level3" number="0.10.2">
<h3><span class="header-section-number">0.10.2</span> Checking Null Error Mean <a class="anchor" id="70"></a></h3>
<p>We are going to use the typical t-test to verify this assumption.</p>
<p>First we compute the estimated errors mean to get first idea:</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>df_predictions_Python[<span class="st">&#39;estimated_errors&#39;</span>].mean()</span></code></pre></div>
<hr />
<div id="checking-null-error-mean-in-r" class="section level5" number="0.10.2.0.1">
<h5><span class="header-section-number">0.10.2.0.1</span> Checking Null Error Mean in <code>R</code></h5>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(<span class="at">x=</span>estimated_errors , <span class="at">mu=</span><span class="dv">0</span>)</span></code></pre></div>
<hr />
</div>
<div id="checking-null-error-mean-in-python" class="section level5" number="0.10.2.0.2">
<h5><span class="header-section-number">0.10.2.0.2</span> Checking Null Error Mean in <code>Python</code></h5>
<div class="sourceCode" id="cb132"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install --upgrade pingouin</span></span></code></pre></div>
<div class="sourceCode" id="cb133"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pingouin <span class="im">as</span> pg</span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>pg.ttest(x<span class="op">=</span> df_predictions_Python[<span class="st">&#39;estimated_errors&#39;</span>] , y<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<p>For any significance level we can accept that the errors mean is zero (0)</p>
<hr />
</div>
</div>
<div id="checking-error-normality" class="section level3" number="0.10.3">
<h3><span class="header-section-number">0.10.3</span> Checking Error Normality <a class="anchor" id="71"></a></h3>
<p>First we are going to check the error normality assumption using the <strong>histogram method</strong>:</p>
<div id="checking-error-normality-in-r-with-histogram-method" class="section level4" number="0.10.3.1">
<h4><span class="header-section-number">0.10.3.1</span> Checking Error Normality in <code>R</code> with histogram method</h4>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> <span class="fu">as.data.frame</span>(estimated_errors) ,</span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a> <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> estimated_errors)) <span class="sc">+</span></span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span>..density..), <span class="at">color=</span><span class="st">&quot;black&quot;</span>, <span class="at">fill=</span><span class="st">&quot;orange&quot;</span>,</span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a> <span class="at">position =</span> <span class="st">&#39;identity&#39;</span>) <span class="sc">+</span></span>
<span id="cb134-7"><a href="#cb134-7" aria-hidden="true" tabindex="-1"></a> <span class="fu">stat_function</span>( <span class="at">fun =</span> dnorm , </span>
<span id="cb134-8"><a href="#cb134-8" aria-hidden="true" tabindex="-1"></a> <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="fu">mean</span>(estimated_errors), <span class="at">sd =</span> <span class="fu">sd</span>(estimated_errors)),</span>
<span id="cb134-9"><a href="#cb134-9" aria-hidden="true" tabindex="-1"></a> <span class="at">lwd =</span> <span class="fl">1.2</span>,  <span class="at">col =</span> <span class="st">&#39;blue&#39;</span></span>
<span id="cb134-10"><a href="#cb134-10" aria-hidden="true" tabindex="-1"></a> )</span></code></pre></div>
<hr />
</div>
<div id="checking-error-normality-in-python-with-histogram-method" class="section level4" number="0.10.3.2">
<h4><span class="header-section-number">0.10.3.2</span> Checking Error Normality in <code>Python</code> with histogram method</h4>
<div class="sourceCode" id="cb135"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb135-3"><a href="#cb135-3" aria-hidden="true" tabindex="-1"></a>ggplot(data <span class="op">=</span> df_predictions_Python ,</span>
<span id="cb135-4"><a href="#cb135-4" aria-hidden="true" tabindex="-1"></a> mapping <span class="op">=</span> aes(x <span class="op">=</span> df_predictions_Python[<span class="st">&#39;estimated_errors&#39;</span>])) </span>
<span id="cb135-5"><a href="#cb135-5" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_histogram(aes(y <span class="op">=</span> after_stat(<span class="st">&#39;density&#39;</span>)), color<span class="op">=</span><span class="st">&quot;black&quot;</span>, fill<span class="op">=</span><span class="st">&quot;pink&quot;</span>, position <span class="op">=</span> <span class="st">&#39;identity&#39;</span>, bins<span class="op">=</span><span class="dv">35</span>) </span>
<span id="cb135-6"><a href="#cb135-6" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> stat_function( fun <span class="op">=</span> stats.norm.pdf , args <span class="op">=</span> <span class="bu">dict</span>( loc<span class="op">=</span>mean(df_predictions_Python[<span class="st">&#39;estimated_errors&#39;</span>]), scale<span class="op">=</span>sd(df_predictions_Python[<span class="st">&#39;estimated_errors&#39;</span>])) , color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, size<span class="op">=</span><span class="dv">1</span> )</span>
<span id="cb135-7"><a href="#cb135-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<hr />
<p>Now we are going to check the error normality assumption using the Shapiro statistical test:</p>
</div>
<div id="checking-error-normality-in-r-with-shapiro-test" class="section level4" number="0.10.3.3">
<h4><span class="header-section-number">0.10.3.3</span> Checking Error Normality in <code>R</code> with Shapiro test</h4>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(estimated_errors)</span></code></pre></div>
<hr />
</div>
<div id="checking-error-normality-in-python-with-shapiro-test" class="section level4" number="0.10.3.4">
<h4><span class="header-section-number">0.10.3.4</span> Checking Error Normality in <code>Python</code> with Shapiro test</h4>
<div class="sourceCode" id="cb137"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>shapiro_test <span class="op">=</span> stats.shapiro(df_predictions_Python[<span class="st">&#39;estimated_errors&#39;</span>])</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a>shapiro_test</span></code></pre></div>
<p>For any significance level we have to reject the errors normality hypothesis.</p>
<hr />
</div>
</div>
<div id="checking-null-error-correlation" class="section level3" number="0.10.4">
<h3><span class="header-section-number">0.10.4</span> Checking Null Error Correlation <a class="anchor" id="72"></a></h3>
<p>We are going to check the null error correlation assumption through the <strong>Durban-Watson test</strong>.</p>
<div id="hypothesis" class="section level5" number="0.10.4.0.1">
<h5><span class="header-section-number">0.10.4.0.1</span> Hypothesis:</h5>
<p><span class="math display">\[H_0: \hspace{0.1cm} corr\left(\hat{\varepsilon}_{(1)},\hat{\varepsilon}_{(2)}\right) = 0 \]</span></p>
<p><span class="math display">\[H_1: \hspace{0.1cm} corr\left(\hat{\varepsilon}_{(1)},\hat{\varepsilon}_{(2)}\right) \neq 0\]</span></p>
<p>Where:</p>
<p><span class="math display">\[\hat{\varepsilon}_{(1)} = (\hat{\varepsilon}_1 , \hat{\varepsilon}_2 ,..., \hat{\varepsilon}_{n-1})\]</span></p>
<p><span class="math display">\[\hat{\varepsilon}_{(2)} = (\hat{\varepsilon}_2  ,...,\hat{\varepsilon}_{n-1}, \hat{\varepsilon}_{n})\]</span></p>
</div>
<div id="test-statistic-1" class="section level5" number="0.10.4.0.2">
<h5><span class="header-section-number">0.10.4.0.2</span> Test Statistic :</h5>
<p><span class="math display">\[\begin{gather*}
DW = \dfrac{\sum_{i=2}^n (\hat{\varepsilon}_i - \hat{\varepsilon}_{i-1})^2 }{\sum_{i=2}^n \hat{\varepsilon}_{i}^2 } = \dfrac{sum\left( \hspace{0.1cm}( \hat{\varepsilon}_{(2)} - \hat{\varepsilon}_{(1)})^2 \hspace{0.1cm} \right)}{sum\left( \hspace{0.1cm} (\hat{\varepsilon}_{(2)})^2 \hspace{0.1cm} \right)}
\end{gather*}\]</span></p>
<p>The test statistic is approximately equal to <span class="math inline">\(\hspace{0.1cm} 2\cdot (1-r) \hspace{0.1cm}\)</span> where <span class="math inline">\(r\)</span> is the sample autocorrelation of the residuals. Thus, the test statistic will always be between <span class="math inline">\(0\)</span> and <span class="math inline">\(4\)</span> with the following interpretation:</p>
<p>A test statistic of <span class="math inline">\(2\)</span> indicates <strong>no serial correlation</strong>.</p>
<p>The closer the test statistics is to <span class="math inline">\(0\)</span>, the more evidence of <strong>positive serial correlation</strong>.</p>
<p>The closer the test statistics is to <span class="math inline">\(4\)</span>, the more evidence of <strong>negative serial correlation</strong>.</p>
<hr />
</div>
<div id="durban-watson-test-in-r" class="section level5" number="0.10.4.0.3">
<h5><span class="header-section-number">0.10.4.0.3</span> Durban-Watson test in <code>R</code></h5>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>estimated_errors_1 <span class="ot">=</span> estimated_errors[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a>estimated_errors_2 <span class="ot">=</span> estimated_errors[<span class="sc">-</span><span class="fu">length</span>(estimated_errors)]</span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-6"><a href="#cb138-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb138-7"><a href="#cb138-7" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_point</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x=</span>estimated_errors_1 , <span class="at">y =</span> estimated_errors_2 ), <span class="at">color=</span><span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="sc">%%</span>R</span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(zoo)</span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lmtest)</span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true" tabindex="-1"></a><span class="fu">dwtest</span>(price <span class="sc">~</span> size_in_m_2  <span class="sc">+</span> longitude <span class="sc">+</span> latitude <span class="sc">+</span> no_of_bedrooms <span class="sc">+</span> no_of_bathrooms <span class="sc">+</span> quality , <span class="at">data=</span>data_R, <span class="at">alternative =</span> <span class="st">&#39;two.sided&#39;</span> )</span></code></pre></div>
<hr />
</div>
<div id="durban-watson-test-in-python" class="section level5" number="0.10.4.0.4">
<h5><span class="header-section-number">0.10.4.0.4</span> Durban-Watson test in <code>Python</code></h5>
<div class="sourceCode" id="cb140"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>train_errors_1 <span class="op">=</span> pd.DataFrame({ <span class="st">&#39;train_errors_1&#39;</span>: df_predictions_Python.iloc[<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(df_predictions_Python)) , <span class="dv">2</span> ] } )</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>train_errors_2 <span class="op">=</span> pd.DataFrame({ <span class="st">&#39;train_errors_1&#39;</span>: df_predictions_Python.iloc[<span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(df_predictions_Python)<span class="op">-</span><span class="dv">1</span>) , <span class="dv">2</span> ] } )  </span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a>train_errors_2[<span class="st">&#39;index&#39;</span>]<span class="op">=</span><span class="bu">range</span>(<span class="dv">1</span>,<span class="bu">len</span>(train_errors_2)<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a>train_errors_2 <span class="op">=</span> train_errors_2.set_index(<span class="st">&#39;index&#39;</span>)</span>
<span id="cb140-6"><a href="#cb140-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-7"><a href="#cb140-7" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb140-8"><a href="#cb140-8" aria-hidden="true" tabindex="-1"></a>ggplot() </span>
<span id="cb140-9"><a href="#cb140-9" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_point(mapping <span class="op">=</span> aes(x<span class="op">=</span>train_errors_1 , y <span class="op">=</span> train_errors_2 ), color<span class="op">=</span><span class="st">&quot;green&quot;</span>)</span>
<span id="cb140-10"><a href="#cb140-10" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> labs(x<span class="op">=</span><span class="st">&#39;estimated errors 1&#39;</span> , y<span class="op">=</span><span class="st">&#39;estimated errors 2&#39;</span>)</span>
<span id="cb140-11"><a href="#cb140-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb141"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels</span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>statsmodels.stats.stattools.durbin_watson(df_predictions_Python[<span class="st">&#39;estimated_errors&#39;</span>], axis<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<p>Another alternative in Python that gives us the p-value:</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install dwtest</span></span></code></pre></div>
<p>More information about this Python package here <a href="https://github.com/JackywithaWhiteDog/dwtest" class="uri">https://github.com/JackywithaWhiteDog/dwtest</a></p>
<div class="sourceCode" id="cb143"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dwtest <span class="im">import</span> dwtest</span></code></pre></div>
<div class="sourceCode" id="cb144"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>statistic, pvalue <span class="op">=</span> dwtest( <span class="st">&#39;price ~ size_in_m_2  + longitude + latitude + no_of_bedrooms + no_of_bathrooms&#39;</span>  , data_Python)</span></code></pre></div>
<div class="sourceCode" id="cb145"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a>statistic, pvalue</span></code></pre></div>
<p>For any <span class="math inline">\(\alpha\)</span> we reject the null error correlation hypothesis.</p>
<hr />
</div>
</div>
</div>
<div id="checking-linear-assumption" class="section level2" number="0.11">
<h2><span class="header-section-number">0.11</span> Checking Linear Assumption <a class="anchor" id="73"></a></h2>
<p>The linear regression model assumes that there is a linear relationship between the response and the predictors.</p>
<p>If the actually existing relationship is not linear, then all the conclusions we draw from the linear model are questionable.</p>
<p>How to identify nonlinearity?</p>
<p>Make a plot of the estimated errors (residuals) against the predictions of the response (all with the train data).</p>
<p>It is the same plot as the one used to diagnose heteroscedasticity of the residuals (non-constant variance of the residuals). If a non-linear relationship is observed in the graph, it indicates non-linearity between the predictors and the response.</p>
<p>They can also help detect nonlinearity in the relationship between response and predictors by plotting the response versus predictors (one-to-one).</p>
<p>Solutions:</p>
<p>A logarithmic transformation of the response variable could help. But the best option is to use a nonlinear (or nonparametric) regression method.</p>
<p>Scatter Plots: response vs predictors (one-to-one)</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a>ggplot(aes(x <span class="op">=</span> <span class="st">&#39;size_in_m_2&#39;</span> , y <span class="op">=</span> <span class="st">&#39;price&#39;</span>) , data_Python) </span>
<span id="cb146-3"><a href="#cb146-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-4"><a href="#cb146-4" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> geom_point(fill<span class="op">=</span><span class="st">&quot;white&quot;</span>, color <span class="op">=</span> <span class="st">&#39;red&#39;</span>) </span>
<span id="cb146-5"><a href="#cb146-5" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> geom_smooth(method<span class="op">=</span><span class="st">&#39;lm&#39;</span>, color <span class="op">=</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb146-6"><a href="#cb146-6" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> labs(x <span class="op">=</span> <span class="st">&quot;size_in_m_2&quot;</span>, y <span class="op">=</span> <span class="st">&quot;price&quot;</span>)</span>
<span id="cb146-7"><a href="#cb146-7" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> scale_x_continuous( breaks <span class="op">=</span> <span class="bu">range</span>(<span class="bu">int</span>(data_Python[<span class="st">&#39;size_in_m_2&#39;</span>].<span class="bu">min</span>()) , <span class="bu">int</span>(data_Python[<span class="st">&#39;size_in_m_2&#39;</span>].<span class="bu">max</span>()) , <span class="dv">50</span>) ) </span>
<span id="cb146-8"><a href="#cb146-8" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> scale_y_continuous( breaks <span class="op">=</span> <span class="bu">range</span>(<span class="bu">int</span>(data_Python[<span class="st">&#39;price&#39;</span>].<span class="bu">min</span>()) , <span class="bu">int</span>(data_Python[<span class="st">&#39;price&#39;</span>].<span class="bu">max</span>()) , <span class="dv">2000000</span>) )</span>
<span id="cb146-9"><a href="#cb146-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb147"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb147-2"><a href="#cb147-2" aria-hidden="true" tabindex="-1"></a>ggplot(aes(x <span class="op">=</span> <span class="st">&#39;longitude&#39;</span> , y <span class="op">=</span> <span class="st">&#39;price&#39;</span>) , data_Python) </span>
<span id="cb147-3"><a href="#cb147-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-4"><a href="#cb147-4" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> geom_point(fill<span class="op">=</span><span class="st">&quot;white&quot;</span>, color <span class="op">=</span> <span class="st">&#39;red&#39;</span>) </span>
<span id="cb147-5"><a href="#cb147-5" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> geom_smooth(method<span class="op">=</span><span class="st">&#39;lm&#39;</span>, color <span class="op">=</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb147-6"><a href="#cb147-6" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> labs(x <span class="op">=</span> <span class="st">&quot;longitude&quot;</span>, y <span class="op">=</span> <span class="st">&quot;price&quot;</span>)</span>
<span id="cb147-7"><a href="#cb147-7" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> scale_x_continuous( breaks <span class="op">=</span> np.arange (data_Python[<span class="st">&#39;longitude&#39;</span>].<span class="bu">min</span>() , data_Python[<span class="st">&#39;longitude&#39;</span>].<span class="bu">max</span>() , <span class="fl">0.05</span>) )</span>
<span id="cb147-8"><a href="#cb147-8" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> scale_y_continuous( breaks <span class="op">=</span> <span class="bu">range</span>(<span class="bu">int</span>(data_Python[<span class="st">&#39;price&#39;</span>].<span class="bu">min</span>()) , <span class="bu">int</span>(data_Python[<span class="st">&#39;price&#39;</span>].<span class="bu">max</span>()) , <span class="dv">2000000</span>) )</span>
<span id="cb147-9"><a href="#cb147-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb148"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a>ggplot(aes(x <span class="op">=</span> <span class="st">&#39;latitude&#39;</span> , y <span class="op">=</span> <span class="st">&#39;price&#39;</span>) , data_Python) </span>
<span id="cb148-3"><a href="#cb148-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-4"><a href="#cb148-4" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> geom_point(fill<span class="op">=</span><span class="st">&quot;white&quot;</span>, color <span class="op">=</span> <span class="st">&#39;red&#39;</span>) </span>
<span id="cb148-5"><a href="#cb148-5" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> geom_smooth(method<span class="op">=</span><span class="st">&#39;lm&#39;</span>, color <span class="op">=</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb148-6"><a href="#cb148-6" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> labs(x <span class="op">=</span> <span class="st">&quot;latitude&quot;</span>, y <span class="op">=</span> <span class="st">&quot;price&quot;</span>)</span>
<span id="cb148-7"><a href="#cb148-7" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> scale_x_continuous( breaks <span class="op">=</span>  np.arange (data_Python[<span class="st">&#39;latitude&#39;</span>].<span class="bu">min</span>() , data_Python[<span class="st">&#39;latitude&#39;</span>].<span class="bu">max</span>() , <span class="fl">0.05</span>)  ) </span>
<span id="cb148-8"><a href="#cb148-8" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> scale_y_continuous( breaks <span class="op">=</span> <span class="bu">range</span>(<span class="bu">int</span>(data_Python[<span class="st">&#39;price&#39;</span>].<span class="bu">min</span>()) , <span class="bu">int</span>(data_Python[<span class="st">&#39;price&#39;</span>].<span class="bu">max</span>()) , <span class="dv">2000000</span>) )</span>
<span id="cb148-9"><a href="#cb148-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb149"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a>ggplot(aes(x <span class="op">=</span> <span class="st">&#39;no_of_bedrooms&#39;</span> , y <span class="op">=</span> <span class="st">&#39;price&#39;</span>) , data_Python) </span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> geom_point(fill<span class="op">=</span><span class="st">&quot;white&quot;</span>, color <span class="op">=</span> <span class="st">&#39;red&#39;</span>) </span>
<span id="cb149-5"><a href="#cb149-5" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> geom_smooth(method<span class="op">=</span><span class="st">&#39;lm&#39;</span>, color <span class="op">=</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb149-6"><a href="#cb149-6" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> labs(x <span class="op">=</span> <span class="st">&quot;no_of_bedrooms&quot;</span>, y <span class="op">=</span> <span class="st">&quot;price&quot;</span>)</span>
<span id="cb149-7"><a href="#cb149-7" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> scale_x_continuous( breaks <span class="op">=</span>  np.arange (data_Python[<span class="st">&#39;no_of_bedrooms&#39;</span>].<span class="bu">min</span>() , data_Python[<span class="st">&#39;no_of_bedrooms&#39;</span>].<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span> , <span class="dv">1</span>)  ) </span>
<span id="cb149-8"><a href="#cb149-8" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> scale_y_continuous( breaks <span class="op">=</span> <span class="bu">range</span>(<span class="bu">int</span>(data_Python[<span class="st">&#39;price&#39;</span>].<span class="bu">min</span>()) , <span class="bu">int</span>(data_Python[<span class="st">&#39;price&#39;</span>].<span class="bu">max</span>()) , <span class="dv">2000000</span>) )</span>
<span id="cb149-9"><a href="#cb149-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb150"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a>ggplot(aes(x <span class="op">=</span> <span class="st">&#39;no_of_bathrooms&#39;</span> , y <span class="op">=</span> <span class="st">&#39;price&#39;</span>) , data_Python) </span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-4"><a href="#cb150-4" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> geom_point(fill<span class="op">=</span><span class="st">&quot;white&quot;</span>, color <span class="op">=</span> <span class="st">&#39;red&#39;</span>) </span>
<span id="cb150-5"><a href="#cb150-5" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> geom_smooth(method<span class="op">=</span><span class="st">&#39;lm&#39;</span>, color <span class="op">=</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb150-6"><a href="#cb150-6" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> labs(x <span class="op">=</span> <span class="st">&quot;no_of_bathrooms&quot;</span>, y <span class="op">=</span> <span class="st">&quot;price&quot;</span>)</span>
<span id="cb150-7"><a href="#cb150-7" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> scale_x_continuous( breaks <span class="op">=</span>  np.arange (data_Python[<span class="st">&#39;no_of_bathrooms&#39;</span>].<span class="bu">min</span>() , data_Python[<span class="st">&#39;no_of_bathrooms&#39;</span>].<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span> , <span class="dv">1</span>)  ) </span>
<span id="cb150-8"><a href="#cb150-8" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> scale_y_continuous( breaks <span class="op">=</span> <span class="bu">range</span>(<span class="bu">int</span>(data_Python[<span class="st">&#39;price&#39;</span>].<span class="bu">min</span>()) , <span class="bu">int</span>(data_Python[<span class="st">&#39;price&#39;</span>].<span class="bu">max</span>()) , <span class="dv">2000000</span>) )</span>
<span id="cb150-9"><a href="#cb150-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb151"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a>data_Python[<span class="st">&#39;quality&#39;</span>] <span class="op">=</span> data_Python[<span class="st">&#39;quality&#39;</span>].astype(<span class="st">&#39;int&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb152"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>ggplot(aes(x <span class="op">=</span> <span class="st">&#39;quality&#39;</span> , y <span class="op">=</span> <span class="st">&#39;price&#39;</span>) , data_Python) </span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-4"><a href="#cb152-4" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> geom_point(fill<span class="op">=</span><span class="st">&quot;white&quot;</span>, color <span class="op">=</span> <span class="st">&#39;red&#39;</span>) </span>
<span id="cb152-5"><a href="#cb152-5" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> geom_smooth(method<span class="op">=</span><span class="st">&#39;lm&#39;</span>, color <span class="op">=</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb152-6"><a href="#cb152-6" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> labs(x <span class="op">=</span> <span class="st">&quot;quality&quot;</span>, y <span class="op">=</span> <span class="st">&quot;price&quot;</span>)</span>
<span id="cb152-7"><a href="#cb152-7" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> scale_x_continuous( breaks <span class="op">=</span> np.arange (data_Python[<span class="st">&#39;quality&#39;</span>].<span class="bu">min</span>() , data_Python[<span class="st">&#39;quality&#39;</span>].<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span> , <span class="dv">1</span>) ) </span>
<span id="cb152-8"><a href="#cb152-8" aria-hidden="true" tabindex="-1"></a> <span class="op">+</span> scale_y_continuous( breaks <span class="op">=</span> <span class="bu">range</span>(<span class="bu">int</span>(data_Python[<span class="st">&#39;price&#39;</span>].<span class="bu">min</span>()) , <span class="bu">int</span>(data_Python[<span class="st">&#39;price&#39;</span>].<span class="bu">max</span>()) , <span class="dv">2000000</span>) )</span>
<span id="cb152-9"><a href="#cb152-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<hr />
</div>
<div id="finding-outliers" class="section level2" number="0.12">
<h2><span class="header-section-number">0.12</span> Finding Outliers <a class="anchor" id="74"></a></h2>
<p>An outlier is a point that clearly deviates from the relationship between the response and the predictors.</p>
<p>Outliers can greatly affect the estimates of model parameters.</p>
<p>How to identify outliers?</p>
<p>A simple option is with the standardized residuals, given by:</p>
<p><span class="math display">\[ \tilde{\varepsilon_i} = \dfrac{\hat{\varepsilon}_i}{\sqrt{\widehat{Var}(\hat{\varepsilon}_i)}} = \dfrac{\hat{\varepsilon}_i}{ \sqrt{ \widehat{\sigma}^2 \cdot (1-h_{ii})} }  \]</span></p>
<p>Where:</p>
<p><span class="math inline">\(\hspace{0.1cm} h_{ii} \hspace{0.1cm}\)</span> is the <span class="math inline">\(i\)</span>-th element of the principal diagonal of <span class="math inline">\(\hspace{0.1cm} H=X \cdot (X^t \cdot X)^{-1} \cdot X^t \hspace{0.1cm}\)</span></p>
<p>Note that:</p>
<p><span class="math display">\[\widehat{Var}(\hat{\varepsilon}_i) \neq \widehat{Var}(\varepsilon_i)  \]</span></p>
<p>Because <span class="math inline">\(\varepsilon_i\)</span> is a random variable such that <span class="math inline">\(\hspace{0.1cm}\varepsilon_i \sim N (0\hspace{ 0.1cm},\hspace{0.1cm} \sigma^2) \hspace{0.1cm}\)</span> (by initial hypothesis of the model)</p>
<p>And <span class="math inline">\(\hspace{0.1cm} \hat{\varepsilon}_i = y_i - \hat{y}_i \hspace{0.1cm}\)</span> is another random variable such that <span class="math inline">\(\hspace{0.1cm} \hat{\varepsilon}_i \sim N(0\hspace{0.1cm},\hspace{0.1cm}\sigma^2 \cdot (1-h_{ii})) \hspace{0.1cm}\)</span> (can be proof)</p>
<p>Criteria:</p>
<p>Observations whose studentized residuals are greater than 3 (in absolute value) will be considered outliers.</p>
<p>What to do with outliers:</p>
<p>If they are the result of an error in data collection, they can be deleted. If not, it shouldn’t, since it is part of the reality reflected by the sample, and is therefore relevant information.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> np.dot(X <span class="op">@</span> np.linalg.inv( X.T <span class="op">@</span> X ) , X.T) </span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a>standardized_estimated_errors <span class="op">=</span> estimated_errors <span class="op">/</span> np.sqrt(estimated_variance_error<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>np.diag(H)) )</span></code></pre></div>
<div class="sourceCode" id="cb154"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a>Outliers_df <span class="op">=</span> pd.DataFrame({<span class="st">&#39;standardized_estimated_errors &gt; 3&#39;</span> : standardized_estimated_errors <span class="op">&gt;</span> <span class="dv">3</span>})</span></code></pre></div>
<div class="sourceCode" id="cb155"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a>Outliers_df.head()</span></code></pre></div>
<div class="sourceCode" id="cb156"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>Outliers <span class="op">=</span> Outliers_df.loc[ Outliers_df[<span class="st">&#39;standardized_estimated_errors &gt; 3&#39;</span>] <span class="op">==</span> <span class="va">True</span> , : ].index</span></code></pre></div>
<div class="sourceCode" id="cb157"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a>Outliers</span></code></pre></div>
<hr />
</div>
<div id="bibliography" class="section level2" number="0.13">
<h2><span class="header-section-number">0.13</span> Bibliography</h2>
<p>Benitez Peña, S. (2021). <em>El modelo de regresión lineal múltiple</em> [Presentación de PowerPoint]. Aula Global UC3M.</p>
<p>Benitez Peña, S. (2021). <em>Inferencia en el modelo de regresión lineal múltiple</em> [Presentación de PowerPoint]. Aula Global UC3M.</p>
<p>Benitez Peña, S. (2021). <em>Diagnosis y validacion del modelo de regresión lineal múltiple</em> [Presentación de PowerPoint]. Aula Global UC3M.</p>
<p>Galeano, P. (2022). <em>Regresion Lineal Múltiple</em> [Presentación de PowerPoint]. Aula Global UC3M.</p>
<p>Fox, J. (2022). <em>Diagnosing Collinearity</em> [Presentación de PowerPoint]. McMaster University</p>
<p>Faraway, J (2015). <em>Linear Models with R</em> (second edition). CRC Press.</p>
</div>
</div>

   
   
            
      

  <script>
    $(document).ready(function () {

			
 	 	$('#content img').addClass("image-thumb");
		
		$('#content img:not(.no-lightbox)').addClass("image-lb");
	$('#content').magnificPopup({
	    type:'image',
	    closeOnContentClick: false,
	    closeBtnInside: false,
	    delegate: '.image-lb',
	    gallery: {enabled: false },
	    image: {
	        verticalFit: true,
		titleSrc: 'alt'
	    }
 	});
 	    });
  </script>



    <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
	var script = document.createElement("script");
	script.type = "text/javascript";
	script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
	document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>
  
</body>
</html>
